<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="noodp"><title>计算机视觉 - FangjieYu‘s site</title><meta name=Description content="This is my cool site"><meta property="og:url" content="https://fjyu95.github.io/posts/algorithm_knowledge_base/2_%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/">
<meta property="og:site_name" content="FangjieYu‘s site"><meta property="og:title" content="计算机视觉"><meta property="og:description" content="deep-learning、CV（2d、3d）、RS、optical"><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-10-22T00:00:00+00:00"><meta property="article:modified_time" content="2025-04-08T00:00:00+00:00"><meta property="article:tag" content="CV"><meta property="article:tag" content="图像算法"><meta name=twitter:card content="summary"><meta name=twitter:title content="计算机视觉"><meta name=twitter:description content="deep-learning、CV（2d、3d）、RS、optical"><meta name=application-name content="My cool site"><meta name=apple-mobile-web-app-title content="My cool site"><meta name=theme-color content="#ffffff"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://fjyu95.github.io/posts/algorithm_knowledge_base/2_%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/><link rel=prev href=https://fjyu95.github.io/posts/algorithm_knowledge_base/3_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/><link rel=next href=https://fjyu95.github.io/posts/algorithm_knowledge_base/1_%E4%BC%A0%E7%BB%9F%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95/><link rel=stylesheet href=/css/style.min.css><link rel=preload href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css></noscript><link rel=preload href=https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css></noscript><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"计算机视觉","inLanguage":"en-us","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/fjyu95.github.io\/posts\/algorithm_knowledge_base\/2_%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89\/"},"genre":"posts","keywords":"CV, 图像算法","wordcount":3522,"url":"https:\/\/fjyu95.github.io\/posts\/algorithm_knowledge_base\/2_%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89\/","datePublished":"2024-10-22T00:00:00+00:00","dateModified":"2025-04-08T00:00:00+00:00","publisher":{"@type":"Organization","name":""},"author":{"@type":"Person","name":"fjyu95"},"description":""}</script></head><body data-header-desktop=fixed data-header-mobile=auto><script type=text/javascript>(window.localStorage&&localStorage.getItem("theme")?localStorage.getItem("theme")==="dark":"auto"==="auto"?window.matchMedia("(prefers-color-scheme: dark)").matches:"auto"==="dark")&&document.body.setAttribute("theme","dark")</script><div id=mask></div><div class=wrapper><header class=desktop id=header-desktop><div class=header-wrapper><div class=header-title><a href=/ title="FangjieYu‘s site">fjyu95</a></div><div class=menu><div class=menu-inner><a class=menu-item href=/posts/>posts </a><a class=menu-item href=/tags/>tags </a><a class=menu-item href=/categories/>category </a><span class="menu-item delimiter"></span><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme">
<i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></div></div></div></header><header class=mobile id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title="FangjieYu‘s site">fjyu95</a></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><div class=menu id=menu-mobile><a class=menu-item href=/posts/ title>posts</a><a class=menu-item href=/tags/ title>tags</a><a class=menu-item href=/categories/ title>category</a><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme">
<i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></div></div></header><main class=main><div class=container><div class=toc id=toc-auto><h2 class=toc-title>Contents</h2><div class=toc-content id=toc-content-auto></div></div><article class="page single"><h1 class="single-title animate__animated animate__flipInX">计算机视觉</h1><div class=post-meta><div class=post-meta-line><span class=post-author><a href=https://fjyu95.github.io/ title=Author target=_blank rel="noopener noreffer author" class=author><i class="fas fa-user-circle fa-fw" aria-hidden=true></i>fjyu95</a></span>&nbsp;<span class=post-category>included in <a href=/categories/%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%E5%BA%93/><i class="far fa-folder fa-fw" aria-hidden=true></i>算法知识库</a></span></div><div class=post-meta-line><i class="far fa-calendar-alt fa-fw" aria-hidden=true></i>&nbsp;<time datetime=2024-10-22>2024-10-22</time>&nbsp;<i class="fas fa-pencil-alt fa-fw" aria-hidden=true></i>&nbsp;3522 words&nbsp;
<i class="far fa-clock fa-fw" aria-hidden=true></i>&nbsp;8 minutes&nbsp;</div></div><div class="details toc" id=toc-static data-kept><div class="details-summary toc-title"><span>Contents</span>
<span><i class="details-icon fas fa-angle-right" aria-hidden=true></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#计算机视觉>计算机视觉</a><ul><li><a href=#图像分类>图像分类</a><ul><li><a href=#resnet-只学习输入与残差的映射>resnet 只学习输入与残差的映射</a></li></ul></li><li><a href=#目标检测><strong>目标检测</strong></a><ul><li><a href=#yolo>yolo</a></li><li><a href=#yolo的核心思想>YOLO的核心思想</a></li><li><a href=#yolo的具体工作流程>YOLO的具体工作流程</a></li><li><a href=#yolo的主要版本演变>YOLO的主要版本演变</a></li><li><a href=#yolov3-vs-yolov5-性能对比>YOLOv3 vs YOLOv5 性能对比</a></li><li><a href=#yolo的优缺点>YOLO的优缺点</a></li><li><a href=#ssd>ssd</a></li><li><a href=#rcnn系列>rcnn系列</a></li><li><a href=#nms>NMS</a></li></ul></li><li><a href=#图像分割>图像分割</a><ul><li><a href=#空洞空间金字塔池化aspp>空洞空间金字塔池化（ASPP）</a></li></ul></li><li><a href=#点云分割>点云分割</a></li><li><a href=#pointnet>pointnet</a></li><li><a href=#pointnet-1>pointnet++</a></li></ul></li><li><a href=#硕士论文改进>硕士论文改进</a></li></ul></nav></div></div><div class=content id=content><h1 id=计算机视觉>计算机视觉</h1><h2 id=图像分类>图像分类</h2><p>lenet 第一个成功应用 mnist</p><p>alexnet 精心设计网络结构</p><p>vgg 模块化，通用设计范式</p><p>googlenet 并行化</p><h3 id=resnet-只学习输入与残差的映射>resnet 只学习输入与残差的映射</h3><p><img class=lazyload src=/svg/loading.min.svg data-src=/posts/algorithm_knowledge_base/2_%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/image%2045.png data-srcset="/posts/algorithm_knowledge_base/2_%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/image%2045.png, /posts/algorithm_knowledge_base/2_%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/image%2045.png 1.5x, /posts/algorithm_knowledge_base/2_%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/image%2045.png 2x" data-sizes=auto alt=/posts/algorithm_knowledge_base/2_%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/image%2045.png title=image.png width=1022 height=446></p><p><img class=lazyload src=/svg/loading.min.svg data-src=/posts/algorithm_knowledge_base/2_%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/image%2046.png data-srcset="/posts/algorithm_knowledge_base/2_%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/image%2046.png, /posts/algorithm_knowledge_base/2_%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/image%2046.png 1.5x, /posts/algorithm_knowledge_base/2_%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/image%2046.png 2x" data-sizes=auto alt=/posts/algorithm_knowledge_base/2_%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/image%2046.png title=image.png width=846 height=489></p><h2 id=目标检测><strong>目标检测</strong></h2><p>锚框（anchor box）：以每个像素为中心，生成多个缩放比和宽高比不同的边界框</p><h3 id=yolo>yolo</h3><h3 id=yolo的核心思想>YOLO的核心思想</h3><p>YOLO的基本思想是<strong>将输入图像划分为网格，每个网格负责检测该区域内的目标</strong>。每个网格预测多个边界框及其对应的类别分数，经过一系列优化筛选后，输出最终的检测结果。YOLO避免了传统的区域建议步骤，使得检测速度大幅提升。</p><h3 id=yolo的具体工作流程>YOLO的具体工作流程</h3><ol><li><strong>图像划分和特征提取</strong><ul><li>YOLO将输入图像划分为固定大小的网格（例如 7x7）。</li><li>使用CNN提取图像特征并将其映射到这些网格。</li></ul></li><li><strong>边界框预测</strong><ul><li>每个网格预测多个边界框及每个边界框对应的置信度分数，这些分数表示框中是否存在物体以及边界框定位的精确度。</li><li>每个边界框同时预测目标类别的分数，这个分数是类别概率与置信度的乘积。</li></ul></li><li><strong>非极大值抑制（NMS）</strong><ul><li>YOLO使用NMS去除重叠较多的边界框，仅保留最具有代表性的检测结果。</li><li>这一步可以有效减少冗余框，保持检测精度。</li></ul></li><li><strong>输出目标类别和边界框</strong><ul><li>YOLO直接输出最终的边界框和类别，完成目标检测任务。</li></ul></li></ol><h3 id=yolo的主要版本演变>YOLO的主要版本演变</h3><ol><li><strong>YOLOv1</strong><ul><li>初版YOLO将图像划分为7x7网格，每个网格预测2个边界框和20个类别分数，速度较快，但在小目标检测上表现一般。</li></ul></li><li><strong>YOLOv2（YOLO9000）</strong><ul><li>引入Batch Normalization提高稳定性，使用锚框（Anchor Boxes）预测，能够检测9,000个类别，提高了准确率。</li></ul></li><li><strong>YOLOv3</strong></li></ol><ul><li>YOLOv3使用<strong>多尺度预测</strong>，对不同大小的目标检测更加友好，增加了网络深度和复杂度，<strong>提升了小目标的检测性能</strong>。<strong>NMS</strong></li><li><strong>多尺度特征检测</strong><ul><li>YOLOv3引入了FPN（特征金字塔网络）思想，能够在不同特征尺度上进行检测。这种设计特别适合检测不同大小的目标，尤其是小目标的检测。</li><li>具体来说，YOLOv3在三种尺度上检测目标，分别在较低、较中和较高的特征层中进行预测。</li></ul></li><li><strong>Darknet-53主干网络</strong><ul><li>YOLOv3使用了一种新设计的卷积神经网络Darknet-53作为主干网络。Darknet-53由53层卷积层组成，相比于YOLOv2的Darknet-19更加深层，提高了特征提取能力。</li><li>使用了残差块（Residual Block）结构，避免梯度消失问题，并且提升了特征表示能力。</li></ul></li><li><strong>改进的分类和定位方法</strong><ul><li>YOLOv3采用逻辑二元交叉熵损失（Binary Cross-Entropy Loss）来预测每个类别的概率，使分类损失计算更加高效。</li><li>仍然使用非极大值抑制（NMS）来移除重叠边框。</li></ul></li><li><strong>锚框机制</strong><ul><li>YOLOv3引入了预定义的锚框，与R-CNN系列的Region Proposal方法类似，能更好地适应不同尺寸的目标。</li></ul></li></ul><ol><li><strong>YOLOv4和YOLOv5</strong><ul><li>采用了<strong>更多优化技术</strong>（如CSP、SPP模块），性能进一步提升。YOLOv5在结构设计和推理速度上有显著改进，得到了广泛应用。</li></ul></li><li><strong>YOLOv7</strong><ul><li>使用了先进的模型架构和特征增强模块，提升了精度和速度，在主流的COCO数据集上达到较高的检测精度。</li></ul></li></ol><h3 id=yolov3-vs-yolov5-性能对比>YOLOv3 vs YOLOv5 性能对比</h3><table><thead><tr><th>特性</th><th>YOLOv3</th><th>YOLOv5</th></tr></thead><tbody><tr><td>主干网络</td><td>Darknet-53</td><td>自定义CSPNet（轻量化多版本可选）</td></tr><tr><td>主要框架</td><td>Darknet（C）</td><td>PyTorch（更容易上手和开发）</td></tr><tr><td>多尺度检测</td><td>支持</td><td>支持</td></tr><tr><td>数据增强</td><td>较少</td><td>Mosaic增强、混合卷积、自动学习率调整</td></tr><tr><td>模型复杂度</td><td>较高</td><td>更加轻量化（尤其是YOLOv5s）</td></tr><tr><td>推理速度</td><td>较慢（尤其在资源受限设备上）</td><td>更快</td></tr><tr><td>精度和召回</td><td>在大多数任务中表现略逊于YOLOv5</td><td>精度和召回优于YOLOv3，尤其在小目标检测中</td></tr></tbody></table><h3 id=yolo的优缺点>YOLO的优缺点</h3><ul><li><strong>优点</strong><ul><li><strong>速度快</strong>：YOLO可以在一次前向传播内完成目标检测，速度极快，适合实时检测场景。</li><li><strong>端到端设计</strong>：YOLO将目标检测当作一个单一的回归问题，不需要额外的区域建议步骤。</li></ul></li><li><strong>缺点</strong><ul><li><strong>定位精度有限</strong>：YOLO在边框回归方面的精度较高，但仍可能不如基于区域建议的算法（如Faster R-CNN）。</li><li><strong>小目标检测效果较差</strong>：早期版本的YOLO在小目标上表现不佳，但后续版本（如YOLOv3及以上）对此有所改进。</li></ul></li></ul><h3 id=ssd>ssd</h3><p>单发多框</p><p>多尺度检测</p><h3 id=rcnn系列>rcnn系列</h3><p>rcnn</p><p><img class=lazyload src=/svg/loading.min.svg data-src=image%2047.png data-srcset="image%2047.png, image%2047.png 1.5x, image%2047.png 2x" data-sizes=auto alt=image%2047.png title=image.png></p><p><img class=lazyload src=/svg/loading.min.svg data-src=image%2048.png data-srcset="image%2048.png, image%2048.png 1.5x, image%2048.png 2x" data-sizes=auto alt=image%2048.png title=image.png></p><p>fast-rcnn</p><p>faster-rcnn</p><p><strong>rpn区域提议网络</strong></p><p>roi pooling 区域兴趣池化</p><p>mask-rcnn</p><h3 id=nms>NMS</h3><p>去除重叠区域的低置信度检测框，提高检测精度</p><p>NMS基本算法的具体步骤如下：</p><p>1）依据框的分数（即目标的概率）将所有预测框排序；<br>2）选择最大分数的检测框M，将其他与M框重叠度大（IoU超过阈值Nt）的框抑制；<br>3）迭代这一过程直到所有框被检测完成。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># NMS 算法的简化伪代码：</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>nms</span><span class=p>(</span><span class=n>boxes</span><span class=p>,</span> <span class=n>scores</span><span class=p>,</span> <span class=n>iou_threshold</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=c1># boxes 是检测框列表，scores 是对应的置信度分数</span>
</span></span><span class=line><span class=cl>    <span class=n>sorted_indices</span> <span class=o>=</span> <span class=nb>sorted</span><span class=p>(</span><span class=nb>range</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>scores</span><span class=p>)),</span> <span class=n>key</span><span class=o>=</span><span class=k>lambda</span> <span class=n>k</span><span class=p>:</span> <span class=n>scores</span><span class=p>[</span><span class=n>k</span><span class=p>],</span> <span class=n>reverse</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>keep_boxes</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=k>while</span> <span class=n>sorted_indices</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=c1># 选择置信度最高的框</span>
</span></span><span class=line><span class=cl>        <span class=n>current_idx</span> <span class=o>=</span> <span class=n>sorted_indices</span><span class=o>.</span><span class=n>pop</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>keep_boxes</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>boxes</span><span class=p>[</span><span class=n>current_idx</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 计算当前框与其他框的 IoU 并删除重叠度过高的框，更新sorted_indices</span>
</span></span><span class=line><span class=cl>        <span class=n>sorted_indices</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>            <span class=n>idx</span> <span class=k>for</span> <span class=n>idx</span> <span class=ow>in</span> <span class=n>sorted_indices</span>
</span></span><span class=line><span class=cl>            <span class=k>if</span> <span class=n>compute_iou</span><span class=p>(</span><span class=n>boxes</span><span class=p>[</span><span class=n>current_idx</span><span class=p>],</span> <span class=n>boxes</span><span class=p>[</span><span class=n>idx</span><span class=p>])</span> <span class=o>&lt;</span> <span class=n>iou_threshold</span>
</span></span><span class=line><span class=cl>        <span class=p>]</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>keep_boxes</span>
</span></span></code></pre></div><h2 id=图像分割>图像分割</h2><p>FCN 第一个端到端图像分割模型，反卷积</p><p>U-Net 对称式编解码结构，跳层连接+转置卷积</p><p>Segnet 对称式编解码结构，反池化、无跳层连接，计算效率更高</p><p>Deeplab系列 主要是ASPP模块，多个尺度上使用<strong>空洞卷积</strong></p><p>空洞卷积增大了感受野，但是计算量不变</p><h3 id=空洞空间金字塔池化aspp>空洞空间金字塔池化（ASPP）</h3><ul><li><strong>设计</strong>：ASPP 模块使用多个<strong>不同空洞率的空洞卷积</strong>并行处理输入特征图。这些空洞卷积可以捕捉不同尺度的上下文信息。</li><li><strong>聚合</strong>：ASPP 将不同空洞卷积的输出特征图进行拼接（concatenation），生成具有多尺度信息的特征表示。</li><li><strong>全局特征</strong>：ASPP 通常还包括一个全局平均池化层，以获取全局上下文信息，并将其与其他特征图结合。</li></ul><p><a href=https://blog.csdn.net/duanyajun987/article/details/82108006 target=_blank rel="noopener noreffer">对全局平均池化（GAP）过程的理解_全局平均池化层是由谁在哪提出的-CSDN博客</a></p><p>MobileNetUnet</p><p>Unet作为基本结构，MobileNet来提取特征</p><h2 id=点云分割>点云分割</h2><p><a href=https://blog.csdn.net/u014636245/article/details/82763269 target=_blank rel="noopener noreffer">https://blog.csdn.net/u014636245/article/details/82763269</a></p><p><a href=https://blog.csdn.net/weixin_43199584/article/details/104950696 target=_blank rel="noopener noreffer">https://blog.csdn.net/weixin_43199584/article/details/104950696</a></p><p>1、无序性 T-Net对称函数处理</p><p>2、稀疏性 <strong>局部采样+分组</strong>提取局部特征 pointnet++</p><p>3、非结构性 使用mlp而非CNN提取特征</p><h2 id=pointnet>pointnet</h2><p>CVPR2017 提出</p><p><strong>T-Net处理无序点集，基于mlp提取特征，高层特征与浅层特征融合</strong></p><p><strong>缺少邻域信息、对稀疏性敏感</strong></p><p><img class=lazyload src=/svg/loading.min.svg data-src=pointnet.jpg data-srcset="pointnet.jpg, pointnet.jpg 1.5x, pointnet.jpg 2x" data-sizes=auto alt=pointnet.jpg title=pointnet.jpg></p><p>各个点的操作过于独立，点的邻域信息没有得到有效利用（学位论文）</p><ul><li><strong>基本结构</strong>：PointNet 是<strong>首个直接处理点云的深度学习模型</strong>，使用对称函数（如最大池化）来处理无序点集。</li><li><strong>特点</strong>：通过全连接层提取特征，并通过池化层生成全局特征。PointNet 可以用于分类和分割任务。</li><li><strong>分割方法</strong>：在分割任务中，PointNet 会对每个点进行分类，输出每个点的标签。</li></ul><p>共享的mlp理解：</p><ul><li>在 PointNet 中，MLP 的权重是共享的，这意味着对于点云中的每个点，使用相同的 MLP 参数进行特征提取。换句话说，输入点云中的每个点都通过同一组权重来生成其特征表示。</li><li>这种权重共享的设计确保了网络对输入点的顺序不敏感，并使得网络能够处理任意数量的输入点。</li></ul><p>共享的 MLP 是 PointNet 网络设计中的一个核心部分，它通过共享权重来处理无序点云数据，提取每个点的特征，最终实现高效的全局特征聚合。这个设计使得 PointNet 能够在处理复杂的三维数据时，既保持了计算效率，又能够有效地捕捉几何信息。</p><h2 id=pointnet-1>pointnet++</h2><p>通过对数据<strong>进行采样、分组的方式提取局部特征</strong>,并使用MSG(multi-scale grouping)、<br>MRG(multi-resolution grouping)等策略自适应的处理密度不均匀的点云数据**（学位论文）**</p><ul><li><strong>改进</strong>：PointNet++ 是对 PointNet 的扩展，通过分层抽样和分组处理<strong>捕捉局部特征</strong>。</li><li><strong>特点</strong>：在每一层中，PointNet++ 会对点云进行分层处理，使模型能够捕捉更细粒度的特征。</li><li><strong>分割方法</strong>：适用于大规模点云数据，能够有效地进行语义分割和实例分割。</li></ul><h1 id=硕士论文改进>硕士论文改进</h1><p>1、紧凑网络 FC层——深度可分离卷积</p><p>2、结构化剪枝 减少宽度、深度的方式</p><p>3、参数量化 float32 to uint8</p><p><img class=lazyload src=/svg/loading.min.svg data-src=/posts/algorithm_knowledge_base/2_%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/image%2049.png data-srcset="/posts/algorithm_knowledge_base/2_%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/image%2049.png, /posts/algorithm_knowledge_base/2_%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/image%2049.png 1.5x, /posts/algorithm_knowledge_base/2_%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/image%2049.png 2x" data-sizes=auto alt=/posts/algorithm_knowledge_base/2_%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/image%2049.png title=image.png width=1074 height=310></p><p>改进后的PointNet网络共有12层,整体可分为三个部分。<br>第一部分是前5层Conv1~ Conv5,主要作用是<strong>提取浅层特征</strong>,层名称后面的参数如1×1、1×9等表示的是卷积核尺寸,最后一个参数表示输出通道数,对于输入的n个点数据,每个点具有(x、y、z、r、g、b、x&rsquo;、y&rsquo;、z&rsquo;)9个特征,经过第一个卷积层Conv1之后,每个点的特征将会被融合为1个,之后再经过卷积层Conv2~ Conv5逐级提取特征,卷积层中的其他细节还包括高和宽两个方向上的步长分别为[1,1],填充方式使用的是“VALID”类型,激活函数为修正<br>线性单元ReLU,这些参数设定对于其他部分的卷积操作也是一致的;</p><p>第二部分是中间三层,主要作用是<strong>全局特征筛选和增加网络表达能力</strong>,该部分对于Conv5的输出先作一个最大池化,池化时的卷积核尺寸为n×1,相当于个输出通道保留一个特征,共得到256个全局特征,接着经过两个<strong>替代了FC层的可分离卷积操作</strong>对全局特征进行非线性变换,输出128个特征值;</p><p>第三部分是最后四层,作用是对前两部分提取的<strong>浅层和全局特征进行跳层连接并输出最终分割结果</strong>,得到每一个点属于13个类别中各个类别的概率。</p></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span>Updated on 2025-04-08</span></div></div><div class=post-info-line><div class=post-info-md></div><div class=post-info-share><span><a href=javascript:void(0); title="Share on Twitter" data-sharer=twitter data-url=https://fjyu95.github.io/posts/algorithm_knowledge_base/2_%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/ data-title=计算机视觉 data-hashtags=CV,图像算法><i class="fab fa-twitter fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Facebook" data-sharer=facebook data-url=https://fjyu95.github.io/posts/algorithm_knowledge_base/2_%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/ data-hashtag=CV><i class="fab fa-facebook-square fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Hacker News" data-sharer=hackernews data-url=https://fjyu95.github.io/posts/algorithm_knowledge_base/2_%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/ data-title=计算机视觉><i class="fab fa-hacker-news fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Line" data-sharer=line data-url=https://fjyu95.github.io/posts/algorithm_knowledge_base/2_%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/ data-title=计算机视觉><i data-svg-src=https://cdn.jsdelivr.net/npm/simple-icons@7.3.0/icons/line.svg aria-hidden=true></i></a><a href=javascript:void(0); title="Share on 微博" data-sharer=weibo data-url=https://fjyu95.github.io/posts/algorithm_knowledge_base/2_%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/ data-title=计算机视觉><i class="fab fa-weibo fa-fw" aria-hidden=true></i></a></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fas fa-tags fa-fw" aria-hidden=true></i>&nbsp;<a href=/tags/cv/>CV</a>,&nbsp;<a href=/tags/%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95/>图像算法</a></section><section><span><a href=javascript:void(0); onclick=window.history.back()>Back</a></span>&nbsp;|&nbsp;<span><a href=/>Home</a></span></section></div><div class=post-nav><a href=/posts/algorithm_knowledge_base/3_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/ class=prev rel=prev title=机器学习><i class="fas fa-angle-left fa-fw" aria-hidden=true></i>机器学习</a>
<a href=/posts/algorithm_knowledge_base/1_%E4%BC%A0%E7%BB%9F%E5%9B%BE%E5%83%8F%E7%AE%97%E6%B3%95/ class=next rel=next title=传统图像算法>传统图像算法<i class="fas fa-angle-right fa-fw" aria-hidden=true></i></a></div></div></article></div></main><footer class=footer><div class=footer-container><div class=footer-line>Powered by <a href=https://gohugo.io/ target=_blank rel="noopener noreffer" title="Hugo 0.145.0">Hugo</a> | Theme - <a href=https://github.com/dillonzq/LoveIt target=_blank rel="noopener noreffer" title="LoveIt 0.3.0"><i class="far fa-kiss-wink-heart fa-fw" aria-hidden=true></i> LoveIt</a></div><div class=footer-line itemscope itemtype=http://schema.org/CreativeWork><i class="far fa-copyright fa-fw" aria-hidden=true></i><span itemprop=copyrightYear>2020 - 2025</span><span class=author itemprop=copyrightHolder>&nbsp;<a href=https://fjyu95.github.io/ target=_blank>fjyu95</a></span></div></div></footer></div><div id=fixed-buttons><a href=# id=back-to-top class=fixed-button title="Back to Top"><i class="fas fa-arrow-up fa-fw" aria-hidden=true></i>
</a><a href=# id=view-comments class=fixed-button title="View Comments"><i class="fas fa-comment fa-fw" aria-hidden=true></i></a></div><script type=text/javascript src=https://cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/sharer.js@0.5.1/sharer.min.js></script><script type=text/javascript>window.config={code:{copyTitle:"Copy to clipboard",maxShownLines:50},comment:{}}</script><script type=text/javascript src=/js/theme.min.js></script></body></html>
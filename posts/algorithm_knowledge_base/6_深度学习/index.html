<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="noodp"><title>深度学习 - FangjieYu‘s site</title><meta name=Description content="This is my cool site"><meta property="og:url" content="https://fjyu95.github.io/posts/algorithm_knowledge_base/6_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">
<meta property="og:site_name" content="FangjieYu‘s site"><meta property="og:title" content="深度学习"><meta property="og:description" content="deep-learning、CV（2d、3d）、RS、optical"><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-10-22T00:00:00+00:00"><meta property="article:modified_time" content="2025-04-08T00:00:00+00:00"><meta property="article:tag" content="Deep Learning"><meta name=twitter:card content="summary"><meta name=twitter:title content="深度学习"><meta name=twitter:description content="deep-learning、CV（2d、3d）、RS、optical"><meta name=application-name content="My cool site"><meta name=apple-mobile-web-app-title content="My cool site"><meta name=theme-color content="#ffffff"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://fjyu95.github.io/posts/algorithm_knowledge_base/6_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/><link rel=prev href=https://fjyu95.github.io/posts/algorithm_knowledge_base/7_vslam/><link rel=next href=https://fjyu95.github.io/posts/algorithm_knowledge_base/5_%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9/><link rel=stylesheet href=/css/style.min.css><link rel=preload href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css></noscript><link rel=preload href=https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css></noscript><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"深度学习","inLanguage":"en-us","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/fjyu95.github.io\/posts\/algorithm_knowledge_base\/6_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0\/"},"genre":"posts","keywords":"deep learning","wordcount":2905,"url":"https:\/\/fjyu95.github.io\/posts\/algorithm_knowledge_base\/6_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0\/","datePublished":"2024-10-22T00:00:00+00:00","dateModified":"2025-04-08T00:00:00+00:00","publisher":{"@type":"Organization","name":""},"author":{"@type":"Person","name":"fjyu95"},"description":""}</script></head><body data-header-desktop=fixed data-header-mobile=auto><script type=text/javascript>(window.localStorage&&localStorage.getItem("theme")?localStorage.getItem("theme")==="dark":"auto"==="auto"?window.matchMedia("(prefers-color-scheme: dark)").matches:"auto"==="dark")&&document.body.setAttribute("theme","dark")</script><div id=mask></div><div class=wrapper><header class=desktop id=header-desktop><div class=header-wrapper><div class=header-title><a href=/ title="FangjieYu‘s site">fjyu95</a></div><div class=menu><div class=menu-inner><a class=menu-item href=/posts/>posts </a><a class=menu-item href=/tags/>tags </a><a class=menu-item href=/categories/>category </a><a class=menu-item href=/about/>about </a><span class="menu-item delimiter"></span><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme">
<i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></div></div></div></header><header class=mobile id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title="FangjieYu‘s site">fjyu95</a></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><div class=menu id=menu-mobile><a class=menu-item href=/posts/ title>posts</a><a class=menu-item href=/tags/ title>tags</a><a class=menu-item href=/categories/ title>category</a><a class=menu-item href=/about/ title>about</a><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme">
<i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></div></div></header><main class=main><div class=container><div class=toc id=toc-auto><h2 class=toc-title>Contents</h2><div class=toc-content id=toc-content-auto></div></div><article class="page single"><h1 class="single-title animate__animated animate__flipInX">深度学习</h1><div class=post-meta><div class=post-meta-line><span class=post-author><a href=https://fjyu95.github.io/ title=Author target=_blank rel="noopener noreffer author" class=author><i class="fas fa-user-circle fa-fw" aria-hidden=true></i>fjyu95</a></span>&nbsp;<span class=post-category>included in <a href=/categories/%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%E5%BA%93/><i class="far fa-folder fa-fw" aria-hidden=true></i>算法知识库</a></span></div><div class=post-meta-line><i class="far fa-calendar-alt fa-fw" aria-hidden=true></i>&nbsp;<time datetime=2024-10-22>2024-10-22</time>&nbsp;<i class="fas fa-pencil-alt fa-fw" aria-hidden=true></i>&nbsp;2905 words&nbsp;
<i class="far fa-clock fa-fw" aria-hidden=true></i>&nbsp;6 minutes&nbsp;</div></div><div class="details toc" id=toc-static data-kept><div class="details-summary toc-title"><span>Contents</span>
<span><i class="details-icon fas fa-angle-right" aria-hidden=true></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#深度学习>深度学习</a><ul><li><a href=#cnn>CNN</a></li><li><a href=#过拟合欠拟合>过拟合、欠拟合</a><ul><li><a href=#过拟合-模型太复杂测试集上效果差>过拟合 模型太复杂，测试集上效果差</a></li><li><a href=#欠拟合-模型太简单训练集-测试集上效果都差>欠拟合 模型太简单，训练集、 测试集上效果都差</a></li></ul></li><li><a href=#交叉验证>交叉验证</a></li><li><a href=#激活函数>激活函数</a></li><li><a href=#sigmoid和softmax区别>sigmoid和softmax区别</a></li><li><a href=#损失函数>损失函数</a><ul><li><a href=#分类损失>分类损失</a></li><li><a href=#回归损失>回归损失</a></li><li><a href=#检测分割>检测、分割</a></li></ul></li><li><a href=#优化算法>优化算法</a><ul><li><a href=#adam>Adam</a></li></ul></li><li><a href=#rnn>RNN</a></li><li><a href=#cnn与rnn区别>CNN与RNN区别</a><ul><li><a href=#输入数据类型>输入数据类型</a></li><li><a href=#训练和计算>训练和计算</a></li></ul></li><li><a href=#cnn与mlp>CNN与MLP</a><ul><li><a href=#上采样反池化反卷积>上采样、反池化、反卷积</a></li><li><a href=#bnln>BN、LN</a></li></ul></li></ul></li></ul></nav></div></div><div class=content id=content><h1 id=深度学习>深度学习</h1><h2 id=cnn>CNN</h2><p>卷积 使用卷积核（过滤器）提取局部特征，通过<strong>权重共享</strong>降低参数数量</p><p>激活 非线性化</p><p>池化 缓解位置敏感性，增加平移不变性</p><p>全局平均池化 GAP 每个通道的特征图计算一个平均值，一般用来替代全连接层</p><p>全连接 每个神经元与前一层的所有神经元相连，常用于分类任务的输出。</p><pre><code>   对于图像特征，**先Flatten后使用**
</code></pre><p>BN层 提高模型稳定性和收敛性，批次内按特征进行标准化</p><p>初始化 Xavier</p><h2 id=过拟合欠拟合>过拟合、欠拟合</h2><h3 id=过拟合-模型太复杂测试集上效果差>过拟合 模型太复杂，测试集上效果差</h3><p>1、降低模型复杂度</p><p>2、增加数据、生成数据、数据增强</p><p>3、dropout</p><p>4、正则化（L1、L2）</p><ul><li><p><strong>L1 正则化（Lasso）</strong>：通过引入权重绝对值的惩罚项，促使某些权重变为零，从而进行特征选择。<strong>倾向于产生稀疏的权重向量，因为它将一些权重推向零</strong></p><pre><code>在权重接近零时的惩罚更为显著
</code></pre></li><li><p><strong>L2 正则化（Ridge）</strong>：通过引入权重平方和的惩罚项，限制权重的大小，从而防止模型对训练数据的过度拟合。</p></li></ul><p>5、早停 Early Stop</p><p>6、验证交叉</p><h3 id=欠拟合-模型太简单训练集-测试集上效果都差>欠拟合 模型太简单，训练集、 测试集上效果都差</h3><p>增加模型复杂度</p><p>减少正则化</p><p><strong>增加训练时间</strong></p><h2 id=交叉验证>交叉验证</h2><p>用于模型选择</p><h2 id=激活函数>激活函数</h2><p>relu</p><p>Leaky ReLU</p><p>sigmoid 二分类</p><p>tanh (-1, 1)</p><p>当输入在0附近时，tanh函数接近线性变换。函数的形状类似于sigmoid函数，<br>不同的是tanh函数关于坐标系原点中心对称。</p><h2 id=sigmoid和softmax区别>sigmoid和softmax区别</h2><ul><li><p><strong>Sigmoid</strong>：</p><ul><li><p><strong>公式</strong>：σ(x)=1+e−x1​</p><p>σ(x)=11+e−x\sigma(x) = \frac{1}{1 + e^{-x}}</p></li><li><p><strong>输出范围</strong>：将输入值映射到 (0, 1) 区间，<strong>适用于二分类问题</strong>。</p></li></ul></li><li><p><strong>Softmax</strong>：Softmax(zi​)=∑j=1n​ezj​ezi​​</p><ul><li><p><strong>公式</strong>：对于一个向量 z=[z1​,z2​,…,zn​]，Softmax 定义为：</p><p>z=[z1,z2,…,zn]z = [z_1, z_2, \ldots, z_n]</p></li></ul><p>Softmax(zi)=ezi∑j=1nezj\text{Softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{n} e^{z_j}}</p><ul><li><strong>输出范围</strong>：将输入值转换为 (0, 1) 区间，并且所有输出值的和为 1，适用于<strong>多分类</strong>问题。</li></ul></li></ul><h2 id=损失函数>损失函数</h2><p><img class=lazyload src=/svg/loading.min.svg data-src=/posts/algorithm_knowledge_base/6_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/image%2053.png data-srcset="/posts/algorithm_knowledge_base/6_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/image%2053.png, /posts/algorithm_knowledge_base/6_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/image%2053.png 1.5x, /posts/algorithm_knowledge_base/6_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/image%2053.png 2x" data-sizes=auto alt=/posts/algorithm_knowledge_base/6_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/image%2053.png title=image.png width=486 height=669></p><h3 id=分类损失>分类损失</h3><p>0-1损失 二分类</p><p><strong>交叉熵损失</strong></p><p><a href=https://www.vectorexplore.com/tech/loss-functions/cross-entropy/ target=_blank rel="noopener noreffer">https://www.vectorexplore.com/tech/loss-functions/cross-entropy/</a></p><p><strong>衡量两个分布之间的相似程度(距离)</strong></p><p>模型训练时，通过<strong>最小化交叉熵损失函数</strong>，我们可以使模型预测值的概率分布逐步接近真实的概率分布。</p><ul><li><p><strong>二元交叉熵损失（Binary Cross-Entropy Loss）</strong>：</p><p><img class=lazyload src=/svg/loading.min.svg data-src=/posts/algorithm_knowledge_base/6_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/image%2054.png data-srcset="/posts/algorithm_knowledge_base/6_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/image%2054.png, /posts/algorithm_knowledge_base/6_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/image%2054.png 1.5x, /posts/algorithm_knowledge_base/6_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/image%2054.png 2x" data-sizes=auto alt=/posts/algorithm_knowledge_base/6_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/image%2054.png title=image.png width=416 height=58></p><p>n表示样本数量</p></li><li><p><strong>多元交叉熵损失（Categorical Cross-Entropy Loss）</strong>：</p><p><img class=lazyload src=/svg/loading.min.svg data-src=/posts/algorithm_knowledge_base/6_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/image%2055.png data-srcset="/posts/algorithm_knowledge_base/6_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/image%2055.png, /posts/algorithm_knowledge_base/6_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/image%2055.png 1.5x, /posts/algorithm_knowledge_base/6_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/image%2055.png 2x" data-sizes=auto alt=/posts/algorithm_knowledge_base/6_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/image%2055.png title=image.png width=222 height=68></p></li><li><p>适用于多分类问题，C 表示类别数。</p></li><li><p>C=2时，等价于BCE</p></li></ul><p>KL散度/相对熵</p><p>对数损失</p><p>指数损失</p><p>合页损失 svm</p><h3 id=回归损失>回归损失</h3><p><img class=lazyload src=/svg/loading.min.svg data-src=/posts/algorithm_knowledge_base/6_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/image%2056.png data-srcset="/posts/algorithm_knowledge_base/6_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/image%2056.png, /posts/algorithm_knowledge_base/6_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/image%2056.png 1.5x, /posts/algorithm_knowledge_base/6_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/image%2056.png 2x" data-sizes=auto alt=/posts/algorithm_knowledge_base/6_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/image%2056.png title=image.png width=980 height=254></p><p>L1损失-MAE</p><p>L2损失-MSE</p><p>huber损失</p><h3 id=检测分割>检测、分割</h3><p>IoU损失</p><h2 id=优化算法>优化算法</h2><p>梯度下降法（每次使用全部数据，计算一个平均下降方向，速度慢）</p><p>随机梯度下降（每次使用单个数据，不稳定）</p><p>小批量随机梯度下降 （兼顾速度和稳定性）</p><p>动量法 加了一个“动量”项，用于平滑参数更新</p><h3 id=adam>Adam</h3><p><strong>结合了动量法和 RMSProp，计算梯度的一阶矩（均值）和二阶矩（方差）来调整学习率，使得优化过程更加平滑。</strong></p><p>Adam 优化器的主要参数包括学习率（learning rate）、β1（一阶矩估计的指数衰减率）、β2（二阶矩估计的指数衰减率）和ε（数值稳定性参数）。以下是这些参数的详细说明：</p><ol><li><strong>学习率（learning rate）（默认值：0.001）：</strong><br>学习率决定了每次参数更新的步长大小。较小的学习率可以使模型收敛更加稳定，但可能会导致训练速度较慢；较大的学习率可以加快训练速度，但可能会导致模型在参数空间中跳跃过大，难以收敛。</li><li><strong>β1（一阶矩估计的指数衰减率）（默认值：0.9）：</strong><br>Adam 优化器使用指数移动平均来估计梯度的一阶矩，β1 参数控制了一阶矩估计的衰减率。较大的 β1 值会使得历史梯度信息对当前梯度的影响更大，使优化过程更加稳定。</li><li><strong>β2（二阶矩估计的指数衰减率）（默认值：0.999）：</strong><br>Adam 优化器还使用指数移动平均来估计梯度的二阶矩，β2 参数控制了二阶矩估计的衰减率。较大的 β2 值会使得历史梯度平方的影响更大，对梯度更新的方向进行调整，有助于降低噪声对优化过程的影响。</li><li><strong>ε（数值稳定性参数）（默认值：1e-7）：</strong><br>ε 参数是为了提高数值稳定性而添加的一个小的常数，防止分母为零。它在计算梯度的二阶矩估计时使用，避免了除零错误。</li></ol><p>这些参数通常会根据具体的任务和数据集进行调整，以获得最佳的性能和收敛速度。通常情况下，Adam 优化器的默认参数值已经在许多任务中表现良好，因此在大多数情况下，不需要进行额外的调整。</p><h2 id=rnn>RNN</h2><p>循环神经网络是具有<strong>隐状态</strong>的神经网络</p><pre><code>        后一步的输入依赖前一步的输出，层间有连接
</code></pre><p>即隐藏层之间的节点不再无连接而是有连接的，并且隐藏层的输入不仅包括输入层的输出还包括上一时刻隐藏层的输出。</p><p><a href=https://blog.csdn.net/bestrivern/article/details/90723524 target=_blank rel="noopener noreffer">https://blog.csdn.net/bestrivern/article/details/90723524</a></p><p><a href=http://www.jokerak.com/deep-learning/5-RNN/#_5-1-%E6%A6%82%E5%BF%B5 target=_blank rel="noopener noreffer">http://www.jokerak.com/deep-learning/5-RNN/#_5-1-%E6%A6%82%E5%BF%B5</a></p><p>LSTM</p><p>seq2seq</p><h2 id=cnn与rnn区别>CNN与RNN区别</h2><h3 id=输入数据类型>输入数据类型</h3><ul><li><strong>CNN</strong>：<ul><li>主要处理二维或三维数据，如图像（高度、宽度、颜色通道）和视频帧。</li><li>通过卷积层提取空间特征，适合于图像分类、目标检测等任务。</li></ul></li><li><strong>RNN</strong>：<ul><li>主要处理一维序列数据，如时间序列、文本等。</li><li><strong>能够处理变长输入</strong>，适合于自然语言处理、语音识别等任务。</li></ul></li></ul><h3 id=训练和计算>训练和计算</h3><ul><li><strong>CNN</strong>：<ul><li><strong>并行计算较为容易，因为卷积操作可以在不同的区域同时进行</strong>。</li><li>通常使用批量训练，加速计算过程。</li></ul></li><li><strong>RNN</strong>：<ul><li>训练过程通常是顺序的，因为<strong>每个时间步的输出依赖于前一个时间步的状态，难以并行化。</strong></li><li>更<strong>容易出现梯度消失或梯度爆炸</strong>的问题，通常使用长短期记忆网络（LSTM）或门控递归单元（GRU）来缓解这些问题。</li></ul></li></ul><h2 id=cnn与mlp>CNN与MLP</h2><ol><li><strong>结构：</strong><ul><li>多层感知机（MLP）是一种基本的前馈神经网络结构，由<strong>多个全连接层组成，每个神经元与上一层的所有神经元相连</strong>。</li><li>卷积神经网络（CNN）是一种专门用于处理网格结构数据（如图像）的神经网络结构，其中包含卷积层、池化层和全连接层等。卷积层通过卷积操作来提取局部特征，池化层通过池化操作来减少特征图的大小和参数数量。</li></ul></li><li><strong>输入数据的结构：</strong><ul><li>多层感知机（MLP）通常用于处理一维的数据，例如序列数据、文本数据等。</li><li>卷积神经网络（CNN）通常用于处理二维的数据，例如图像数据，但也可以扩展到处理三维的数据，例如视频数据。</li></ul></li><li><strong>参数共享：</strong><ul><li>在卷积神经网络（CNN）中，<strong>卷积操作具有参数共享的特性</strong>，即卷积核在整个特征图上移动时所使用的参数是相同的，这大大减少了需要学习的参数数量，提高了模型的效率和泛化能力。</li><li>在多层感知机（MLP）中，每个神经元与上一层的所有神经元相连，参数独立，没有参数共享的概念。</li></ul></li><li><strong>特征提取：</strong><ul><li>卷积神经网络（CNN）通过卷积操作和池化操作来逐层提取输入数据的特征，从而学习到数据的局部和全局特征，具有良好的特征提取能力。</li><li>多层感知机（MLP）则是通过全连接层来<strong>对输入数据进行线性变换和非线性变换</strong>，对于复杂的结构化数据，需要更多的层和参数来进行学习，且往往需要更多的数据来进行训练。不激活时相当于是多个线性变换</li></ul></li></ol><h3 id=上采样反池化反卷积>上采样、反池化、反卷积</h3><p><a href=https://blog.csdn.net/A_a_ron/article/details/79181108 target=_blank rel="noopener noreffer">https://blog.csdn.net/A_a_ron/article/details/79181108</a></p><p><img class=lazyload src=/svg/loading.min.svg data-src=/posts/algorithm_knowledge_base/6_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/image%2057.png data-srcset="/posts/algorithm_knowledge_base/6_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/image%2057.png, /posts/algorithm_knowledge_base/6_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/image%2057.png 1.5x, /posts/algorithm_knowledge_base/6_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/image%2057.png 2x" data-sizes=auto alt=/posts/algorithm_knowledge_base/6_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/image%2057.png title=image.png width=674 height=863></p><p><a href=https://blog.csdn.net/yq_forever/article/details/101169771 target=_blank rel="noopener noreffer">https://blog.csdn.net/yq_forever/article/details/101169771</a></p><p>上采样 最近邻插值、双线性插值等</p><p>反池化 需要池化索引</p><p>反卷积 参数需要学习</p><h3 id=bnln>BN、LN</h3><ul><li><strong>Batch Normalization (BN)</strong>：<strong>在批次维度上进行归一化</strong>，即对每个 mini-batch 内的<strong>所有样本的同一特征进行归一化</strong>。公式中使用的是每个 mini-batch 内特征的均值和方差。适合卷积神经网络（CNN）等批量大小比较大的场景。<strong>特征归一化</strong></li><li><strong>Layer Normalization (LN)</strong>：在层维度上进行归一化，即对<strong>每个样本的所有特征进行归一化</strong>。公式中使用的是单个样本的所有特征的均值和方差。适合循环神经网络（RNN）和 Transformer 等批量大小较小或动态性强的场景。<strong>样本归一化</strong></li></ul></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span>Updated on 2025-04-08</span></div></div><div class=post-info-line><div class=post-info-md></div><div class=post-info-share><span><a href=javascript:void(0); title="Share on Twitter" data-sharer=twitter data-url=https://fjyu95.github.io/posts/algorithm_knowledge_base/6_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/ data-title=深度学习 data-hashtags="deep learning"><i class="fab fa-twitter fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Facebook" data-sharer=facebook data-url=https://fjyu95.github.io/posts/algorithm_knowledge_base/6_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/ data-hashtag="deep learning"><i class="fab fa-facebook-square fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Hacker News" data-sharer=hackernews data-url=https://fjyu95.github.io/posts/algorithm_knowledge_base/6_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/ data-title=深度学习><i class="fab fa-hacker-news fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Line" data-sharer=line data-url=https://fjyu95.github.io/posts/algorithm_knowledge_base/6_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/ data-title=深度学习><i data-svg-src=https://cdn.jsdelivr.net/npm/simple-icons@7.3.0/icons/line.svg aria-hidden=true></i></a><a href=javascript:void(0); title="Share on 微博" data-sharer=weibo data-url=https://fjyu95.github.io/posts/algorithm_knowledge_base/6_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/ data-title=深度学习><i class="fab fa-weibo fa-fw" aria-hidden=true></i></a></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fas fa-tags fa-fw" aria-hidden=true></i>&nbsp;<a href=/tags/deep-learning/>Deep Learning</a></section><section><span><a href=javascript:void(0); onclick=window.history.back()>Back</a></span>&nbsp;|&nbsp;<span><a href=/>Home</a></span></section></div><div class=post-nav><a href=/posts/algorithm_knowledge_base/7_vslam/ class=prev rel=prev title=视觉slam><i class="fas fa-angle-left fa-fw" aria-hidden=true></i>视觉slam</a>
<a href=/posts/algorithm_knowledge_base/5_%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9/ class=next rel=next title=模型压缩>模型压缩<i class="fas fa-angle-right fa-fw" aria-hidden=true></i></a></div></div></article></div></main><footer class=footer><div class=footer-container><div class=footer-line>Powered by <a href=https://gohugo.io/ target=_blank rel="noopener noreffer" title="Hugo 0.145.0">Hugo</a> | Theme - <a href=https://github.com/dillonzq/LoveIt target=_blank rel="noopener noreffer" title="LoveIt 0.3.0"><i class="far fa-kiss-wink-heart fa-fw" aria-hidden=true></i> LoveIt</a></div><div class=footer-line itemscope itemtype=http://schema.org/CreativeWork><i class="far fa-copyright fa-fw" aria-hidden=true></i><span itemprop=copyrightYear>2020 - 2025</span><span class=author itemprop=copyrightHolder>&nbsp;<a href=https://fjyu95.github.io/ target=_blank>fjyu95</a></span></div></div></footer></div><div id=fixed-buttons><a href=# id=back-to-top class=fixed-button title="Back to Top"><i class="fas fa-arrow-up fa-fw" aria-hidden=true></i>
</a><a href=# id=view-comments class=fixed-button title="View Comments"><i class="fas fa-comment fa-fw" aria-hidden=true></i></a></div><script type=text/javascript src=https://cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/sharer.js@0.5.1/sharer.min.js></script><script type=text/javascript>window.config={code:{copyTitle:"Copy to clipboard",maxShownLines:50},comment:{}}</script><script type=text/javascript src=/js/theme.min.js></script></body></html>
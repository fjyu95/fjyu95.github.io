<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="noodp"><title>3DGS - FangjieYu‘s site</title><meta name=Description content="This is my cool site"><meta property="og:url" content="https://fjyu95.github.io/posts/3dgs-138e4020f06080e7b9ced1f2074982d5/">
<meta property="og:site_name" content="FangjieYu‘s site"><meta property="og:title" content="3DGS"><meta property="og:description" content="GitHub - graphdeco-inria/gaussian-splatting: Original reference implementation of “3D Gaussian Splatting for Real-Time Radiance Field Rendering”
http://www.yindaheng98.top/%E5%9B%BE%E5%BD%A2%E5%AD%A6/3DGaussianSplatting.html#%E5%A6%82%E4%BD%95%E5%AE%9A%E4%B9%89gaussian%E7%82%B9
http://www.yindaheng98.top/%E5%9B%BE%E5%BD%A2%E5%AD%A6/%E7%90%83%E8%B0%90%E7%B3%BB%E6%95%B0.html#%E7%90%83%E9%9D%A2%E9%AB%98%E6%96%AF%EF%BC%88spherical-gaussian%EF%BC%89
http://www.yindaheng98.top/%E5%9B%BE%E5%BD%A2%E5%AD%A6/3D%E9%AB%98%E6%96%AF%E6%95%B0%E5%AD%A6%E6%8E%A8%E5%AF%BC.html#derivation-of-sampling
https://www.find.org.tw/index/tech_obser/browse/7eaa3e8ffc7876e74509befd5ed50b8a/
3DGS使用隨機梯度下降法進行訓練。步驟如下：
SfM Points：首先利用運動恢復結構技術（Structure from Motion，SfM），從一組2D相片中找到不同照片中疊合的特徵點，來估計出初步的3D點雲及相機位置，程式碼中是透過調用COLMAP庫來完成。 Initialization：透過SfM獲得的點雲及數據進行初始化，算出各個高斯橢球的位置、形狀、顏色、透明度等參數。 Projection：將每一顆高斯橢球，根據它們與各個相機位置的距離（深度），由近到遠排序進行投影；程式碼中是將中心點座標去跟變換矩陣、協方差矩陣等數據進行運算，就能將三維空間中的高斯分布轉換到二維的平面上。 Differentiable Tile Rasterizer：透過可微光柵化渲染（即前文提及的「Splatting」得到一張張2D圖像（即流程上的「Image」）。多个3d高斯点颜色合成吗？ Adaptive Density Control：根據步驟4.所獲得之渲染2D圖像（即流程上的「Image」）與Ground Truth圖像（原先上傳的照片）之間的差異，計算出loss值，並將loss值沿藍色箭頭方向回傳。藍色箭頭向上即更新3D高斯橢球的參數，向下則送入自適應密度控制來更新高斯橢球的密度，如將小顆的高斯克隆成兩顆、將大顆的高斯分裂兩小顆等，以更好地擬合照片中的細節。 全文翻译 https://blog.csdn.net/m0_38068229/article/details/137171860"><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-11-08T00:00:00+00:00"><meta property="article:modified_time" content="2025-04-09T00:00:00+00:00"><meta property="article:tag" content="3d"><meta property="article:tag" content="Gaussian Splatting"><meta name=twitter:card content="summary"><meta name=twitter:title content="3DGS"><meta name=twitter:description content="GitHub - graphdeco-inria/gaussian-splatting: Original reference implementation of “3D Gaussian Splatting for Real-Time Radiance Field Rendering”
http://www.yindaheng98.top/%E5%9B%BE%E5%BD%A2%E5%AD%A6/3DGaussianSplatting.html#%E5%A6%82%E4%BD%95%E5%AE%9A%E4%B9%89gaussian%E7%82%B9
http://www.yindaheng98.top/%E5%9B%BE%E5%BD%A2%E5%AD%A6/%E7%90%83%E8%B0%90%E7%B3%BB%E6%95%B0.html#%E7%90%83%E9%9D%A2%E9%AB%98%E6%96%AF%EF%BC%88spherical-gaussian%EF%BC%89
http://www.yindaheng98.top/%E5%9B%BE%E5%BD%A2%E5%AD%A6/3D%E9%AB%98%E6%96%AF%E6%95%B0%E5%AD%A6%E6%8E%A8%E5%AF%BC.html#derivation-of-sampling
https://www.find.org.tw/index/tech_obser/browse/7eaa3e8ffc7876e74509befd5ed50b8a/
3DGS使用隨機梯度下降法進行訓練。步驟如下：
SfM Points：首先利用運動恢復結構技術（Structure from Motion，SfM），從一組2D相片中找到不同照片中疊合的特徵點，來估計出初步的3D點雲及相機位置，程式碼中是透過調用COLMAP庫來完成。 Initialization：透過SfM獲得的點雲及數據進行初始化，算出各個高斯橢球的位置、形狀、顏色、透明度等參數。 Projection：將每一顆高斯橢球，根據它們與各個相機位置的距離（深度），由近到遠排序進行投影；程式碼中是將中心點座標去跟變換矩陣、協方差矩陣等數據進行運算，就能將三維空間中的高斯分布轉換到二維的平面上。 Differentiable Tile Rasterizer：透過可微光柵化渲染（即前文提及的「Splatting」得到一張張2D圖像（即流程上的「Image」）。多个3d高斯点颜色合成吗？ Adaptive Density Control：根據步驟4.所獲得之渲染2D圖像（即流程上的「Image」）與Ground Truth圖像（原先上傳的照片）之間的差異，計算出loss值，並將loss值沿藍色箭頭方向回傳。藍色箭頭向上即更新3D高斯橢球的參數，向下則送入自適應密度控制來更新高斯橢球的密度，如將小顆的高斯克隆成兩顆、將大顆的高斯分裂兩小顆等，以更好地擬合照片中的細節。 全文翻译 https://blog.csdn.net/m0_38068229/article/details/137171860"><meta name=application-name content="My cool site"><meta name=apple-mobile-web-app-title content="My cool site"><meta name=theme-color content="#ffffff"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://fjyu95.github.io/posts/3dgs-138e4020f06080e7b9ced1f2074982d5/><link rel=prev href=https://fjyu95.github.io/posts/summary-127e4020f060808da8e0f23caf646e39/><link rel=next href=https://fjyu95.github.io/posts/%D1%85%D0%B0%D1%86%D1%803d%D1%89%D0%B7%D0%BD%D1%85%D1%87%D1%87%D1%8F-139e4020f060802d9629dfad7d7f0b5c/><link rel=stylesheet href=/css/style.min.css><link rel=preload href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css></noscript><link rel=preload href=https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css></noscript><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"3DGS","inLanguage":"en-us","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/fjyu95.github.io\/posts\/3dgs-138e4020f06080e7b9ced1f2074982d5\/"},"genre":"posts","keywords":"3d, gaussian splatting","wordcount":7341,"url":"https:\/\/fjyu95.github.io\/posts\/3dgs-138e4020f06080e7b9ced1f2074982d5\/","datePublished":"2024-11-08T00:00:00+00:00","dateModified":"2025-04-09T00:00:00+00:00","publisher":{"@type":"Organization","name":""},"author":{"@type":"Person","name":"fjyu95"},"description":""}</script></head><body data-header-desktop=fixed data-header-mobile=auto><script type=text/javascript>(window.localStorage&&localStorage.getItem("theme")?localStorage.getItem("theme")==="dark":"auto"==="auto"?window.matchMedia("(prefers-color-scheme: dark)").matches:"auto"==="dark")&&document.body.setAttribute("theme","dark")</script><div id=mask></div><div class=wrapper><header class=desktop id=header-desktop><div class=header-wrapper><div class=header-title><a href=/ title="FangjieYu‘s site">fjyu95</a></div><div class=menu><div class=menu-inner><a class=menu-item href=/posts/>posts </a><a class=menu-item href=/tags/>tags </a><a class=menu-item href=/categories/>category </a><span class="menu-item delimiter"></span><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme">
<i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></div></div></div></header><header class=mobile id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title="FangjieYu‘s site">fjyu95</a></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><div class=menu id=menu-mobile><a class=menu-item href=/posts/ title>posts</a><a class=menu-item href=/tags/ title>tags</a><a class=menu-item href=/categories/ title>category</a><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme">
<i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></div></div></header><main class=main><div class=container><div class=toc id=toc-auto><h2 class=toc-title>Contents</h2><div class=toc-content id=toc-content-auto></div></div><article class="page single"><h1 class="single-title animate__animated animate__flipInX">3DGS</h1><div class=post-meta><div class=post-meta-line><span class=post-author><a href=https://fjyu95.github.io/ title=Author target=_blank rel="noopener noreffer author" class=author><i class="fas fa-user-circle fa-fw" aria-hidden=true></i>fjyu95</a></span>&nbsp;<span class=post-category>included in <a href=/categories/%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA/><i class="far fa-folder fa-fw" aria-hidden=true></i>三维重建</a></span></div><div class=post-meta-line><i class="far fa-calendar-alt fa-fw" aria-hidden=true></i>&nbsp;<time datetime=2024-11-08>2024-11-08</time>&nbsp;<i class="fas fa-pencil-alt fa-fw" aria-hidden=true></i>&nbsp;7341 words&nbsp;
<i class="far fa-clock fa-fw" aria-hidden=true></i>&nbsp;15 minutes&nbsp;</div></div><div class="details toc" id=toc-static data-kept><div class="details-summary toc-title"><span>Contents</span>
<span><i class="details-icon fas fa-angle-right" aria-hidden=true></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#全文翻译>全文翻译</a></li><li><a href=#算法原理>算法原理</a><ul><li><ul><li><a href=#1-高斯分布表示3d点云>1. <strong>高斯分布表示3D点云</strong></a></li><li><a href=#2-投影与融合>2. <strong>投影与融合</strong></a></li><li><a href=#3-权重与透明度>3. <strong>权重与透明度</strong></a></li><li><a href=#4-优化与加速>4. <strong>优化与加速</strong></a></li><li><a href=#5-应用场景>5. <strong>应用场景</strong></a></li><li><a href=#与nerf的对比>与NeRF的对比</a></li></ul></li></ul></li><li><a href=#算法理解>算法理解</a></li><li><a href=#源码解读>源码解读</a></li><li><a href=#qa>Q&amp;A</a><ul><li><a href=#是否包含神经网络>是否包含神经网络</a><ul><li><a href=#1-传统-3d-gaussian-splatting-的原理><strong>1. 传统 3D Gaussian Splatting 的原理</strong></a></li><li><a href=#1-核心流程><strong>(1) 核心流程</strong></a></li><li><a href=#2-技术特点><strong>(2) 技术特点</strong></a></li><li><a href=#2-与神经网络的结合方向><strong>2. 与神经网络的结合方向</strong></a></li><li><a href=#1-参数优化加速><strong>(1) 参数优化加速</strong></a></li><li><a href=#2-动态场景建模><strong>(2) 动态场景建模</strong></a></li><li><a href=#3-与-nerf-的对比><strong>3. 与 NeRF 的对比</strong></a></li><li><a href=#4-典型应用案例><strong>4. 典型应用案例</strong></a></li><li><a href=#1-纯-gaussian-splatting><strong>(1) 纯 Gaussian Splatting</strong></a></li><li><a href=#2-神经网络增强版><strong>(2) 神经网络增强版</strong></a></li><li><a href=#总结><strong>总结</strong></a></li></ul></li><li><a href=#相机视角表示>相机视角表示</a><ul><li><a href=#gpt>GPT</a></li><li><a href=#相机视角的基本表示>相机视角的基本表示</a></li><li><a href=#在-3d-gaussian-splatting-中的具体实现>在 3D Gaussian Splatting 中的具体实现</a></li><li><a href=#相机视角表示的实际应用>相机视角表示的实际应用</a></li><li><a href=#视角表示与深度学习中的结合>视角表示与深度学习中的结合</a></li><li><a href=#总结-1>总结</a></li><li><a href=#code>code</a></li></ul></li></ul></li><li><a href=#查看器>查看器</a></li><li><a href=#思考>思考</a><ul><li><a href=#3dgs-vd-航空摄影测量>3DGS v.d. 航空摄影测量</a><ul><li><a href=#3dgs3d-gaussian-splatting与传统航空摄影测量建模效果对比>3DGS（3D Gaussian Splatting）与传统航空摄影测量建模效果对比</a></li><li><a href=#1-核心原理对比><strong>1. 核心原理对比</strong></a></li><li><a href=#2-建模效果对比><strong>2. 建模效果对比</strong></a></li><li><a href=#1-几何精度><strong>(1) 几何精度</strong></a></li><li><a href=#2-视觉效果><strong>(2) 视觉效果</strong></a></li><li><a href=#3-处理效率><strong>(3) 处理效率</strong></a></li><li><a href=#3-适用场景对比><strong>3. 适用场景对比</strong></a></li><li><a href=#4-优缺点总结><strong>4. 优缺点总结</strong></a></li><li><a href=#5-未来趋势><strong>5. 未来趋势</strong></a></li><li><a href=#选择建议><strong>选择建议</strong></a></li></ul></li></ul></li></ul></nav></div></div><div class=content id=content><p><a href="https://github.com/graphdeco-inria/gaussian-splatting?tab=readme-ov-file" target=_blank rel="noopener noreffer">GitHub - graphdeco-inria/gaussian-splatting: Original reference implementation of &ldquo;3D Gaussian Splatting for Real-Time Radiance Field Rendering&rdquo;</a></p><p><a href=http://www.yindaheng98.top/%E5%9B%BE%E5%BD%A2%E5%AD%A6/3DGaussianSplatting.html#%E5%A6%82%E4%BD%95%E5%AE%9A%E4%B9%89gaussian%E7%82%B9 target=_blank rel="noopener noreffer">http://www.yindaheng98.top/%E5%9B%BE%E5%BD%A2%E5%AD%A6/3DGaussianSplatting.html#%E5%A6%82%E4%BD%95%E5%AE%9A%E4%B9%89gaussian%E7%82%B9</a></p><p><a href=http://www.yindaheng98.top/%E5%9B%BE%E5%BD%A2%E5%AD%A6/%E7%90%83%E8%B0%90%E7%B3%BB%E6%95%B0.html#%E7%90%83%E9%9D%A2%E9%AB%98%E6%96%AF%EF%BC%88spherical-gaussian%EF%BC%89 target=_blank rel="noopener noreffer">http://www.yindaheng98.top/%E5%9B%BE%E5%BD%A2%E5%AD%A6/%E7%90%83%E8%B0%90%E7%B3%BB%E6%95%B0.html#%E7%90%83%E9%9D%A2%E9%AB%98%E6%96%AF%EF%BC%88spherical-gaussian%EF%BC%89</a></p><p><a href=http://www.yindaheng98.top/%E5%9B%BE%E5%BD%A2%E5%AD%A6/3D%E9%AB%98%E6%96%AF%E6%95%B0%E5%AD%A6%E6%8E%A8%E5%AF%BC.html#derivation-of-sampling target=_blank rel="noopener noreffer">http://www.yindaheng98.top/%E5%9B%BE%E5%BD%A2%E5%AD%A6/3D%E9%AB%98%E6%96%AF%E6%95%B0%E5%AD%A6%E6%8E%A8%E5%AF%BC.html#derivation-of-sampling</a></p><p><a href=https://www.find.org.tw/index/tech_obser/browse/7eaa3e8ffc7876e74509befd5ed50b8a/ target=_blank rel="noopener noreffer">https://www.find.org.tw/index/tech_obser/browse/7eaa3e8ffc7876e74509befd5ed50b8a/</a></p><p><img class=lazyload src=/svg/loading.min.svg data-src=3DGS%20138e4020f06080e7b9ced1f2074982d5/image.png data-srcset="3DGS%20138e4020f06080e7b9ced1f2074982d5/image.png, 3DGS%20138e4020f06080e7b9ced1f2074982d5/image.png 1.5x, 3DGS%20138e4020f06080e7b9ced1f2074982d5/image.png 2x" data-sizes=auto alt=3DGS%20138e4020f06080e7b9ced1f2074982d5/image.png title=image.png></p><p>3DGS使用隨機梯度下降法進行訓練。步驟如下：</p><ol><li>SfM Points：首先利用運動恢復結構技術（Structure from Motion，SfM），從一組2D相片中找到不同照片中疊合的特徵點，來估計出初步的3D點雲及相機位置，程式碼中是透過調用COLMAP庫來完成。</li><li>Initialization：透過SfM獲得的點雲及<strong>數據進行初始化</strong>，算出各個高斯橢球的位置、形狀、顏色、透明度等參數。</li><li>Projection：將每一顆高斯橢球，根據它們與各個相機位置的距離（深度），由近到遠排序進行投影；程式碼中是將中心點座標去跟變換矩陣、協方差矩陣等數據進行運算，就能<strong>將三維空間中的高斯分布轉換到二維的平面上</strong>。</li><li>Differentiable Tile Rasterizer：透過<strong>可微光柵化渲染</strong>（即前文提及的「Splatting」得到一張張2D圖像（即流程上的「Image」）。多个3d高斯点颜色合成吗？</li><li>Adaptive Density Control：根據步驟4.所獲得之渲染2D圖像（即流程上的「Image」）與Ground Truth圖像（原先上傳的照片）之間的差異，計算出loss值，並將loss值沿藍色箭頭方向回傳。藍色箭頭<strong>向上即更新3D高斯橢球的參數</strong>，向下則送入自<strong>適應密度控制來更新高斯橢球的密度</strong>，如將小顆的高斯克隆成兩顆、將大顆的高斯分裂兩小顆等，以更好地擬合照片中的細節。</li></ol><h1 id=全文翻译>全文翻译</h1><p><a href=https://blog.csdn.net/m0_38068229/article/details/137171860 target=_blank rel="noopener noreffer">https://blog.csdn.net/m0_38068229/article/details/137171860</a></p><p><a href=https://blog.csdn.net/m0_52065936/article/details/132805104 target=_blank rel="noopener noreffer">https://blog.csdn.net/m0_52065936/article/details/132805104</a></p><h1 id=算法原理>算法原理</h1><p>from chatgpt</p><p>3D Gaussian Splatting是一种用于三维重建的算法，其主要思路是使用三维高斯分布来表示场景中的点，实现高效且连续的场景重建。与传统的NeRF（Neural Radiance Fields）等方法相比，3D Gaussian Splatting在渲染效率和重建速度方面具有显著优势。以下是3D Gaussian Splatting算法的核心原理：</p><h3 id=1-高斯分布表示3d点云>1. <strong>高斯分布表示3D点云</strong></h3><ul><li>在3D Gaussian Splatting中，场景中的每个点由一个三维的高斯分布来表示，这个分布通常由位置、颜色、大小和方向参数来描述。</li><li>与点云中的离散点不同，3D高斯分布可以形成一个连续的概率分布函数（PDF），使得这些点具有空间延展性和模糊性。</li><li>通过这种分布，渲染时可以避免单独的采样点，仅需计算每个像素位置上高斯分布的叠加效果。</li></ul><h3 id=2-投影与融合>2. <strong>投影与融合</strong></h3><ul><li>在渲染阶段，3D高斯分布的中心点会投影到图像平面，并生成二维的“splat”（点状）表示。</li><li>这些splat在像素平面上叠加，并<strong>根据其权重和透明度信息融合，形成最后的渲染结果</strong>。</li><li>这种投影方法能够将高斯体积投影到图像平面，减少了体积渲染中的高维采样步骤。</li></ul><h3 id=3-权重与透明度>3. <strong>权重与透明度</strong></h3><ul><li><p>3D Gaussian Splatting<strong>使用alpha合成方法来处理不同的透明度和颜色信息</strong>。</p><p>透明度是根据和椭圆中心的距离直接计算的吗？</p></li><li><p>每个高斯体积根据其距离、方向和视角影响渲染中的权重和透明度，使得最终的图像具有真实的深度感和透明效果。</p></li></ul><h3 id=4-优化与加速>4. <strong>优化与加速</strong></h3><ul><li>该算法通常结合了多层次优化技术，包括多尺度表征、渐进式细化等。</li><li>在具体实现中，还可以使用GPU加速以处理高分辨率点云和快速渲染，使得实时应用成为可能。</li></ul><h3 id=5-应用场景>5. <strong>应用场景</strong></h3><ul><li>3D Gaussian Splatting适用于场景重建、虚拟现实（VR）渲染、视频游戏以及电影制作等需要高质量3D渲染的领域。</li><li>它的渲染速度相对较快，并且具有较强的连续性，非常适合需要快速场景预览和即时效果的应用。</li></ul><h3 id=与nerf的对比>与NeRF的对比</h3><ul><li>3D Gaussian Splatting算法与NeRF相比，在<strong>渲染效率上更具优势</strong>，因为NeRF需要对整个三维空间<strong>进行稠密采样</strong>，而高斯分布可以直接形成连续表示。</li><li>相较于NeRF的体积渲染，3D Gaussian Splatting在减少采样点的同时仍能保留图像质量，因此更适合实时渲染和场景重建任务。</li></ul><p>通过3D Gaussian Splatting的高效三维点表示与投影融合策略，这一算法能够在保持较高图像质量的同时，实现快速且连续的3D场景渲染。</p><h1 id=算法理解>算法理解</h1><p><strong>3D Gaussian点定义：</strong></p><p><img class=lazyload src=/svg/loading.min.svg data-src=3DGS%20138e4020f06080e7b9ced1f2074982d5/image%201.png data-srcset="3DGS%20138e4020f06080e7b9ced1f2074982d5/image%201.png, 3DGS%20138e4020f06080e7b9ced1f2074982d5/image%201.png 1.5x, 3DGS%20138e4020f06080e7b9ced1f2074982d5/image%201.png 2x" data-sizes=auto alt=3DGS%20138e4020f06080e7b9ced1f2074982d5/image%201.png title=image.png></p><p>关键参数：</p><p>x：空间位置</p><pre><code> **增加或减少Gaussian点，利用位置梯度**
</code></pre><p><img class=lazyload src=/svg/loading.min.svg data-src=3DGS%20138e4020f06080e7b9ced1f2074982d5/image%202.png data-srcset="3DGS%20138e4020f06080e7b9ced1f2074982d5/image%202.png, 3DGS%20138e4020f06080e7b9ced1f2074982d5/image%202.png 1.5x, 3DGS%20138e4020f06080e7b9ced1f2074982d5/image%202.png 2x" data-sizes=auto alt=3DGS%20138e4020f06080e7b9ced1f2074982d5/image%202.png title=image.png></p><p>Σ：三维高斯分布的协方差矩阵，表示椭球的缩放和旋转</p><pre><code>  Gaussian点的参数只有Σ，其决定了椭球的形状和对称轴方向，不决定椭球的位置。

  训练过程就是用渲染图和原图比较计算视野中高斯点的矩阵Σ的梯度，然后梯度下降调Σ。
</code></pre><p>Σ=<em>RSSTRT</em></p><p><em>α：不透明度（中心为1，越往四周越透明）</em></p><p>Color：用<a href=http://www.yindaheng98.top/%E5%9B%BE%E5%BD%A2%E5%AD%A6/%E7%90%83%E8%B0%90%E7%B3%BB%E6%95%B0.html target=_blank rel="noopener noreffer"><strong>球谐系数</strong></a>来表示每个高斯的颜色 ，<strong>不同视角颜色不同</strong>。16X3（RGB）矩阵</p><pre><code>       球谐系数是一组表示任意函数的“基函数”的系数，（`sh_degree`+1）^2*3
</code></pre><p><img class=lazyload src=/svg/loading.min.svg data-src=3DGS%20138e4020f06080e7b9ced1f2074982d5/image%203.png data-srcset="3DGS%20138e4020f06080e7b9ced1f2074982d5/image%203.png, 3DGS%20138e4020f06080e7b9ced1f2074982d5/image%203.png 1.5x, 3DGS%20138e4020f06080e7b9ced1f2074982d5/image%203.png 2x" data-sizes=auto alt=3DGS%20138e4020f06080e7b9ced1f2074982d5/image%203.png title=image.png></p><p><a href=https://www.51cto.com/article/799122.html target=_blank rel="noopener noreffer">https://www.51cto.com/article/799122.html</a></p><p>3DGS算法中定义一个3D Gaussian的参数：</p><ul><li><strong>Position (Mean μ)</strong>: location (XYZ)</li><li><strong>Covariance Matrix (Σ)</strong>: rotation and scaling</li><li><strong>Opacity (<em><strong>α</strong></em>)</strong>: Transparency，这个参数会在alpha blending阶段时与相乘</li><li><strong>Color (RGB) or Spherical Harmonics (SH) coefficients</strong></li></ul><p>使用随机梯度下降SGD，对Mean、Covariance Matrix、α、Color进行参数优化。</p><p><img class=lazyload src=/svg/loading.min.svg data-src=3DGS%20138e4020f06080e7b9ced1f2074982d5/image%204.png data-srcset="3DGS%20138e4020f06080e7b9ced1f2074982d5/image%204.png, 3DGS%20138e4020f06080e7b9ced1f2074982d5/image%204.png 1.5x, 3DGS%20138e4020f06080e7b9ced1f2074982d5/image%204.png 2x" data-sizes=auto alt=3DGS%20138e4020f06080e7b9ced1f2074982d5/image%204.png title=image.png></p><ul><li>使用SfM(Structure from Motion)初步创建一组point cloud</li><li>将步骤1创建的point cloud转换为3D Gaussian</li><li>使用SGD训练。训练过程是将3D Gaussian使用differentiable Gaussian rasterization方法rasterize到图像上，<strong>通过生成图像与真实图像来计算loss值，调整参数，实现自动densification调整</strong></li><li>Differentiable Gaussian Rasterization</li></ul><p><strong>可以简单将其概括为如下几步：</strong></p><ul><li><p>录一段视频或者拍一组不同角度的照片，用一些技术（例如SfM）估计点云。或者直接随机初始化一组点云。</p></li><li><p>点云中的每一个点，代表着一个三维的高斯分布，所以除了点的位置（均值）以外，还有协方差，以及不透明度，以及颜色（球谐系数）。直观可以理解为一个”椭球体“。</p></li><li><p>将这些椭球体沿着特定的角度投影到对应位姿所在的投影平面上，这一步也叫“splatting“，一个椭球体投影到平面上会得到一个椭圆（代码实现时其实是以长轴为直径的圆），然后通过计算待求解像素和椭圆中心的距离，我们可以得到不透明度（离的越近，说明越不透明）。每个椭球体又各自代表自己的颜色，这是距离无关的。于是就可以进行alpha compositing，来合成颜色。然后快速的对所有像素做这样的计算，这被称作”快速可微光栅化“。</p></li><li><p>于是可以得到整个图片，再和ground truth比较，得到损失，然后梯度反传，随机梯度下降，进行优化。</p><p><img class=lazyload src=/svg/loading.min.svg data-src=3DGS%20138e4020f06080e7b9ced1f2074982d5/image%205.png data-srcset="3DGS%20138e4020f06080e7b9ced1f2074982d5/image%205.png, 3DGS%20138e4020f06080e7b9ced1f2074982d5/image%205.png 1.5x, 3DGS%20138e4020f06080e7b9ced1f2074982d5/image%205.png 2x" data-sizes=auto alt=3DGS%20138e4020f06080e7b9ced1f2074982d5/image%205.png title=image.png></p></li></ul><h1 id=源码解读>源码解读</h1><p><a href=https://www.cnblogs.com/sxq-blog/p/18224667 target=_blank rel="noopener noreffer">https://www.cnblogs.com/sxq-blog/p/18224667</a></p><h1 id=qa>Q&amp;A</h1><p>1、3d高斯点如何映射到的2d平面？</p><p>透视投影 近大远小，将远处的平面挤压之后再做正交投影</p><p>正交投影 相机无限远的特殊情况，渲染时直接把z轴扔掉</p><p>2、2d上同一个像素点覆盖的时候3d高斯点颜色如何合成？</p><p>球谐系数的使用 不同视角有不同颜色值</p><p><strong>一组球面上的基函数，把高斯块上面的点的颜色建模成了一个球谐函数</strong></p><p><strong>α-blending</strong></p><p>3、16*16瓦片的作用</p><p><strong>Optimized GPU Rendering</strong></p><ol><li>将屏幕分为16*16的tiles，保留99%在视锥里的Gaussians，</li><li>给每个Gaussian一个key=depth+ID，然后将Gaussians进行GPU Radix Sort，得到了每个tile里根据depth排序的Gaussian列表，</li><li>每个tile分配thread block，每个block先load到shared memory，对于每个pixel，累积color和alpha，达到饱和时停止。</li></ol><p>4、损失函数及优化算法？</p><pre><code> The loss function is L1 combined with a D-SSIM term:
</code></pre><p><img class=lazyload src=/svg/loading.min.svg data-src=3DGS%20138e4020f06080e7b9ced1f2074982d5/image%206.png data-srcset="3DGS%20138e4020f06080e7b9ced1f2074982d5/image%206.png, 3DGS%20138e4020f06080e7b9ced1f2074982d5/image%206.png 1.5x, 3DGS%20138e4020f06080e7b9ced1f2074982d5/image%206.png 2x" data-sizes=auto alt=3DGS%20138e4020f06080e7b9ced1f2074982d5/image%206.png title=image.png></p><p>也可以包括<code>Depth regularization</code></p><p>优化算法：</p><p><strong>Adam计算梯度的一阶矩（均值）和二阶矩（方差）来调整学习率</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>  <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>optimizer_type</span> <span class=o>==</span> <span class=s2>&#34;default&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>		  <span class=bp>self</span><span class=o>.</span><span class=n>optimizer</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>optim</span><span class=o>.</span><span class=n>Adam</span><span class=p>(</span><span class=n>l</span><span class=p>,</span> <span class=n>lr</span><span class=o>=</span><span class=mf>0.0</span><span class=p>,</span> <span class=n>eps</span><span class=o>=</span><span class=mf>1e-15</span><span class=p>)</span>
</span></span></code></pre></div><h2 id=是否包含神经网络>是否包含神经网络</h2><p>3D Gaussian Splatting<strong>本身并不包含神经网络。它是一种基于物理的渲染技术</strong>，用于在3D场景中模拟光线的传播和散射。在这种技术中，物体被表示为大量的小颗粒（splat），每个颗粒都对应一个高斯函数，用于描述光线的分布和散射。<br>然而，3D Gaussian Splatting可以与神经网络结合使用，以实现更高效、更真实的渲染效果。例如，可以使用神经网络来学习场景的几何结构、材质属性和光照条件，从而生成更准确的高斯函数参数。此外，神经网络还可以用于加速渲染过程，通过学习输入图像和输出图像之间的映射关系，实现实时或近实时的渲染。<br>总的来说，3D Gaussian Splatting本身不包含神经网络，但可以与神经网络结合使用，以提升渲染效果和效率。</p><p>在标准的 <strong>3D Gaussian Splatting</strong> 方法中，<strong>并未直接使用神经网络</strong>。它的核心是一种基于数学建模和传统计算机图形学的渲染技术，通过显式的高斯分布建模和投影来实现高效的点云或稀疏体积数据渲染。不过，近年来一些改进方法可能会结合神经网络进行优化。以下是详细分析：</p><hr><h3 id=1-传统-3d-gaussian-splatting-的原理><strong>1. 传统 3D Gaussian Splatting 的原理</strong></h3><h3 id=1-核心流程><strong>(1) 核心流程</strong></h3><ul><li><strong>高斯分布建模</strong>：每个 3D 点被建模为各向异性高斯分布（包含位置、协方差矩阵、颜色、透明度等参数）。</li><li><strong>投影与积分</strong>：将 3D 高斯分布投影到 2D 屏幕空间，通过解析积分或近似方法计算像素颜色。</li><li><strong>Alpha 混合</strong>：根据深度排序对重叠的高斯分布进行透明度混合，生成最终图像。</li></ul><h3 id=2-技术特点><strong>(2) 技术特点</strong></h3><ul><li><strong>显式参数化</strong>：所有参数（位置、协方差、颜色）均为显式定义，无需神经网络隐式表示。</li><li><strong>优化依赖</strong>：通过非线性优化（如梯度下降）调整高斯参数，而非神经网络的端到端训练。</li></ul><hr><h3 id=2-与神经网络的结合方向><strong>2. 与神经网络的结合方向</strong></h3><p>虽然传统方法本身不依赖神经网络，但在近年研究中，<strong>3D Gaussian Splatting 的改进版本可能引入神经网络</strong>，例如：</p><h3 id=1-参数优化加速><strong>(1) 参数优化加速</strong></h3><ul><li><strong>神经网络辅助初始化</strong>：<br>使用神经网络预测初始高斯参数（如协方差矩阵、颜色），替代随机初始化，加速收敛。</li><li><strong>可微分渲染</strong>：<br>将 Gaussian Splatting 的渲染过程设计为可微分模块，与神经网络联合训练，优化几何和外观。</li></ul><h3 id=2-动态场景建模><strong>(2) 动态场景建模</strong></h3><ul><li><strong>形变场预测</strong>：<br>用神经网络预测动态场景中高斯参数的时变形变（如运动轨迹），替代传统物理模拟。</li><li><strong>神经辐射场混合</strong>：<br>结合 NeRF 的隐式辐射场与 Gaussian Splatting 的显式表达，提升渲染质量（如 <a href=https://arxiv.org/abs/2308.04079 target=_blank rel="noopener noreffer">3D-GS+NeRF 混合方法</a>）。</li></ul><hr><h3 id=3-与-nerf-的对比><strong>3. 与 NeRF 的对比</strong></h3><table><thead><tr><th><strong>维度</strong></th><th><strong>3D Gaussian Splatting</strong></th><th><strong>NeRF</strong></th></tr></thead><tbody><tr><td><strong>表示形式</strong></td><td>显式高斯分布</td><td>隐式神经辐射场</td></tr><tr><td><strong>是否需要 NN</strong></td><td>否（传统方法）</td><td>是（MLP 网络建模）</td></tr><tr><td><strong>渲染速度</strong></td><td>实时级（>30 FPS）</td><td>分钟级（需逐像素采样）</td></tr><tr><td><strong>内存占用</strong></td><td>较高（存储显式参数）</td><td>较低（仅存储网络权重）</td></tr><tr><td><strong>动态场景支持</strong></td><td>需额外扩展（如形变场）</td><td>需时序建模（如 NSFF、DyNeRF）</td></tr></tbody></table><hr><h3 id=4-典型应用案例><strong>4. 典型应用案例</strong></h3><h3 id=1-纯-gaussian-splatting><strong>(1) 纯 Gaussian Splatting</strong></h3><ul><li><strong>大规模点云渲染</strong>：如 LiDAR 数据可视化。</li><li><strong>实时 AR/VR</strong>：依赖高效光栅化，无需神经网络推理。</li></ul><h3 id=2-神经网络增强版><strong>(2) 神经网络增强版</strong></h3><ul><li><strong>高质量表面重建</strong>：用神经网络优化高斯分布参数，填补空洞（如 <a href=https://arxiv.org/abs/2303.15136 target=_blank rel="noopener noreffer">Neural Splatting</a>）。</li><li><strong>动态对象建模</strong>：结合 Transformer 预测高斯参数的动态变化。</li></ul><hr><h3 id=总结><strong>总结</strong></h3><ul><li><strong>传统 3D Gaussian Splatting</strong>：不依赖神经网络，基于数学显式建模，适合实时渲染。</li><li><strong>改进版本</strong>：可能引入神经网络以优化参数、处理动态场景或增强细节，但核心仍是高斯投影框架。</li></ul><p>若您的场景需要实时性且无需复杂细节，传统方法足够高效；若追求电影级质量或动态建模，可探索与神经网络的混合方案。</p><h2 id=相机视角表示>相机视角表示</h2><h3 id=gpt>GPT</h3><p>在 <strong>3D Gaussian Splatting</strong> 中，相机视角的表示非常重要，因为它直接影响渲染结果。相机视角决定了你如何从不同的角度观察和渲染3D场景中的高斯点（splats）。为了有效地在 3D 空间中进行渲染和视觉化，我们需要用某种方式来表示和操作相机的视角。</p><h3 id=相机视角的基本表示>相机视角的基本表示</h3><p>相机的视角通常由以下几个元素来描述：</p><ol><li><p><strong>相机位置（Camera Position）</strong>：</p><ul><li><p>这是相机在世界坐标系中的位置。它决定了相机从哪个位置观看场景。</p></li><li><p>在3D渲染中，相机的位置通常用一个 3D 向量表示，例如 。</p><p>C=(x,y,z)\mathbf{C} = (x, y, z)</p></li></ul></li><li><p><strong>视线方向（Camera Orientation or Look-At）</strong>：</p><ul><li>视线方向是指相机正在观察的方向。它通常通过相机朝向的单位向量来表示。</li><li>可以通过欧拉角（Pitch, Yaw, Roll）或者四元数来描述相机的旋转，或者直接通过一个目标点来定义相机的“朝向”（Look-at）。</li></ul></li><li><p><strong>相机的投影矩阵（Projection Matrix）</strong>：</p><ul><li>投影矩阵定义了从相机到场景的投影方式，通常是正交投影（Orthographic Projection）或透视投影（Perspective Projection）。</li><li>透视投影会根据物体到相机的距离进行缩放，而正交投影则保持物体的大小不变。</li></ul></li><li><p><strong>视场（Field of View, FOV）</strong>：</p><ul><li>视场定义了相机的视角，即相机能够看到的范围。通常用角度来表示，例如水平视角或者垂直视角。</li><li>FOV 和投影矩阵一起决定了渲染时场景的可见区域。</li></ul></li></ol><h3 id=在-3d-gaussian-splatting-中的具体实现>在 3D Gaussian Splatting 中的具体实现</h3><p>在 <strong>3D Gaussian Splatting</strong> 中，相机视角主要用来决定如何渲染从高斯点表示的场景。具体来说，相机的视角与高斯点的投影相关，影响点云或高斯点的呈现方式。为了在 GPU 上进行渲染，通常需要对相机的视角进行有效表示，并将其与点云数据结合，以下是一些关键步骤：</p><ol><li><strong>高斯点与相机的关系</strong>：<ul><li>在 Gaussian Splatting 中，每个高斯点都有一个位置、大小和权重。当相机视角发生变化时，点云数据会根据相机的位置和方向进行重新投影，显示在屏幕上的位置也会发生变化。</li><li>高斯点的大小、透明度、颜色等信息可能会根据视角进行动态调整。例如，当一个高斯点靠近相机时，它可能看起来更大，而远离相机的点则看起来更小。</li></ul></li><li><strong>相机矩阵和坐标变换</strong>：<ul><li><strong>视图矩阵（View Matrix）</strong>：用于将世界坐标系中的点转换为相机坐标系中的点。这通常通过旋转和平移矩阵来实现，确保相机能够正确地观察场景。</li><li><strong>投影矩阵（Projection Matrix）</strong>：用于将 3D 相机坐标系中的点投影到 2D 屏幕上。这个矩阵将相机坐标系的点转换为裁剪空间中的点，并根据视场和近远裁剪平面进行缩放。</li><li>这两个矩阵共同定义了如何在屏幕上呈现高斯点。</li></ul></li><li><strong>视角影响的渲染</strong>：<ul><li>在渲染过程中，相机的视角决定了如何从 3D 空间中的高斯点投影到 2D 屏幕上。如果相机移动或旋转，视角就会变化，导致不同的高斯点投影到屏幕上的位置变化。</li><li>相机的变化也会影响高斯点的透明度和权重，尤其是对于远离相机的点，通常需要通过透明度衰减来模拟物体远离视点时的视觉效果。</li></ul></li><li><strong>视角变化和高斯点的自适应调整</strong>：<ul><li>高斯点的表现可能不仅仅依赖于相机视角，还可能根据相机的视角自适应调整。比如在某些情况下，视角的改变可能导致对近处或远处物体的不同处理，例如近处物体显示更多的细节或较大尺寸的高斯点。</li></ul></li></ol><h3 id=相机视角表示的实际应用>相机视角表示的实际应用</h3><p>在 <strong>3D Gaussian Splatting</strong> 中，一种常见的做法是通过一个<strong>基于视角的渲染管线</strong>来处理相机视角。这个管线包括以下几个步骤：</p><ol><li><strong>高斯点的投影计算</strong>：<ul><li>根据当前的相机位置、朝向以及视角信息，将 3D 高斯点投影到 2D 屏幕空间中。每个点的大小和权重也可能根据视角进行调整。</li></ul></li><li><strong>动态调整高斯点的渲染属性</strong>：<ul><li>例如，根据相机距离调整高斯点的大小，或者根据光照和视角调整颜色和透明度。</li><li>高斯点的权重也可能基于视角变化来调整，确保在不同的视角下渲染效果自然。</li></ul></li><li><strong>GPU 加速渲染</strong>：<ul><li>相机视角的信息常常会通过矩阵传递给渲染引擎，在 GPU 上进行高效的视角转换和渲染。CUDA 或 OpenGL 等技术常常用于加速这些计算。</li></ul></li></ol><h3 id=视角表示与深度学习中的结合>视角表示与深度学习中的结合</h3><p>在一些 <strong>基于深度学习的 3D Gaussian Splatting</strong> 方法中，相机视角的表示可以与神经网络模型相结合。例如，在通过多视角图片重建 3D 场景的任务中，神经网络可以根据多个视角的图像数据来学习高斯点的空间分布和渲染参数。在这些方法中，相机视角可能被输入到网络中，作为条件来影响高斯点的生成和渲染效果。</p><h3 id=总结-1>总结</h3><ul><li>在 <strong>3D Gaussian Splatting</strong> 中，相机视角的表示包括位置、朝向、投影矩阵、视场等因素，直接影响渲染结果。</li><li>相机视角决定了如何将 3D 高斯点投影到 2D 屏幕，并且影响高斯点的大小、透明度、颜色等渲染属性。</li><li>相机矩阵（视图矩阵和投影矩阵）用于将 3D 点转换到相机坐标系中，并最终映射到屏幕空间。</li><li>在现代的基于神经网络的 3D 渲染中，相机视角的表示可能会与神经网络结合，进一步优化和生成更加真实的 3D 场景。</li></ul><h3 id=code>code</h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># camera_utils.py</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>Camera</span><span class=p>(</span><span class=n>resolution</span><span class=p>,</span> <span class=n>colmap_id</span><span class=o>=</span><span class=n>cam_info</span><span class=o>.</span><span class=n>uid</span><span class=p>,</span> <span class=n>R</span><span class=o>=</span><span class=n>cam_info</span><span class=o>.</span><span class=n>R</span><span class=p>,</span> <span class=n>T</span><span class=o>=</span><span class=n>cam_info</span><span class=o>.</span><span class=n>T</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>                  <span class=n>FoVx</span><span class=o>=</span><span class=n>cam_info</span><span class=o>.</span><span class=n>FovX</span><span class=p>,</span> <span class=n>FoVy</span><span class=o>=</span><span class=n>cam_info</span><span class=o>.</span><span class=n>FovY</span><span class=p>,</span> <span class=n>depth_params</span><span class=o>=</span><span class=n>cam_info</span><span class=o>.</span><span class=n>depth_params</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                  <span class=n>image</span><span class=o>=</span><span class=n>image</span><span class=p>,</span> <span class=n>invdepthmap</span><span class=o>=</span><span class=n>invdepthmap</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                  <span class=n>image_name</span><span class=o>=</span><span class=n>cam_info</span><span class=o>.</span><span class=n>image_name</span><span class=p>,</span> <span class=n>uid</span><span class=o>=</span><span class=nb>id</span><span class=p>,</span> <span class=n>data_device</span><span class=o>=</span><span class=n>args</span><span class=o>.</span><span class=n>data_device</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                  <span class=n>train_test_exp</span><span class=o>=</span><span class=n>args</span><span class=o>.</span><span class=n>train_test_exp</span><span class=p>,</span> <span class=n>is_test_dataset</span><span class=o>=</span><span class=n>is_test_dataset</span><span class=p>,</span> <span class=n>is_test_view</span><span class=o>=</span><span class=n>cam_info</span><span class=o>.</span><span class=n>is_test</span><span class=p>)</span>
</span></span><span class=line><span class=cl> <span class=n>相机视角是通过R</span><span class=err>、</span><span class=n>T</span><span class=err>、</span><span class=n>FOV等来表达的</span>
</span></span></code></pre></div><h1 id=查看器>查看器</h1><p><a href=https://projects.markkellogg.org/threejs/demo_gaussian_splats_3d.php target=_blank rel="noopener noreffer">https://projects.markkellogg.org/threejs/demo_gaussian_splats_3d.php</a></p><p><a href=https://playcanvas.com/supersplat/editor target=_blank rel="noopener noreffer">https://playcanvas.com/supersplat/editor</a></p><p><a href=https://ernestchu.github.io/splat/ target=_blank rel="noopener noreffer">https://ernestchu.github.io/splat/</a></p><h1 id=思考>思考</h1><h2 id=3dgs-vd-航空摄影测量>3DGS v.d. 航空摄影测量</h2><h3 id=3dgs3d-gaussian-splatting与传统航空摄影测量建模效果对比>3DGS（3D Gaussian Splatting）与传统航空摄影测量建模效果对比</h3><p>3D Gaussian Splatting（3DGS）是近年来兴起的一种基于点云和概率分布的实时渲染技术，而传统航空摄影测量建模则是基于多视角几何和密集匹配的经典方法。二者在建模原理、效果表现和应用场景上存在显著差异，以下是详细对比：</p><hr><h3 id=1-核心原理对比><strong>1. 核心原理对比</strong></h3><table><thead><tr><th><strong>技术</strong></th><th><strong>传统航空摄影测量建模</strong></th><th><strong>3DGS（3D Gaussian Splatting）</strong></th></tr></thead><tbody><tr><td><strong>建模基础</strong></td><td>基于多视角影像的几何重建（SFM + MVS）</td><td>基于概率分布的点云表示（高斯分布参数化场景）</td></tr><tr><td><strong>数据输入</strong></td><td>高重叠率航拍影像 + POS数据（位置姿态信息）</td><td>多视角影像（可无需精确位姿）</td></tr><tr><td><strong>输出形式</strong></td><td>密集点云 → 网格模型（Mesh） + 纹理贴图</td><td>概率化点云（高斯椭球体） + 实时渲染</td></tr><tr><td><strong>计算核心</strong></td><td>几何匹配、三角测量、全局优化</td><td>概率密度建模、可微分渲染、GPU加速优化</td></tr></tbody></table><hr><h3 id=2-建模效果对比><strong>2. 建模效果对比</strong></h3><h3 id=1-几何精度><strong>(1) 几何精度</strong></h3><ul><li><strong>传统摄影测量</strong><ul><li><strong>优势</strong>：依赖精确的影像匹配和空中三角测量，几何精度高（可达厘米级），适合工程测绘和地形建模。</li><li><strong>劣势</strong>：对影像质量、重叠率和光照一致性要求高，复杂场景（如植被、透明物体）易出现孔洞或噪声。</li></ul></li><li><strong>3DGS</strong><ul><li><strong>优势</strong>：通过高斯分布建模，能更自然地表达复杂表面（如树叶、毛发），细节表现更柔和。</li><li><strong>劣势</strong>：几何精度依赖概率优化，绝对精度较低（适用于视觉展示而非测绘）。</li></ul></li></ul><h3 id=2-视觉效果><strong>(2) 视觉效果</strong></h3><ul><li><strong>传统摄影测量</strong><ul><li>依赖纹理贴图，模型表面色彩真实但可能因光照差异出现接缝。</li><li>静态模型，不支持动态光照或实时交互。</li></ul></li><li><strong>3DGS</strong><ul><li>基于概率体渲染，支持动态光照和视角插值，视觉过渡更平滑。</li><li>实时渲染性能强（如游戏引擎集成），适合VR/AR应用。</li></ul></li></ul><h3 id=3-处理效率><strong>(3) 处理效率</strong></h3><ul><li><strong>传统摄影测量</strong><ul><li>流程复杂：空中三角测量 → 密集匹配 → 网格化 → 纹理映射，耗时长（小时级）。</li><li>依赖高性能CPU/集群计算。</li></ul></li><li><strong>3DGS</strong><ul><li>端到端优化，支持GPU加速，训练和渲染速度更快（分钟级到实时）。</li><li>对硬件要求较低（消费级显卡即可）。</li></ul></li></ul><hr><h3 id=3-适用场景对比><strong>3. 适用场景对比</strong></h3><table><thead><tr><th><strong>场景</strong></th><th><strong>传统摄影测量</strong></th><th><strong>3DGS</strong></th></tr></thead><tbody><tr><td><strong>地形测绘</strong></td><td>✅ 高精度DEM/DSM生成，工程测量</td><td>❌ 几何精度不足</td></tr><tr><td><strong>文化遗产保护</strong></td><td>✅ 高保真静态模型（如古建筑扫描）</td><td>✅ 动态展示、虚拟漫游</td></tr><tr><td><strong>影视/游戏</strong></td><td>❌ 静态模型，后期需手动优化</td><td>✅ 实时渲染、动态光照支持</td></tr><tr><td><strong>动态场景</strong></td><td>❌ 仅支持静态场景</td><td>✅ 可扩展至动态物体（如人物、车辆）</td></tr><tr><td><strong>大规模场景</strong></td><td>✅ 分块处理技术成熟</td><td>❌ 显存限制，适合中小场景</td></tr></tbody></table><hr><h3 id=4-优缺点总结><strong>4. 优缺点总结</strong></h3><table><thead><tr><th><strong>技术</strong></th><th><strong>优点</strong></th><th><strong>缺点</strong></th></tr></thead><tbody><tr><td><strong>传统摄影测量</strong></td><td>- 高几何精度<br>- 大场景支持成熟</td><td>- 流程复杂耗时<br>- 动态效果差</td></tr><tr><td><strong>3DGS</strong></td><td>- 实时渲染<br>- 细节自然<br>- 动态扩展性强</td><td>- 几何精度低<br>- 显存限制大场景</td></tr></tbody></table><hr><h3 id=5-未来趋势><strong>5. 未来趋势</strong></h3><ul><li><strong>传统摄影测量</strong>：向自动化、AI辅助优化（如语义分割修复孔洞）发展。</li><li><strong>3DGS</strong>：与NeRF（神经辐射场）结合，提升几何精度和大场景支持能力。</li></ul><hr><h3 id=选择建议><strong>选择建议</strong></h3><ul><li><strong>优先传统摄影测量</strong>：测绘、工程、考古等需要高精度几何数据的场景。</li><li><strong>优先3DGS</strong>：影视特效、游戏开发、虚拟展厅等注重实时交互与视觉效果的领域。</li></ul><p>两种技术并非完全替代，未来可能通过混合建模（如3DGS优化表面细节 + 摄影测量保障几何精度）实现更全面的三维重建。</p></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span>Updated on 2025-04-09</span></div></div><div class=post-info-line><div class=post-info-md></div><div class=post-info-share><span><a href=javascript:void(0); title="Share on Twitter" data-sharer=twitter data-url=https://fjyu95.github.io/posts/3dgs-138e4020f06080e7b9ced1f2074982d5/ data-title=3DGS data-hashtags="3d,gaussian splatting"><i class="fab fa-twitter fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Facebook" data-sharer=facebook data-url=https://fjyu95.github.io/posts/3dgs-138e4020f06080e7b9ced1f2074982d5/ data-hashtag=3d><i class="fab fa-facebook-square fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Hacker News" data-sharer=hackernews data-url=https://fjyu95.github.io/posts/3dgs-138e4020f06080e7b9ced1f2074982d5/ data-title=3DGS><i class="fab fa-hacker-news fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Line" data-sharer=line data-url=https://fjyu95.github.io/posts/3dgs-138e4020f06080e7b9ced1f2074982d5/ data-title=3DGS><i data-svg-src=https://cdn.jsdelivr.net/npm/simple-icons@7.3.0/icons/line.svg aria-hidden=true></i></a><a href=javascript:void(0); title="Share on 微博" data-sharer=weibo data-url=https://fjyu95.github.io/posts/3dgs-138e4020f06080e7b9ced1f2074982d5/ data-title=3DGS><i class="fab fa-weibo fa-fw" aria-hidden=true></i></a></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fas fa-tags fa-fw" aria-hidden=true></i>&nbsp;<a href=/tags/3d/>3d</a>,&nbsp;<a href=/tags/gaussian-splatting/>Gaussian Splatting</a></section><section><span><a href=javascript:void(0); onclick=window.history.back()>Back</a></span>&nbsp;|&nbsp;<span><a href=/>Home</a></span></section></div><div class=post-nav><a href=/posts/summary-127e4020f060808da8e0f23caf646e39/ class=prev rel=prev title=知识笔记><i class="fas fa-angle-left fa-fw" aria-hidden=true></i>知识笔记</a>
<a href=/posts/%D1%85%D0%B0%D1%86%D1%803d%D1%89%D0%B7%D0%BD%D1%85%D1%87%D1%87%D1%8F-139e4020f060802d9629dfad7d7f0b5c/ class=next rel=next title=开源3d重建系统>开源3d重建系统<i class="fas fa-angle-right fa-fw" aria-hidden=true></i></a></div></div></article></div></main><footer class=footer><div class=footer-container><div class=footer-line>Powered by <a href=https://gohugo.io/ target=_blank rel="noopener noreffer" title="Hugo 0.145.0">Hugo</a> | Theme - <a href=https://github.com/dillonzq/LoveIt target=_blank rel="noopener noreffer" title="LoveIt 0.3.0"><i class="far fa-kiss-wink-heart fa-fw" aria-hidden=true></i> LoveIt</a></div><div class=footer-line itemscope itemtype=http://schema.org/CreativeWork><i class="far fa-copyright fa-fw" aria-hidden=true></i><span itemprop=copyrightYear>2020 - 2025</span><span class=author itemprop=copyrightHolder>&nbsp;<a href=https://fjyu95.github.io/ target=_blank>fjyu95</a></span></div></div></footer></div><div id=fixed-buttons><a href=# id=back-to-top class=fixed-button title="Back to Top"><i class="fas fa-arrow-up fa-fw" aria-hidden=true></i>
</a><a href=# id=view-comments class=fixed-button title="View Comments"><i class="fas fa-comment fa-fw" aria-hidden=true></i></a></div><script type=text/javascript src=https://cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/sharer.js@0.5.1/sharer.min.js></script><script type=text/javascript>window.config={code:{copyTitle:"Copy to clipboard",maxShownLines:50},comment:{}}</script><script type=text/javascript src=/js/theme.min.js></script></body></html>
<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="noodp"><title>Transformer - FangjieYu‘s site</title><meta name=Description content="This is my cool site"><meta property="og:url" content="https://fjyu95.github.io/posts/%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%E5%BA%93/9_%E5%A4%A7%E6%A8%A1%E5%9E%8B/">
<meta property="og:site_name" content="FangjieYu‘s site"><meta property="og:title" content="Transformer"><meta property="og:description" content="deep-learning、CV（2d、3d）、RS、optical"><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-10-22T00:00:00+00:00"><meta property="article:modified_time" content="2025-04-08T00:00:00+00:00"><meta property="article:tag" content="注意力机制"><meta property="article:tag" content="Transformer"><meta name=twitter:card content="summary"><meta name=twitter:title content="Transformer"><meta name=twitter:description content="deep-learning、CV（2d、3d）、RS、optical"><meta name=application-name content="My cool site"><meta name=apple-mobile-web-app-title content="My cool site"><meta name=theme-color content="#ffffff"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://fjyu95.github.io/posts/%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%E5%BA%93/9_%E5%A4%A7%E6%A8%A1%E5%9E%8B/><link rel=next href=https://fjyu95.github.io/posts/%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%E5%BA%93/8_%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA/><link rel=stylesheet href=/css/style.min.css><link rel=preload href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css></noscript><link rel=preload href=https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css></noscript><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"Transformer","inLanguage":"en-us","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/fjyu95.github.io\/posts\/%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%E5%BA%93\/9_%E5%A4%A7%E6%A8%A1%E5%9E%8B\/"},"genre":"posts","keywords":"注意力机制, transformer","wordcount":3428,"url":"https:\/\/fjyu95.github.io\/posts\/%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%E5%BA%93\/9_%E5%A4%A7%E6%A8%A1%E5%9E%8B\/","datePublished":"2024-10-22T00:00:00+00:00","dateModified":"2025-04-08T00:00:00+00:00","publisher":{"@type":"Organization","name":""},"author":{"@type":"Person","name":"fjyu95"},"description":""}</script></head><body data-header-desktop=fixed data-header-mobile=auto><script type=text/javascript>(window.localStorage&&localStorage.getItem("theme")?localStorage.getItem("theme")==="dark":"auto"==="auto"?window.matchMedia("(prefers-color-scheme: dark)").matches:"auto"==="dark")&&document.body.setAttribute("theme","dark")</script><div id=mask></div><div class=wrapper><header class=desktop id=header-desktop><div class=header-wrapper><div class=header-title><a href=/ title="FangjieYu‘s site">fjyu95</a></div><div class=menu><div class=menu-inner><a class=menu-item href=/posts/>posts </a><a class=menu-item href=/tags/>tags </a><a class=menu-item href=/categories/>category </a><span class="menu-item delimiter"></span><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme">
<i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></div></div></div></header><header class=mobile id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title="FangjieYu‘s site">fjyu95</a></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><div class=menu id=menu-mobile><a class=menu-item href=/posts/ title>posts</a><a class=menu-item href=/tags/ title>tags</a><a class=menu-item href=/categories/ title>category</a><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme">
<i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></div></div></header><main class=main><div class=container><div class=toc id=toc-auto><h2 class=toc-title>Contents</h2><div class=toc-content id=toc-content-auto></div></div><article class="page single"><h1 class="single-title animate__animated animate__flipInX">Transformer</h1><div class=post-meta><div class=post-meta-line><span class=post-author><a href=https://fjyu95.github.io/ title=Author target=_blank rel="noopener noreffer author" class=author><i class="fas fa-user-circle fa-fw" aria-hidden=true></i>fjyu95</a></span>&nbsp;<span class=post-category>included in <a href=/categories/%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%E5%BA%93/><i class="far fa-folder fa-fw" aria-hidden=true></i>算法知识库</a></span></div><div class=post-meta-line><i class="far fa-calendar-alt fa-fw" aria-hidden=true></i>&nbsp;<time datetime=2024-10-22>2024-10-22</time>&nbsp;<i class="fas fa-pencil-alt fa-fw" aria-hidden=true></i>&nbsp;3428 words&nbsp;
<i class="far fa-clock fa-fw" aria-hidden=true></i>&nbsp;7 minutes&nbsp;</div></div><div class="details toc" id=toc-static data-kept><div class="details-summary toc-title"><span>Contents</span>
<span><i class="details-icon fas fa-angle-right" aria-hidden=true></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#transformer架构>transformer架构</a><ul><li><ul><li><a href=#attention-is-all-you-need><strong>Attention Is All You Need</strong></a></li><li><a href=#注意力机制>注意力机制</a></li></ul></li><li><a href=#自注意力机制-self-attention>自注意力机制 <strong>（Self-Attention）</strong></a><ul><li><a href=#concat-操作>concat 操作</a></li></ul></li><li><a href=#线性变换>线性变换</a><ul><li><a href=#1-解码器输出>1. 解码器输出</a></li><li><a href=#2-线性变换>2. 线性变换</a></li><li><a href=#3-输出解释>3. 输出解释</a></li><li><a href=#4-进一步处理>4. 进一步处理</a></li><li><a href=#总结>总结</a></li></ul></li><li><a href=#softmax激活>softmax激活</a><ul><li><a href=#1-模型结构>1. 模型结构</a></li><li><a href=#2-生成输出序列的步骤>2. 生成输出序列的步骤</a></li><li><a href=#21-输入处理>2.1 输入处理</a></li><li><a href=#22-线性变换>2.2 线性变换</a></li><li><a href=#23-softmax-函数>2.3 Softmax 函数</a></li><li><a href=#3-输出解释-1>3. 输出解释</a></li><li><a href=#4-损失计算>4. 损失计算</a></li><li><a href=#总结-1>总结</a></li></ul></li><li><a href=#训练中的shifted-right操作>训练中的shifted right操作</a><ul><li><a href=#1-shifted-right-的操作步骤>1. Shifted Right 的操作步骤</a></li><li><a href=#2-为什么需要-shifted-right>2. 为什么需要 Shifted Right</a></li><li><a href=#3-实际应用>3. 实际应用</a></li><li><a href=#4-masking-和-shifted-right-配合>4. Masking 和 Shifted Right 配合</a></li><li><a href=#总结-2>总结</a></li></ul></li><li><a href=#ln层>LN层</a></li></ul></li><li><a href=#vit>ViT</a></li></ul></nav></div></div><div class=content id=content><h1 id=transformer架构>transformer架构</h1><p><a href="https://www.notion.so/Transformer-138d578c02cb47a48eb99517a7e8af3c?pvs=21" target=_blank rel="noopener noreffer"><strong>Transformer</strong></a></p><p>基于自注意力机制和多头注意力机制</p><h3 id=attention-is-all-you-need><strong>Attention Is All You Need</strong></h3><ul><li><strong>作者</strong>：Vaswani et al.</li><li><strong>年份</strong>：2017</li><li><strong>链接</strong>：<a href=https://arxiv.org/abs/1706.03762 target=_blank rel="noopener noreffer">论文链接</a></li><li><strong>贡献</strong>：提出了 Transformer 架构，完全基于自注意力机制，取代了传统的循环神经网络（RNN）。这是注意力机制在自然语言处理中的重要里程碑。</li></ul><h3 id=注意力机制>注意力机制</h3><p>注意力就是权重，即关注程度</p><p>Q，K，V</p><p>查询（自主性提示）、键、值</p><p><img class=lazyload src=/svg/loading.min.svg data-src=/posts/%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%E5%BA%93/9_%E5%A4%A7%E6%A8%A1%E5%9E%8B/1.png data-srcset="/posts/%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%E5%BA%93/9_%E5%A4%A7%E6%A8%A1%E5%9E%8B/1.png, /posts/%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%E5%BA%93/9_%E5%A4%A7%E6%A8%A1%E5%9E%8B/1.png 1.5x, /posts/%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%E5%BA%93/9_%E5%A4%A7%E6%A8%A1%E5%9E%8B/1.png 2x" data-sizes=auto alt=/posts/%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%E5%BA%93/9_%E5%A4%A7%E6%A8%A1%E5%9E%8B/1.png title=注意力机制&amp;Transformer_赵云_20240408_页面_08.png width=1419 height=744></p><p>权重由<strong>注意力评分函数</strong>计算</p><h2 id=自注意力机制-self-attention>自注意力机制 <strong>（Self-Attention）</strong></h2><p>source=target这种特殊的注意力机制</p><p>在<strong>同一输入序列内部计算注意力</strong>，使得每个元素都可以与其他元素进行交互，增强上下文信息的理解。</p><p><img class=lazyload src=/svg/loading.min.svg data-src=/posts/%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%E5%BA%93/9_%E5%A4%A7%E6%A8%A1%E5%9E%8B/2.png data-srcset="/posts/%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%E5%BA%93/9_%E5%A4%A7%E6%A8%A1%E5%9E%8B/2.png, /posts/%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%E5%BA%93/9_%E5%A4%A7%E6%A8%A1%E5%9E%8B/2.png 1.5x, /posts/%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%E5%BA%93/9_%E5%A4%A7%E6%A8%A1%E5%9E%8B/2.png 2x" data-sizes=auto alt=/posts/%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%E5%BA%93/9_%E5%A4%A7%E6%A8%A1%E5%9E%8B/2.png title=注意力机制&amp;Transformer_赵云_20240408_页面_14.png width=1415 height=794></p><p>每个词表示为其他所有词的加权和</p><p><strong>整体架构</strong></p><p><img class=lazyload src=/svg/loading.min.svg data-src=/posts/%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%E5%BA%93/9_%E5%A4%A7%E6%A8%A1%E5%9E%8B/3.png data-srcset="/posts/%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%E5%BA%93/9_%E5%A4%A7%E6%A8%A1%E5%9E%8B/3.png, /posts/%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%E5%BA%93/9_%E5%A4%A7%E6%A8%A1%E5%9E%8B/3.png 1.5x, /posts/%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%E5%BA%93/9_%E5%A4%A7%E6%A8%A1%E5%9E%8B/3.png 2x" data-sizes=auto alt=/posts/%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%E5%BA%93/9_%E5%A4%A7%E6%A8%A1%E5%9E%8B/3.png title=注意力机制&amp;Transformer_赵云_20240408_页面_13.png width=1392 height=709></p><p><strong>训练时使用的是教师强制，预测第n+1个token时输入前n个token真实值</strong></p><p>训练时：第i个decoder的输入 = encoder输出 + ground truth embeding</p><p>预测时：第i个decoder的输入 = encoder输出 + 第(i-1)个decoder输出</p><p><strong>训练时因为知道ground truth embeding，相当于知道正确答案，网络可以一次训练完成</strong>。</p><p>预测时，首先输入start，输出预测的第一个单词 然后start和新单词组成新的query，再输入decoder来预测下一个单词，循环往复 直至end</p><p><strong>相当于训练时是有标签的</strong></p><p><img class=lazyload src=/svg/loading.min.svg data-src=/posts/%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%E5%BA%93/9_%E5%A4%A7%E6%A8%A1%E5%9E%8B/4.png data-srcset="/posts/%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%E5%BA%93/9_%E5%A4%A7%E6%A8%A1%E5%9E%8B/4.png, /posts/%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%E5%BA%93/9_%E5%A4%A7%E6%A8%A1%E5%9E%8B/4.png 1.5x, /posts/%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%E5%BA%93/9_%E5%A4%A7%E6%A8%A1%E5%9E%8B/4.png 2x" data-sizes=auto alt=/posts/%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%E5%BA%93/9_%E5%A4%A7%E6%A8%A1%E5%9E%8B/4.png title=注意力机制&amp;Transformer_赵云_20240408_页面_15.png width=1380 height=772></p><h3 id=concat-操作>concat 操作</h3><pre tabindex=0><code>拼接输出：将所有注意力头的输出拼接在一起，形成一个大的特征向量。
Concat(Attention1,Attention2,…,Attentionh)=Attention1∥Attention2∥…∥Attentionh
Concat(Attention1,Attention2,…,Attentionh)=Attention1∥Attention2∥…∥Attentionh

其中 hh 是注意力头的数量，∥∥ 表示拼接操作。
</code></pre><p><img class=lazyload src=/svg/loading.min.svg data-src=/posts/%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%E5%BA%93/9_%E5%A4%A7%E6%A8%A1%E5%9E%8B/5.png data-srcset="/posts/%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%E5%BA%93/9_%E5%A4%A7%E6%A8%A1%E5%9E%8B/5.png, /posts/%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%E5%BA%93/9_%E5%A4%A7%E6%A8%A1%E5%9E%8B/5.png 1.5x, /posts/%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%E5%BA%93/9_%E5%A4%A7%E6%A8%A1%E5%9E%8B/5.png 2x" data-sizes=auto alt=/posts/%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%E5%BA%93/9_%E5%A4%A7%E6%A8%A1%E5%9E%8B/5.png title=注意力机制&amp;Transformer_赵云_20240408_页面_16.png width=1419 height=707></p><p>Transformer 模型中的前馈网络主要负责对输入进行特征转换和表示学习，以便于模型理解输入序列的语义信息和结构关系。这些<strong>前馈网络通常是浅层的，只有一个或两个全连接隐藏层，以减少模型的复杂度，并避免过拟合</strong>。</p><p><img class=lazyload src=/svg/loading.min.svg data-src=/posts/%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%E5%BA%93/9_%E5%A4%A7%E6%A8%A1%E5%9E%8B/6.png data-srcset="/posts/%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%E5%BA%93/9_%E5%A4%A7%E6%A8%A1%E5%9E%8B/6.png, /posts/%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%E5%BA%93/9_%E5%A4%A7%E6%A8%A1%E5%9E%8B/6.png 1.5x, /posts/%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%E5%BA%93/9_%E5%A4%A7%E6%A8%A1%E5%9E%8B/6.png 2x" data-sizes=auto alt=/posts/%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%E5%BA%93/9_%E5%A4%A7%E6%A8%A1%E5%9E%8B/6.png title=注意力机制&amp;Transformer_赵云_20240408_页面_17.png width=1437 height=721></p><p><img class=lazyload src=/svg/loading.min.svg data-src=/posts/%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%E5%BA%93/9_%E5%A4%A7%E6%A8%A1%E5%9E%8B/7.png data-srcset="/posts/%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%E5%BA%93/9_%E5%A4%A7%E6%A8%A1%E5%9E%8B/7.png, /posts/%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%E5%BA%93/9_%E5%A4%A7%E6%A8%A1%E5%9E%8B/7.png 1.5x, /posts/%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%E5%BA%93/9_%E5%A4%A7%E6%A8%A1%E5%9E%8B/7.png 2x" data-sizes=auto alt=/posts/%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%E5%BA%93/9_%E5%A4%A7%E6%A8%A1%E5%9E%8B/7.png title=注意力机制&amp;Transformer_赵云_20240408_页面_18.png width=1400 height=731></p><h2 id=线性变换>线性变换</h2><p>在 Transformer 模型中，线性变换后的输出是一个 logits 向量，这个向量用于表示每个可能输出（通常是词汇表中的词）的得分。具体来说，以下是线性变换后输出的详细过程：</p><h3 id=1-解码器输出>1. 解码器输出</h3><p>在 Transformer 的解码器部分，经过多层自注意力和编码器-解码器注意力之后，最终生成的输出是一个表示当前时间步的上下文特征的向量。这一向量通常被称为解码器的输出。</p><h3 id=2-线性变换>2. 线性变换</h3><ul><li><p><strong>线性层</strong>：解码器的输出经过一个线性层（全连接层），将其映射到词汇表的维度。假设词汇表大小为 VVV，解码器的输出向量维度为 dmodeld_{model}dmodel​，则线性变换的公式为：Logits=W⋅Output+b</p><p>Logits=W⋅Output+b\text{Logits} = W \cdot \text{Output} + b</p><p>其中：</p><ul><li><p>WWW 是权重矩阵，维度为 V×dmodel​。</p><p>V×dmodelV \times d_{model}</p></li><li><p>Output\text{Output}Output 是解码器的输出向量，维度为 dmodel​。</p><p>dmodeld_{model}</p></li><li><p>bbb 是偏置项，维度为 V。</p><p>VV</p></li></ul></li></ul><h3 id=3-输出解释>3. 输出解释</h3><ul><li><p><strong>Logits 向量</strong>：线性变换后的输出，即 logits 向量，维度为 V，表示模型对于词汇表中每个词的得分。这些得分并不直接表示概率，而是用来计算每个词的相对可能性。</p><p>VV</p></li></ul><h3 id=4-进一步处理>4. 进一步处理</h3><ul><li><p><strong>Softmax 函数</strong>：将 logits 向量传递给 Softmax 函数，转换为概率分布，使得每个词的输出概率可以用来选择下一个生成的词。P(yi​=k)=∑j​elogitsj​elogitsk​​</p><p>P(yi=k)=elogitsk∑jelogitsjP(y_i = k) = \frac{e^{\text{logits}<em>k}}{\sum</em>{j} e^{\text{logits}_j}}</p></li></ul><h3 id=总结>总结</h3><p>在 Transformer 中，线性变换后的输出是一个 logits 向量，它用于表示模型对每个可能输出的得分。这个 logits 向量随后通过 Softmax 函数转换为概率分布，从而实现下一个词的预测。</p><h2 id=softmax激活>softmax激活</h2><p>在 Transformer 模型中，Softmax 函数被用于预测输出概率，特别是在生成序列时，例如文本生成、翻译等任务。以下是 Transformer 如何利用 Softmax 进行输出概率预测的详细步骤：</p><h3 id=1-模型结构>1. 模型结构</h3><p>Transformer 主要由编码器和解码器组成。解码器负责生成输出序列，其最后一层通常是线性层后接 Softmax 函数。</p><h3 id=2-生成输出序列的步骤>2. 生成输出序列的步骤</h3><h3 id=21-输入处理>2.1 输入处理</h3><ul><li>在解码器中，输入通常是前一个时间步的输出（或者是特定的起始符号）。</li><li>解码器中的自注意力机制和编码器-解码器注意力机制用于生成当前时间步的上下文表示。</li></ul><h3 id=22-线性变换>2.2 线性变换</h3><ul><li><p>解码器的输出经过线性层（也称为全连接层），将其映射到词汇表大小的维度。Logits=W⋅Output</p><p>Logits=W⋅Output\text{Logits} = W \cdot \text{Output}</p><p>其中 WWW 是权重矩阵，Output\text{Output}Output 是解码器的输出。</p></li></ul><h3 id=23-softmax-函数>2.3 Softmax 函数</h3><ul><li><p>对于生成的 logits，使用 Softmax 函数将其转换为概率分布：P(yi​=k)=∑j​elogitsj​elogitsk​​</p><p>P(yi=k)=elogitsk∑jelogitsjP(y_i = k) = \frac{e^{\text{logits}<em>k}}{\sum</em>{j} e^{\text{logits}_j}}</p><p>其中 kkk 表示词汇表中的每个词，logitsk\text{logits}_klogitsk​ 是对应词的 logits 值。</p></li></ul><h3 id=3-输出解释-1>3. 输出解释</h3><ul><li><p>Softmax 的输出 P(yi​) 表示当前时间步生成每个词的概率。这些概率可以用于选择最可能的下一个词：</p><p>P(yi)P(y_i)</p><ul><li><strong>采样</strong>：可以通过随机选择一个词（根据概率分布）来生成文本。</li><li><strong>贪婪搜索</strong>：选择概率最大的词作为输出。</li><li><strong>束搜索（Beam Search）</strong>：在生成过程中维护多个候选序列，以提高生成的质量。</li></ul></li></ul><h3 id=4-损失计算>4. 损失计算</h3><ul><li><p>在训练过程中，使用交叉熵损失来衡量模型预测的概率分布与实际目标分布之间的差距：Loss=−i∑​yi​log(P(yi​))</p><p>Loss=−∑iyilog⁡(P(yi))\text{Loss} = -\sum_{i} y_i \log(P(y_i))</p><p>其中 yiy_iyi​ 是目标词的独热编码，P(yi)P(y_i)P(yi​) 是模型预测的概率。</p></li></ul><h3 id=总结-1>总结</h3><p>在 Transformer 中，Softmax 函数通过将线性变换后的输出映射为概率分布，使得模型能够预测下一个词的可能性。这一机制使得 Transformer 在序列生成任务中表现出色，能够灵活处理多种类型的文本生成和翻译任务。</p><h2 id=训练中的shifted-right操作>训练中的shifted right操作</h2><p>在 Transformer 模型的训练中，“<strong>shifted right</strong>” 是指将目标输出序列向右移动一个位置，从而在解码器中进行自回归的训练过程。这是序列生成任务（例如机器翻译、文本生成）中常用的技巧，用于<strong>确保解码器在预测当前词时只能访问先前的词，而不能看到未来词，从而避免“数据泄露”</strong>。以下是这一操作的详细过程：</p><h3 id=1-shifted-right-的操作步骤>1. Shifted Right 的操作步骤</h3><p>假设我们有目标输出序列 y=[y1,y2,y3,…,yn]\text{y} = [y_1, y_2, y_3, \ldots, y_n]y=[y1​,y2​,y3​,…,yn​]：</p><ul><li><strong>原始目标序列</strong>：包含模型应该预测的词，例如 <code>[y_1, y_2, y_3, \ldots, y_n]</code>。</li><li><strong>Shifted Right 序列</strong>：在解码器输入时，将目标序列右移一位，并在最前面插入一个特定的起始标记 <code>&lt;start></code>（例如 <code>[&lt;start>, y_1, y_2, \ldots, y_{n-1}]</code>），用于指导模型生成完整的输出序列。</li></ul><h3 id=2-为什么需要-shifted-right>2. 为什么需要 Shifted Right</h3><ul><li><p><strong>自回归建模</strong>：在生成第 t 个词 yt​ 时，模型只能访问它之前的词（即 <code>[y_1, y_2, \ldots, y_{t-1}]</code>），从而模拟生成过程中的自回归特性。</p><p>tt</p><p>yty_t</p></li><li><p><strong>避免信息泄露</strong>：如果解码器在第 t 步直接看到 yt​ 之后的词，模型会利用未来信息，导致训练与实际生成过程不一致。因此，Shifted Right 强制模型只能基于先前的上下文来预测当前词。</p><p>tt</p><p>yty_t</p></li></ul><h3 id=3-实际应用>3. 实际应用</h3><p>在具体实现中，Shifted Right 通常在解码器输入中完成。例如：</p><ul><li><strong>训练输入</strong>：输入解码器的序列为 <code>[&lt;start>, y_1, y_2, ..., y_{n-1}]</code>。</li><li><strong>训练标签</strong>：模型的预测目标为 <code>[y_1, y_2, y_3, ..., y_n]</code>，用于计算损失。</li></ul><p>这样，解码器在第 ttt 步预测 yty_tyt​ 时，能够访问的是 [<start>,y1,y2,&mldr;,yt−1][<start>, y_1, y_2, &mldr;, y_{t-1}][<start>,y1​,y2​,&mldr;,yt−1​] 而不是包含未来词的完整序列。</p><h3 id=4-masking-和-shifted-right-配合>4. Masking 和 Shifted Right 配合</h3><p>在实际操作中，Transformer 的解码器还使用了 <strong>Masking（掩码）</strong> 技术，进一步确保模型仅关注之前的词。例如，利用三角矩阵掩码让解码器在第 ttt 步只能访问到 [<start>,y1,y2,…,yt−1][<start>, y_1, y_2, \ldots, y_{t-1}][<start>,y1​,y2​,…,yt−1​]。</p><h3 id=总结-2>总结</h3><p>Shifted Right 是 Transformer 模型在解码器中训练序列生成任务的关键操作，通过向右移动目标序列，模型能够模拟生成序列的自回归过程，确保预测时不会使用未来信息。</p><h2 id=ln层>LN层</h2><ul><li><strong>Batch Normalization (BN)</strong>：<strong>在批次维度上进行归一化</strong>，即对每个 mini-batch 内的<strong>所有样本的同一特征进行归一化</strong>。公式中使用的是每个 mini-batch 内特征的均值和方差。适合卷积神经网络（CNN）等批量大小比较大的场景。</li><li><strong>Layer Normalization (LN)</strong>：在层维度上进行归一化，即对<strong>每个样本的所有特征进行归一化</strong>。公式中使用的是单个样本的所有特征的均值和方差。适合循环神经网络（RNN）和 Transformer 等批量大小较小或动态性强的场景。</li></ul><h1 id=vit>ViT</h1><p>待续&mldr;</p></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span>Updated on 2025-04-08</span></div></div><div class=post-info-line><div class=post-info-md></div><div class=post-info-share><span><a href=javascript:void(0); title="Share on Twitter" data-sharer=twitter data-url=https://fjyu95.github.io/posts/%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%E5%BA%93/9_%E5%A4%A7%E6%A8%A1%E5%9E%8B/ data-title=Transformer data-hashtags=注意力机制,transformer><i class="fab fa-twitter fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Facebook" data-sharer=facebook data-url=https://fjyu95.github.io/posts/%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%E5%BA%93/9_%E5%A4%A7%E6%A8%A1%E5%9E%8B/ data-hashtag=注意力机制><i class="fab fa-facebook-square fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Hacker News" data-sharer=hackernews data-url=https://fjyu95.github.io/posts/%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%E5%BA%93/9_%E5%A4%A7%E6%A8%A1%E5%9E%8B/ data-title=Transformer><i class="fab fa-hacker-news fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Line" data-sharer=line data-url=https://fjyu95.github.io/posts/%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%E5%BA%93/9_%E5%A4%A7%E6%A8%A1%E5%9E%8B/ data-title=Transformer><i data-svg-src=https://cdn.jsdelivr.net/npm/simple-icons@7.3.0/icons/line.svg aria-hidden=true></i></a><a href=javascript:void(0); title="Share on 微博" data-sharer=weibo data-url=https://fjyu95.github.io/posts/%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%E5%BA%93/9_%E5%A4%A7%E6%A8%A1%E5%9E%8B/ data-title=Transformer><i class="fab fa-weibo fa-fw" aria-hidden=true></i></a></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fas fa-tags fa-fw" aria-hidden=true></i>&nbsp;<a href=/tags/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/>注意力机制</a>,&nbsp;<a href=/tags/transformer/>Transformer</a></section><section><span><a href=javascript:void(0); onclick=window.history.back()>Back</a></span>&nbsp;|&nbsp;<span><a href=/>Home</a></span></section></div><div class=post-nav><a href=/posts/%E7%AE%97%E6%B3%95%E7%9F%A5%E8%AF%86%E5%BA%93/8_%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA/ class=next rel=next title=三维重建>三维重建<i class="fas fa-angle-right fa-fw" aria-hidden=true></i></a></div></div></article></div></main><footer class=footer><div class=footer-container><div class=footer-line>Powered by <a href=https://gohugo.io/ target=_blank rel="noopener noreffer" title="Hugo 0.145.0">Hugo</a> | Theme - <a href=https://github.com/dillonzq/LoveIt target=_blank rel="noopener noreffer" title="LoveIt 0.3.0"><i class="far fa-kiss-wink-heart fa-fw" aria-hidden=true></i> LoveIt</a></div><div class=footer-line itemscope itemtype=http://schema.org/CreativeWork><i class="far fa-copyright fa-fw" aria-hidden=true></i><span itemprop=copyrightYear>2020 - 2025</span><span class=author itemprop=copyrightHolder>&nbsp;<a href=https://fjyu95.github.io/ target=_blank>fjyu95</a></span></div></div></footer></div><div id=fixed-buttons><a href=# id=back-to-top class=fixed-button title="Back to Top"><i class="fas fa-arrow-up fa-fw" aria-hidden=true></i>
</a><a href=# id=view-comments class=fixed-button title="View Comments"><i class="fas fa-comment fa-fw" aria-hidden=true></i></a></div><script type=text/javascript src=https://cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/sharer.js@0.5.1/sharer.min.js></script><script type=text/javascript>window.config={code:{copyTitle:"Copy to clipboard",maxShownLines:50},comment:{}}</script><script type=text/javascript src=/js/theme.min.js></script></body></html>
<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="noodp"><title>知识笔记 - FangjieYu‘s site</title><meta name=Description content="This is my cool site"><meta property="og:url" content="https://fjyu95.github.io/posts/summary-127e4020f060808da8e0f23caf646e39/">
<meta property="og:site_name" content="FangjieYu‘s site"><meta property="og:title" content="知识笔记"><meta property="og:description" content="deep-learning、CV（2d、3d）、RS、optical
视觉slam https://wykxwyc.github.io/2019/03/21/Small-Talk/
视觉slam框架 整个视觉 SLAM 流程包括以下步骤。
传感器信息读取。在视觉 SLAM 中主要为相机图像信息的读取和预处理。如果是在机器人中，还可能有码盘、惯性传感器等信息的读取和同步。 视觉里程计（Visual Odometry，VO）。视觉里程计的任务是估算相邻图像间相机的运动，以及局部地图的样子。VO 又称为前端（Front End）。 后端优化（Optimization）。后端接受不同时刻视觉里程计测量的相机位姿，以及回环检测的信息，对它们进行优化，得到全局一致的轨迹和地图。由于接在 VO 之后，又称为后端（Back End）。 回环检测（Loop Closing）。回环检测判断机器人是否到达过先前的位置。如果检测到回环，它会把信息提供给后端进行处理。 建图（Mapping）。它根据估计的轨迹，建立与任务要求对应的地图。 数学基础 向量一般默认指列向量"><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-10-22T00:00:00+00:00"><meta property="article:modified_time" content="2025-04-08T00:00:00+00:00"><meta property="article:tag" content="Deep Learning"><meta property="article:tag" content="CV"><meta property="article:tag" content="遥感"><meta property="article:tag" content="光学"><meta name=twitter:card content="summary"><meta name=twitter:title content="知识笔记"><meta name=twitter:description content="deep-learning、CV（2d、3d）、RS、optical
视觉slam https://wykxwyc.github.io/2019/03/21/Small-Talk/
视觉slam框架 整个视觉 SLAM 流程包括以下步骤。
传感器信息读取。在视觉 SLAM 中主要为相机图像信息的读取和预处理。如果是在机器人中，还可能有码盘、惯性传感器等信息的读取和同步。 视觉里程计（Visual Odometry，VO）。视觉里程计的任务是估算相邻图像间相机的运动，以及局部地图的样子。VO 又称为前端（Front End）。 后端优化（Optimization）。后端接受不同时刻视觉里程计测量的相机位姿，以及回环检测的信息，对它们进行优化，得到全局一致的轨迹和地图。由于接在 VO 之后，又称为后端（Back End）。 回环检测（Loop Closing）。回环检测判断机器人是否到达过先前的位置。如果检测到回环，它会把信息提供给后端进行处理。 建图（Mapping）。它根据估计的轨迹，建立与任务要求对应的地图。 数学基础 向量一般默认指列向量"><meta name=application-name content="My cool site"><meta name=apple-mobile-web-app-title content="My cool site"><meta name=theme-color content="#ffffff"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://fjyu95.github.io/posts/summary-127e4020f060808da8e0f23caf646e39/><link rel=prev href=https://fjyu95.github.io/posts/%D1%86%D1%81%D0%B4%D1%85%D1%86%D0%BA%D0%B0%D1%85%D0%B7-4930a5ba56eb4f559ace49268f2a64b0/><link rel=next href=https://fjyu95.github.io/posts/3dgs-138e4020f06080e7b9ced1f2074982d5/><link rel=stylesheet href=/css/style.min.css><link rel=preload href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css></noscript><link rel=preload href=https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css></noscript><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"知识笔记","inLanguage":"en-us","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/fjyu95.github.io\/posts\/summary-127e4020f060808da8e0f23caf646e39\/"},"genre":"posts","keywords":"deep learning, CV, 遥感, 光学","wordcount":25815,"url":"https:\/\/fjyu95.github.io\/posts\/summary-127e4020f060808da8e0f23caf646e39\/","datePublished":"2024-10-22T00:00:00+00:00","dateModified":"2025-04-08T00:00:00+00:00","publisher":{"@type":"Organization","name":""},"author":{"@type":"Person","name":"fjyu95"},"description":""}</script></head><body data-header-desktop=fixed data-header-mobile=auto><script type=text/javascript>(window.localStorage&&localStorage.getItem("theme")?localStorage.getItem("theme")==="dark":"auto"==="auto"?window.matchMedia("(prefers-color-scheme: dark)").matches:"auto"==="dark")&&document.body.setAttribute("theme","dark")</script><div id=mask></div><div class=wrapper><header class=desktop id=header-desktop><div class=header-wrapper><div class=header-title><a href=/ title="FangjieYu‘s site">fjyu95</a></div><div class=menu><div class=menu-inner><a class=menu-item href=/posts/>posts </a><a class=menu-item href=/tags/>tags </a><a class=menu-item href=/categories/>category </a><span class="menu-item delimiter"></span><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme">
<i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></div></div></div></header><header class=mobile id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title="FangjieYu‘s site">fjyu95</a></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><div class=menu id=menu-mobile><a class=menu-item href=/posts/ title>posts</a><a class=menu-item href=/tags/ title>tags</a><a class=menu-item href=/categories/ title>category</a><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme">
<i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></div></div></header><main class=main><div class=container><div class=toc id=toc-auto><h2 class=toc-title>Contents</h2><div class=toc-content id=toc-content-auto></div></div><article class="page single"><h1 class="single-title animate__animated animate__flipInX">知识笔记</h1><div class=post-meta><div class=post-meta-line><span class=post-author><a href=https://fjyu95.github.io/ title=Author target=_blank rel="noopener noreffer author" class=author><i class="fas fa-user-circle fa-fw" aria-hidden=true></i>fjyu95</a></span>&nbsp;<span class=post-category>included in <a href=/categories/%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA/><i class="far fa-folder fa-fw" aria-hidden=true></i>三维重建</a>&nbsp;<a href=/categories/slam/><i class="far fa-folder fa-fw" aria-hidden=true></i>Slam</a></span></div><div class=post-meta-line><i class="far fa-calendar-alt fa-fw" aria-hidden=true></i>&nbsp;<time datetime=2024-10-22>2024-10-22</time>&nbsp;<i class="fas fa-pencil-alt fa-fw" aria-hidden=true></i>&nbsp;25815 words&nbsp;
<i class="far fa-clock fa-fw" aria-hidden=true></i>&nbsp;52 minutes&nbsp;</div></div><div class="details toc" id=toc-static data-kept><div class="details-summary toc-title"><span>Contents</span>
<span><i class="details-icon fas fa-angle-right" aria-hidden=true></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#视觉slam>视觉slam</a><ul><li><a href=#视觉slam框架>视觉slam框架</a></li><li><a href=#数学基础>数学基础</a><ul><li><a href=#齐次坐标>齐次坐标</a></li><li><a href=#内外积>内外积</a></li></ul></li><li><a href=#svd分解>SVD分解</a><ul><li><a href=#1-svd的数学形式>1. <strong>SVD的数学形式</strong></a></li><li><a href=#2-奇异值>2. <strong>奇异值</strong></a></li><li><a href=#3-几何解释>3. <strong>几何解释</strong></a></li></ul></li><li><a href=#非线性优化>非线性优化</a></li><li><a href=#几种投影变换>几种投影变换</a><ul><li><a href=#三维仿射变换>三维仿射变换</a></li></ul></li><li><a href=#三维刚体运动表示>三维刚体运动表示</a><ul><li><a href=#1旋转矩阵--不够紧凑冗余优化困难带有约束>1、旋转矩阵 <strong>不够紧凑、冗余、优化困难（带有约束）</strong></a></li><li><a href=#2旋转向量-轴-角表示法>2、旋转向量 轴-角表示法</a></li><li><a href=#3欧拉角-直观>3、欧拉角 <strong>直观</strong></a></li><li><a href=#4四元数>4、四元数</a></li></ul></li><li><a href=#相机模型>相机模型</a><ul><li><a href=#针孔相机模型>针孔相机模型</a></li><li><a href=#双目相机>双目相机</a></li><li><a href=#rgb-d相机-结构光tof>RGB-D相机 结构光、ToF</a></li></ul></li><li><a href=#相机畸变>相机畸变</a><ul><li><a href=#径向畸变>径向畸变</a></li><li><a href=#切向畸变>切向畸变</a></li><li><a href=#畸变校正>畸变校正</a></li></ul></li><li><a href=#相机标定>相机标定</a><ul><li><a href=#相机标定的目的>相机标定的目的</a></li></ul></li><li><a href=#视觉里程计>视觉里程计</a><ul><li><a href=#视觉里程计的基本原理>视觉里程计的基本原理</a></li><li><a href=#视觉里程计的分类>视觉里程计的分类</a></li><li><a href=#常用的视觉里程计算法>常用的视觉里程计算法</a></li></ul></li><li><a href=#2d-2d-对极几何>2d-2d 对极几何</a><ul><li><a href=#基础矩阵f>基础矩阵F</a></li><li><a href=#本质矩阵e>本质矩阵E</a></li><li><a href=#单应矩阵h>单应矩阵H</a></li><li><a href=#自由度>自由度</a></li></ul></li><li><a href=#3d-2d-pnp>3d-2d PnP</a><ul><li><a href=#pnp问题的定义>PnP问题的定义</a></li><li><a href=#pnp问题的求解步骤>PnP问题的求解步骤</a></li><li><a href=#dltdirect-linear-transformation><strong>DLT（Direct Linear Transformation）</strong></a></li><li><a href=#p3p>P3P</a></li><li><a href=#bundle-adjustment>Bundle Adjustment</a></li><li><a href=#光束法平差的基本原理>光束法平差的基本原理</a></li></ul></li><li><a href=#3d-3d-icp>3d-3d ICP</a></li><li><a href=#三角测量空间前方交会-已知观察未知>三角测量（空间前方交会 已知观察未知）</a></li><li><a href=#orb-slam2>orb-slam2</a></li></ul></li><li><a href=#三维重建>三维重建</a><ul><li><a href=#sfm算法原理>sfm算法原理</a><ul><li><a href=#sfm-流程总结><strong>SfM 流程总结：</strong></a></li></ul></li><li><a href=#colmap>colmap</a></li></ul></li><li><a href=#openmvs>openMVS</a><ul><li><a href=#nerf>NeRF</a></li><li><a href=#sfm与slam关系>sfm与slam关系</a></li><li><a href=#ply数据格式>ply数据格式</a></li></ul></li><li><a href=#传统图像算法>传统图像算法</a><ul><li><a href=#特征提取><strong>特征提取</strong></a><ul><li><a href=#主方向与描述子的关系>主方向与描述子的关系</a></li></ul></li><li><a href=#边缘检测>边缘检测</a></li><li><a href=#图像拼接>图像拼接</a></li><li><a href=#特征点法最常用>特征点法（最常用）</a><ul><li><a href=#总结对比>总结对比</a></li></ul></li><li><a href=#光流法>光流法</a></li><li><a href=#图像配准>图像配准</a></li><li><a href=#去噪增强>去噪、增强</a></li><li><a href=#形态学处理>形态学处理</a></li><li><a href=#hdr><strong>HDR</strong></a></li><li><a href=#变换技术>变换技术</a><ul><li><a href=#傅里叶变换><strong>傅里叶变换</strong></a></li><li><a href=#空间域与频率域>空间域与频率域</a></li><li><a href=#低频与高频>低频与高频</a></li></ul></li><li><a href=#raw图像>RAW图像</a></li><li><a href=#isp-图像信号处理>ISP 图像信号处理</a><ul><li><a href=#主要isp算法模块>主要ISP算法模块</a></li></ul></li><li><a href=#3a算法>3A算法</a><ul><li><a href=#1-自动对焦auto-focusaf>1. 自动对焦（Auto Focus，AF）</a></li><li><a href=#2-自动曝光auto-exposureae>2. 自动曝光（Auto Exposure，AE）</a></li><li><a href=#3-自动白平衡auto-white-balanceawb>3. 自动白平衡（Auto White Balance，AWB）</a></li></ul></li></ul></li><li><a href=#计算机视觉>计算机视觉</a><ul><li><a href=#图像分类>图像分类</a><ul><li><a href=#resnet-只学习输入与残差的映射>resnet 只学习输入与残差的映射</a></li></ul></li><li><a href=#目标检测><strong>目标检测</strong></a><ul><li><a href=#yolo>yolo</a></li><li><a href=#yolo的核心思想>YOLO的核心思想</a></li><li><a href=#yolo的具体工作流程>YOLO的具体工作流程</a></li><li><a href=#yolo的主要版本演变>YOLO的主要版本演变</a></li><li><a href=#yolov3-vs-yolov5-性能对比>YOLOv3 vs YOLOv5 性能对比</a></li><li><a href=#yolo的优缺点>YOLO的优缺点</a></li><li><a href=#ssd>ssd</a></li><li><a href=#rcnn系列>rcnn系列</a></li><li><a href=#nms>NMS</a></li></ul></li><li><a href=#图像分割>图像分割</a><ul><li><a href=#空洞空间金字塔池化aspp>空洞空间金字塔池化（ASPP）</a></li></ul></li><li><a href=#点云分割>点云分割</a></li><li><a href=#pointnet>pointnet</a></li><li><a href=#pointnet-1>pointnet++</a></li></ul></li><li><a href=#硕士论文改进>硕士论文改进</a></li><li><a href=#模型压缩>模型压缩</a><ul><li><a href=#经典轻量化模型>经典轻量化模型</a><ul><li><a href=#squeezenet-改进alexnet设计fire-module>squeezenet 改进AlexNet，设计Fire Module，</a></li><li><a href=#mobilenet><strong>mobilenet</strong></a></li><li><a href=#深度可分离卷积的组成>深度可分离卷积的组成</a></li><li><a href=#shufflenet>shufflenet</a></li><li><a href=#xception>Xception</a></li></ul></li><li><a href=#轻量化模型设计>轻量化模型设计</a></li></ul></li><li><a href=#机器学习>机器学习</a><ul><li><a href=#分类>分类</a><ul><li><a href=#svm>SVM</a></li><li><a href=#1-c惩罚系数>1. <code>C</code>（惩罚系数）</a></li><li><a href=#2-kernel核函数类型>2. <code>kernel</code>（核函数类型）</a></li><li><a href=#3-degree多项式核的阶数>3. <code>degree</code>（多项式核的阶数）</a></li></ul></li><li><a href=#回归>回归</a></li><li><a href=#聚类无监督>聚类（无监督）</a><ul><li><a href=#算法伪代码>算法伪代码</a></li></ul></li><li><a href=#降维>降维</a></li><li><a href=#离群检测>离群检测</a><ul><li><a href=#1-基于统计的方法>1. 基于统计的方法</a></li></ul></li></ul></li><li><a href=#深度学习>深度学习</a><ul><li><a href=#cnn>CNN</a></li><li><a href=#过拟合欠拟合>过拟合、欠拟合</a><ul><li><a href=#过拟合-模型太复杂测试集上效果差>过拟合 模型太复杂，测试集上效果差</a></li><li><a href=#欠拟合-模型太简单训练集-测试集上效果都差>欠拟合 模型太简单，训练集、 测试集上效果都差</a></li></ul></li><li><a href=#交叉验证>交叉验证</a></li><li><a href=#激活函数>激活函数</a></li><li><a href=#sigmoid和softmax区别>sigmoid和softmax区别</a></li><li><a href=#损失函数>损失函数</a><ul><li><a href=#分类损失>分类损失</a></li><li><a href=#回归损失>回归损失</a></li><li><a href=#检测分割>检测、分割</a></li></ul></li><li><a href=#优化算法>优化算法</a><ul><li><a href=#adam>Adam</a></li></ul></li><li><a href=#rnn>RNN</a></li><li><a href=#cnn与rnn区别>CNN与RNN区别</a><ul><li><a href=#输入数据类型>输入数据类型</a></li><li><a href=#训练和计算>训练和计算</a></li></ul></li><li><a href=#cnn与mlp>CNN与MLP</a><ul><li><a href=#上采样反池化反卷积>上采样、反池化、反卷积</a></li><li><a href=#bnln>BN、LN</a></li></ul></li></ul></li><li><a href=#数据分析>数据分析</a></li><li><a href=#大模型>大模型</a><ul><li><a href=#gan>GAN</a></li><li><a href=#chatgpt>chatgpt</a></li><li><a href=#transformer架构>transformer架构</a><ul><li><a href=#attention-is-all-you-need><strong>Attention Is All You Need</strong></a></li><li><a href=#注意力机制>注意力机制</a></li></ul></li><li><a href=#自注意力机制-self-attention>自注意力机制 <strong>（Self-Attention）</strong></a><ul><li><a href=#concat-操作>concat 操作</a></li></ul></li><li><a href=#线性变换>线性变换</a><ul><li><a href=#1-解码器输出>1. 解码器输出</a></li><li><a href=#2-线性变换>2. 线性变换</a></li><li><a href=#3-输出解释>3. 输出解释</a></li><li><a href=#4-进一步处理>4. 进一步处理</a></li><li><a href=#总结>总结</a></li></ul></li><li><a href=#softmax激活>softmax激活</a><ul><li><a href=#1-模型结构>1. 模型结构</a></li><li><a href=#2-生成输出序列的步骤>2. 生成输出序列的步骤</a></li><li><a href=#21-输入处理>2.1 输入处理</a></li><li><a href=#22-线性变换>2.2 线性变换</a></li><li><a href=#23-softmax-函数>2.3 Softmax 函数</a></li><li><a href=#3-输出解释-1>3. 输出解释</a></li><li><a href=#4-损失计算>4. 损失计算</a></li><li><a href=#总结-1>总结</a></li></ul></li><li><a href=#训练中的shifted-right操作>训练中的shifted right操作</a><ul><li><a href=#1-shifted-right-的操作步骤>1. Shifted Right 的操作步骤</a></li><li><a href=#2-为什么需要-shifted-right>2. 为什么需要 Shifted Right</a></li><li><a href=#3-实际应用>3. 实际应用</a></li><li><a href=#4-masking-和-shifted-right-配合>4. Masking 和 Shifted Right 配合</a></li><li><a href=#总结-2>总结</a></li><li><a href=#ln层>LN层</a></li></ul></li></ul></li><li><a href=#量子点光谱传感技术>量子点光谱传感技术</a></li><li><a href=#火点监测>火点监测</a><ul><li><a href=#遥感火点监测原理>遥感火点监测原理</a></li><li><a href=#静止卫星火点判识计算机实现过程><strong>静止卫星火点判识计算机实现过程</strong></a></li><li><a href=#极轨>极轨</a></li></ul></li></ul></nav></div></div><div class=content id=content><p>deep-learning、CV（2d、3d）、RS、optical</p><h1 id=视觉slam>视觉slam</h1><p><a href=https://wykxwyc.github.io/2019/03/21/Small-Talk/ target=_blank rel="noopener noreffer">https://wykxwyc.github.io/2019/03/21/Small-Talk/</a></p><h2 id=视觉slam框架>视觉slam框架</h2><p><img class=lazyload src=/svg/loading.min.svg data-src=Summary%20127e4020f060808da8e0f23caf646e39/image.png data-srcset="Summary%20127e4020f060808da8e0f23caf646e39/image.png, Summary%20127e4020f060808da8e0f23caf646e39/image.png 1.5x, Summary%20127e4020f060808da8e0f23caf646e39/image.png 2x" data-sizes=auto alt=Summary%20127e4020f060808da8e0f23caf646e39/image.png title=image.png></p><p>整个视觉 SLAM 流程包括以下步骤。</p><ol><li>传感器信息读取。在视觉 SLAM 中主要为相机图像信息的读取和预处理。如果是在机器人中，还可能有码盘、惯性传感器等信息的读取和同步。</li><li>视觉里程计（Visual Odometry，VO）。视觉里程计的任务是<strong>估算相邻图像间相机的运动，以及局部地图的样子</strong>。VO 又称为前端（Front End）。</li><li>后端优化（Optimization）。后端接受不同时刻视觉里程计测量的相机位姿，以及<strong>回环检测</strong>的信息，对它们进行<strong>优化，得到全局一致的轨迹和地图</strong>。由于接在 VO 之后，又称为后端（Back End）。</li><li>回环检测（Loop Closing）。回环检测判断机器人是否到达过先前的位置。如果检测到回环，它会把信息提供给后端进行处理。</li><li>建图（Mapping）。它根据估计的轨迹，建立与任务要求对应的地图。</li></ol><h2 id=数学基础>数学基础</h2><p>向量一般默认指列向量</p><h3 id=齐次坐标>齐次坐标</h3><p><a href=https://blog.csdn.net/zhuiqiuzhuoyue583/article/details/95228010 target=_blank rel="noopener noreffer"></a></p><p><a href=https://blog.csdn.net/zhuiqiuzhuoyue583/article/details/95230246 target=_blank rel="noopener noreffer">https://blog.csdn.net/zhuiqiuzhuoyue583/article/details/95230246</a></p><p><strong>齐次坐标可以表示无穷远处的点。</strong></p><p><strong>齐次坐标把各种变换都统一了起来，即 把缩放，旋转，平移等变换都统一起来，都表示成一连串的矩阵相乘的形式。保证了形式上的线性一致性。</strong></p><p>齐次坐标就是将一个原本是n维的向量用一个n+1维向量来表示，<strong>合并矩阵运算中的乘法和加法，简化运算操作。</strong></p><h3 id=内外积>内外积</h3><p><a href=https://www.zhihu.com/question/21080171 target=_blank rel="noopener noreffer">https://www.zhihu.com/question/21080171</a></p><p><strong>内积</strong></p><p>数量积 点乘 一个数</p><p><img class=lazyload src=/svg/loading.min.svg data-src=Summary%20127e4020f060808da8e0f23caf646e39/image%201.png data-srcset="Summary%20127e4020f060808da8e0f23caf646e39/image%201.png, Summary%20127e4020f060808da8e0f23caf646e39/image%201.png 1.5x, Summary%20127e4020f060808da8e0f23caf646e39/image%201.png 2x" data-sizes=auto alt=Summary%20127e4020f060808da8e0f23caf646e39/image%201.png title=image.png></p><p><strong>外积</strong></p><p>向量积 叉乘 一个向量</p><p><img class=lazyload src=/svg/loading.min.svg data-src=Summary%20127e4020f060808da8e0f23caf646e39/image%202.png data-srcset="Summary%20127e4020f060808da8e0f23caf646e39/image%202.png, Summary%20127e4020f060808da8e0f23caf646e39/image%202.png 1.5x, Summary%20127e4020f060808da8e0f23caf646e39/image%202.png 2x" data-sizes=auto alt=Summary%20127e4020f060808da8e0f23caf646e39/image%202.png title=image.png></p><p>方向垂直于两个向量，大小为 |a| |b| sin ⟨a, b⟩，是两个向量张成的四边形的有向面积</p><p>把外积 a × b 写成了矩阵与向量的乘法 a∧ b，把它变成了线性运算</p><p>∧ 记成一个反对称符号，<strong>任意向量都对应着唯一的一个反对称矩阵</strong></p><p><img class=lazyload src=/svg/loading.min.svg data-src=Summary%20127e4020f060808da8e0f23caf646e39/image%203.png data-srcset="Summary%20127e4020f060808da8e0f23caf646e39/image%203.png, Summary%20127e4020f060808da8e0f23caf646e39/image%203.png 1.5x, Summary%20127e4020f060808da8e0f23caf646e39/image%203.png 2x" data-sizes=auto alt=Summary%20127e4020f060808da8e0f23caf646e39/image%203.png title=image.png></p><h2 id=svd分解>SVD分解</h2><p><a href=https://zhuanlan.zhihu.com/p/79669616 target=_blank rel="noopener noreffer">https://zhuanlan.zhihu.com/p/79669616</a></p><p><a href=https://zhaoxuhui.top/blog/2018/03/17/SVD&amp;SLAM.html target=_blank rel="noopener noreffer">https://zhaoxuhui.top/blog/2018/03/17/SVD&SLAM.html</a></p><p><a href=https://www.cnblogs.com/LeftNotEasy/archive/2011/01/19/svd-and-applications.html target=_blank rel="noopener noreffer">https://www.cnblogs.com/LeftNotEasy/archive/2011/01/19/svd-and-applications.html</a></p><h3 id=1-svd的数学形式>1. <strong>SVD的数学形式</strong></h3><p><img class=lazyload src=/svg/loading.min.svg data-src=Summary%20127e4020f060808da8e0f23caf646e39/image%204.png data-srcset="Summary%20127e4020f060808da8e0f23caf646e39/image%204.png, Summary%20127e4020f060808da8e0f23caf646e39/image%204.png 1.5x, Summary%20127e4020f060808da8e0f23caf646e39/image%204.png 2x" data-sizes=auto alt=Summary%20127e4020f060808da8e0f23caf646e39/image%204.png title=image.png></p><h3 id=2-奇异值>2. <strong>奇异值</strong></h3><ul><li>奇异值是矩阵 A 的非负平方根，表示了矩阵在不同方向上的“重要性”或“能量”。较大的奇异值对应于矩阵中更重要的特征，而较小的奇异值则表示较不重要的特征。</li><li>通过奇异值的大小，可以判断矩阵的秩、条件数以及数据的可分性。</li></ul><h3 id=3-几何解释>3. <strong>几何解释</strong></h3><ul><li><p>SVD 的几何解释是将数据空间的线性变换可视化。矩阵 A 可以被视为将输入空间中的点映射到输出空间。通过 SVD，可以将这个映射过程分解为三个步骤：</p><p>AA</p><ol><li><strong>旋转</strong>（通过 VT）将输入空间中的点旋转到新的坐标系。</li><li><strong>缩放</strong>（通过 Σ）在新的坐标系中按奇异值的大小缩放。</li><li><strong>再次旋转</strong>（通过 U）将缩放后的点旋转到输出空间。</li></ol></li></ul><p><strong>应用</strong></p><p>数据降维 PCA</p><p>矩阵近似</p><p>图像压缩</p><p>numpy scipy已实现</p><h2 id=非线性优化>非线性优化</h2><h2 id=几种投影变换>几种投影变换</h2><p><strong>仿射变换=线性变换+平移</strong></p><p><strong>平移、旋转、放缩、剪切、反射</strong></p><p><a href=https://blog.csdn.net/sgfmby1994/article/details/62426331 target=_blank rel="noopener noreffer">https://blog.csdn.net/sgfmby1994/article/details/62426331</a></p><p><a href=https://www.cnblogs.com/shine-lee/p/10950963.html target=_blank rel="noopener noreffer">https://www.cnblogs.com/shine-lee/p/10950963.html</a></p><p>projective/perspective：三维到二维的变换</p><p>Homography：两个平面间的映射，描述<strong>同一平面在不同视角下的变换</strong></p><p><img class=lazyload src=/svg/loading.min.svg data-src=Summary%20127e4020f060808da8e0f23caf646e39/image%205.png data-srcset="Summary%20127e4020f060808da8e0f23caf646e39/image%205.png, Summary%20127e4020f060808da8e0f23caf646e39/image%205.png 1.5x, Summary%20127e4020f060808da8e0f23caf646e39/image%205.png 2x" data-sizes=auto alt=Summary%20127e4020f060808da8e0f23caf646e39/image%205.png title=image.png></p><p><img class=lazyload src=/svg/loading.min.svg data-src=Summary%20127e4020f060808da8e0f23caf646e39/image%206.png data-srcset="Summary%20127e4020f060808da8e0f23caf646e39/image%206.png, Summary%20127e4020f060808da8e0f23caf646e39/image%206.png 1.5x, Summary%20127e4020f060808da8e0f23caf646e39/image%206.png 2x" data-sizes=auto alt=Summary%20127e4020f060808da8e0f23caf646e39/image%206.png title=image.png></p><p><img class=lazyload src=/svg/loading.min.svg data-src=Summary%20127e4020f060808da8e0f23caf646e39/image%207.png data-srcset="Summary%20127e4020f060808da8e0f23caf646e39/image%207.png, Summary%20127e4020f060808da8e0f23caf646e39/image%207.png 1.5x, Summary%20127e4020f060808da8e0f23caf646e39/image%207.png 2x" data-sizes=auto alt=Summary%20127e4020f060808da8e0f23caf646e39/image%207.png title=image.png></p><p><img class=lazyload src=/svg/loading.min.svg data-src=Summary%20127e4020f060808da8e0f23caf646e39/image%208.png data-srcset="Summary%20127e4020f060808da8e0f23caf646e39/image%208.png, Summary%20127e4020f060808da8e0f23caf646e39/image%208.png 1.5x, Summary%20127e4020f060808da8e0f23caf646e39/image%208.png 2x" data-sizes=auto alt=Summary%20127e4020f060808da8e0f23caf646e39/image%208.png title=image.png></p><p><strong>二维仿射变换</strong>是一种线性变换，保持直线性和平行性。可以进行平移、旋转、缩放、剪切等操作，常见于图像处理、计算机图形学等领域。</p><p>二维仿射变换可以用一个 3×3 的矩阵来表示：</p><p><img class=lazyload src=/svg/loading.min.svg data-src=Summary%20127e4020f060808da8e0f23caf646e39/image%209.png data-srcset="Summary%20127e4020f060808da8e0f23caf646e39/image%209.png, Summary%20127e4020f060808da8e0f23caf646e39/image%209.png 1.5x, Summary%20127e4020f060808da8e0f23caf646e39/image%209.png 2x" data-sizes=auto alt=Summary%20127e4020f060808da8e0f23caf646e39/image%209.png title=image.png></p><p>其中，(x,y)(x, y)(x,y) 是原始点的坐标，(x′,y′)(x&rsquo;, y&rsquo;)(x′,y′) 是变换后的坐标，a,b,c,da, b, c, da,b,c,d 定义线性变换（旋转、缩放、剪切等），而 tx,tytx, tytx,ty 代表平移。</p><h3 id=三维仿射变换>三维仿射变换</h3><p><strong>三维仿射变换</strong>扩展了二维仿射变换，允许在三维空间中进行类似的操作，如平移、旋转、缩放和剪切等。它通常用 4×4 的矩阵来表示：</p><p><img class=lazyload src=/svg/loading.min.svg data-src=Summary%20127e4020f060808da8e0f23caf646e39/image%2010.png data-srcset="Summary%20127e4020f060808da8e0f23caf646e39/image%2010.png, Summary%20127e4020f060808da8e0f23caf646e39/image%2010.png 1.5x, Summary%20127e4020f060808da8e0f23caf646e39/image%2010.png 2x" data-sizes=auto alt=Summary%20127e4020f060808da8e0f23caf646e39/image%2010.png title=image.png></p><p>其中，(x,y,z)(x, y, z)(x,y,z) 是原始点的坐标，(x′,y′,z′)(x&rsquo;, y&rsquo;, z&rsquo;)(x′,y′,z′) 是变换后的坐标，矩阵 aija_{ij}aij 代表线性变换，而 tx,ty,tztx, ty, tztx,ty,tz 代表平移。</p><h2 id=三维刚体运动表示>三维刚体运动表示</h2><h3 id=1旋转矩阵--不够紧凑冗余优化困难带有约束>1、旋转矩阵 <strong>不够紧凑、冗余、优化困难（带有约束）</strong></h3><p>4×4 的矩阵，将平移和旋转结合在一起，写成齐次坐标形式</p><p><img class=lazyload src=/svg/loading.min.svg data-src=Summary%20127e4020f060808da8e0f23caf646e39/image%2011.png data-srcset="Summary%20127e4020f060808da8e0f23caf646e39/image%2011.png, Summary%20127e4020f060808da8e0f23caf646e39/image%2011.png 1.5x, Summary%20127e4020f060808da8e0f23caf646e39/image%2011.png 2x" data-sizes=auto alt=Summary%20127e4020f060808da8e0f23caf646e39/image%2011.png title=image.png></p><p>R是9个量表示3个自由度</p><p>有三个互相垂直的坐标轴（x, y, z）。旋转矩阵的 <strong>每一列</strong> 表示<strong>旋转后的坐标轴在原坐标系中的方向。</strong></p><p>例如世界坐标系到相机坐标系的旋转变换中，旋转矩阵的列向量表示的是相机坐标系的x、y、z轴在世界坐标系中的方向。</p><p><strong>列向量表示旋转后的基向量</strong>：</p><ul><li>旋转矩阵的每一列表示旋转后坐标轴上的基向量（单位向量）在原坐标系中的表示。</li><li>例如，第一列表示旋转后x轴上的基向量在原坐标系中的坐标，第二列和第三列分别表示旋转后y轴和z轴上的基向量。</li></ul><p><strong>旋转角度和轴</strong>：</p><ul><li>旋转矩阵可以唯一地确定一个旋转的角度和旋转轴。</li><li>通过旋转矩阵，可以计算出旋转的角度（通常使用弧度表示）和旋转轴的方向。</li></ul><p><strong>无奇异性</strong></p><h3 id=2旋转向量-轴-角表示法>2、旋转向量 轴-角表示法</h3><p>方向与旋转轴一致，长度等于旋转角**（3维向量）**</p><p>旋转向量通常表示为 r=θu，其中：</p><ul><li>r是旋转向量。</li><li>θ是旋转角度（弧度）。例 θ=π/2</li><li>u是单位向量，表示旋转轴。例 u=(0.707,0,0.707)</li></ul><h3 id=3欧拉角-直观>3、欧拉角 <strong>直观</strong></h3><p>Yaw（偏航角）、Pitch（俯仰角）和 Roll（翻滚角）</p><p>万向锁问题（丢失自由度）rpy</p><h3 id=4四元数>4、四元数</h3><p>既是紧凑的，也没有奇异性</p><p>没有欧拉角的万向节锁问题，适合进行连续旋转的插值计算</p><p><img class=lazyload src=/svg/loading.min.svg data-src=Summary%20127e4020f060808da8e0f23caf646e39/image%2012.png data-srcset="Summary%20127e4020f060808da8e0f23caf646e39/image%2012.png, Summary%20127e4020f060808da8e0f23caf646e39/image%2012.png 1.5x, Summary%20127e4020f060808da8e0f23caf646e39/image%2012.png 2x" data-sizes=auto alt=Summary%20127e4020f060808da8e0f23caf646e39/image%2012.png title=image.png></p><p><img class=lazyload src=/svg/loading.min.svg data-src=Summary%20127e4020f060808da8e0f23caf646e39/image%2013.png data-srcset="Summary%20127e4020f060808da8e0f23caf646e39/image%2013.png, Summary%20127e4020f060808da8e0f23caf646e39/image%2013.png 1.5x, Summary%20127e4020f060808da8e0f23caf646e39/image%2013.png 2x" data-sizes=auto alt=Summary%20127e4020f060808da8e0f23caf646e39/image%2013.png title=image.png></p><p>表示方法 维度 直观性 计算效率 是否有奇异性
旋转矩阵 3×3 一般 慢（9 个数） 无
欧拉角 3 直观 快 有（万向锁问题）
四元数 4 不直观 快 无
旋转向量 3 直观 快 无</p><h2 id=相机模型>相机模型</h2><p>物理空间Z=1的<strong>归一化平面与像素坐标系只差一个内参矩阵K</strong>，Puv = KPc</p><h3 id=针孔相机模型>针孔相机模型</h3><p><img class=lazyload src=/svg/loading.min.svg data-src=Summary%20127e4020f060808da8e0f23caf646e39/image%2014.png data-srcset="Summary%20127e4020f060808da8e0f23caf646e39/image%2014.png, Summary%20127e4020f060808da8e0f23caf646e39/image%2014.png 1.5x, Summary%20127e4020f060808da8e0f23caf646e39/image%2014.png 2x" data-sizes=auto alt=Summary%20127e4020f060808da8e0f23caf646e39/image%2014.png title=image.png></p><p><img class=lazyload src=/svg/loading.min.svg data-src=Summary%20127e4020f060808da8e0f23caf646e39/image%2015.png data-srcset="Summary%20127e4020f060808da8e0f23caf646e39/image%2015.png, Summary%20127e4020f060808da8e0f23caf646e39/image%2015.png 1.5x, Summary%20127e4020f060808da8e0f23caf646e39/image%2015.png 2x" data-sizes=auto alt=Summary%20127e4020f060808da8e0f23caf646e39/image%2015.png title=image.png></p><p><img class=lazyload src=/svg/loading.min.svg data-src=Summary%20127e4020f060808da8e0f23caf646e39/image%2016.png data-srcset="Summary%20127e4020f060808da8e0f23caf646e39/image%2016.png, Summary%20127e4020f060808da8e0f23caf646e39/image%2016.png 1.5x, Summary%20127e4020f060808da8e0f23caf646e39/image%2016.png 2x" data-sizes=auto alt=Summary%20127e4020f060808da8e0f23caf646e39/image%2016.png title=image.png></p><p><img class=lazyload src=/svg/loading.min.svg data-src=Summary%20127e4020f060808da8e0f23caf646e39/image%2017.png data-srcset="Summary%20127e4020f060808da8e0f23caf646e39/image%2017.png, Summary%20127e4020f060808da8e0f23caf646e39/image%2017.png 1.5x, Summary%20127e4020f060808da8e0f23caf646e39/image%2017.png 2x" data-sizes=auto alt=Summary%20127e4020f060808da8e0f23caf646e39/image%2017.png title=image.png></p><p>比例因子α, β 的单位为<strong>像素/米，把物理距离转换为像素距离</strong></p><p>相机内参fx , fy代表<strong>焦距f长度对应的像素距离</strong></p><p>主点坐标cx,cy代表相机光心在图像上的投影点位置，一般是图像的像素中心</p><p>单目相机的成像过程（针孔相机模型）：</p><ol><li>首先，世界坐标系下有一个固定的点 P ，世界坐标为 Pw 。</li><li>由于相机在运动，它的运动由 R, t 或变换矩阵 T ∈ SE(3) 描述。P 的相机坐标为 P̃c =RPw + t。</li><li>这时的 P̃c 的分量为 X, Y, Z，把它们<strong>投影到归一化平面 Z = 1 上</strong>，得到 P 的归一化坐标：Pc = [X/Z, Y /Z, 1]T。</li><li>有畸变时，根据畸变参数计算 Pc 发生畸变后的坐标。</li><li>最后，P 的归一化坐标经过内参后，对应到它的像素坐标：Puv = KPc 。
综上所述，我们一共谈到了四种坐标：世界坐标、相机坐标、归一化坐标和像素坐标。</li></ol><h3 id=双目相机>双目相机</h3><p>利用基线长度b、视差d来估算深度</p><p><img class=lazyload src=/svg/loading.min.svg data-src=Summary%20127e4020f060808da8e0f23caf646e39/image%2018.png data-srcset="Summary%20127e4020f060808da8e0f23caf646e39/image%2018.png, Summary%20127e4020f060808da8e0f23caf646e39/image%2018.png 1.5x, Summary%20127e4020f060808da8e0f23caf646e39/image%2018.png 2x" data-sizes=auto alt=Summary%20127e4020f060808da8e0f23caf646e39/image%2018.png title=image.png></p><p><img class=lazyload src=/svg/loading.min.svg data-src=Summary%20127e4020f060808da8e0f23caf646e39/image%2019.png data-srcset="Summary%20127e4020f060808da8e0f23caf646e39/image%2019.png, Summary%20127e4020f060808da8e0f23caf646e39/image%2019.png 1.5x, Summary%20127e4020f060808da8e0f23caf646e39/image%2019.png 2x" data-sizes=auto alt=Summary%20127e4020f060808da8e0f23caf646e39/image%2019.png title=image.png></p><p><strong>视差越小，深度越远</strong></p><p><strong>基线越长，测量越远</strong></p><h3 id=rgb-d相机-结构光tof>RGB-D相机 结构光、ToF</h3><p>深度图转点云 相似三角形原理</p><p><img class=lazyload src=/svg/loading.min.svg data-src=Summary%20127e4020f060808da8e0f23caf646e39/image%2020.png data-srcset="Summary%20127e4020f060808da8e0f23caf646e39/image%2020.png, Summary%20127e4020f060808da8e0f23caf646e39/image%2020.png 1.5x, Summary%20127e4020f060808da8e0f23caf646e39/image%2020.png 2x" data-sizes=auto alt=Summary%20127e4020f060808da8e0f23caf646e39/image%2020.png title=image.png></p><p><img class=lazyload src=/svg/loading.min.svg data-src=Summary%20127e4020f060808da8e0f23caf646e39/image%2021.png data-srcset="Summary%20127e4020f060808da8e0f23caf646e39/image%2021.png, Summary%20127e4020f060808da8e0f23caf646e39/image%2021.png 1.5x, Summary%20127e4020f060808da8e0f23caf646e39/image%2021.png 2x" data-sizes=auto alt=Summary%20127e4020f060808da8e0f23caf646e39/image%2021.png title=image.png></p><p>假设内参已知，知道Z的信息后，就可推算出像素坐标u,v对应的X,Y空间坐标</p><p><img class=lazyload src=/svg/loading.min.svg data-src=Summary%20127e4020f060808da8e0f23caf646e39/image%2022.png data-srcset="Summary%20127e4020f060808da8e0f23caf646e39/image%2022.png, Summary%20127e4020f060808da8e0f23caf646e39/image%2022.png 1.5x, Summary%20127e4020f060808da8e0f23caf646e39/image%2022.png 2x" data-sizes=auto alt=Summary%20127e4020f060808da8e0f23caf646e39/image%2022.png title=image.png></p><h2 id=相机畸变>相机畸变</h2><h3 id=径向畸变>径向畸变</h3><p>由透镜形状引起的畸变</p><p>桶形畸变和枕形畸变</p><p><img class=lazyload src=/svg/loading.min.svg data-src=Summary%20127e4020f060808da8e0f23caf646e39/image%2023.png data-srcset="Summary%20127e4020f060808da8e0f23caf646e39/image%2023.png, Summary%20127e4020f060808da8e0f23caf646e39/image%2023.png 1.5x, Summary%20127e4020f060808da8e0f23caf646e39/image%2023.png 2x" data-sizes=auto alt=Summary%20127e4020f060808da8e0f23caf646e39/image%2023.png title=image.png></p><h3 id=切向畸变>切向畸变</h3><p>相机的组装过程中透镜和成像面不严格平行</p><p><img class=lazyload src=/svg/loading.min.svg data-src=Summary%20127e4020f060808da8e0f23caf646e39/image%2024.png data-srcset="Summary%20127e4020f060808da8e0f23caf646e39/image%2024.png, Summary%20127e4020f060808da8e0f23caf646e39/image%2024.png 1.5x, Summary%20127e4020f060808da8e0f23caf646e39/image%2024.png 2x" data-sizes=auto alt=Summary%20127e4020f060808da8e0f23caf646e39/image%2024.png title=image.png></p><h3 id=畸变校正>畸变校正</h3><p>对于相机坐标系中的一点 P ，我们能够通过 5 个畸变系数找到这个点在像素平面上的正确位置：</p><ol><li>将三维空间点投影到归一化图像平面。设它的归一化坐标为 [x, y]T 。</li><li>对<strong>归一化平面上</strong>的点计算径向畸变和切向畸变。Z值（深度）为1的假设平面</li></ol><p><img class=lazyload src=/svg/loading.min.svg data-src=Summary%20127e4020f060808da8e0f23caf646e39/image%2025.png data-srcset="Summary%20127e4020f060808da8e0f23caf646e39/image%2025.png, Summary%20127e4020f060808da8e0f23caf646e39/image%2025.png 1.5x, Summary%20127e4020f060808da8e0f23caf646e39/image%2025.png 2x" data-sizes=auto alt=Summary%20127e4020f060808da8e0f23caf646e39/image%2025.png title=image.png></p><p>将畸变后的点通过内参数矩阵投影到像素平面，得到该点在图像上的正确位置。</p><h2 id=相机标定>相机标定</h2><h3 id=相机标定的目的>相机标定的目的</h3><ol><li><strong>校正畸变</strong>：大多数相机镜头都会产生一定的畸变，常见的有径向畸变和切向畸变。标定可以获取相机的畸变系数，进而校正图像，使其符合真实场景。</li><li><strong>获取内参矩阵</strong>：相机的内参描述了相机的焦距、主点位置等信息。内参矩阵在将三维空间的点投影到二维图像上时非常重要。</li><li><strong>获取外参矩阵</strong>：外参描述了相机在三维空间中的位置和姿态，通常以旋转矩阵和位移向量表示。外参矩阵可以用于计算相机与世界坐标系的关系。</li></ol><p><a href=https://zhuanlan.zhihu.com/p/94244568 target=_blank rel="noopener noreffer">zhuanlan.zhihu.com</a></p><p>张正友标定法标定相机的内外参数的思路如下：</p><p>将世界坐标系固定于棋盘格上，则棋盘格上任一点的物理坐标W=0，任一角点的U、V坐标也已知，<strong>最少4个角点就可以求解Homography矩阵</strong>。</p><p>1）、求解内参矩阵与外参矩阵的积，即H；</p><p>2）、求解内参矩阵K；</p><p>3）、求解外参矩阵T，KT=H；</p><p>4）、求解畸变参数。</p><h2 id=视觉里程计>视觉里程计</h2><p>特征点法、光流法（灰度不变假设）</p><p><a href=https://blog.csdn.net/qq_41839222/article/details/86483071 target=_blank rel="noopener noreffer">https://blog.csdn.net/qq_41839222/article/details/86483071</a></p><p><img class=lazyload src=/svg/loading.min.svg data-src=Summary%20127e4020f060808da8e0f23caf646e39/image%2026.png data-srcset="Summary%20127e4020f060808da8e0f23caf646e39/image%2026.png, Summary%20127e4020f060808da8e0f23caf646e39/image%2026.png 1.5x, Summary%20127e4020f060808da8e0f23caf646e39/image%2026.png 2x" data-sizes=auto alt=Summary%20127e4020f060808da8e0f23caf646e39/image%2026.png title=image.png></p><p>视觉里程计（Visual Odometry，VO）是一种基于图像的技术，通过分<strong>析连续图像帧来估计相机（或移动设备）的运动轨迹</strong>。视觉里程计广泛应用于机器人、自动驾驶、增强现实（AR）和虚拟现实（VR）等领域，帮助设备在没有GPS或其他定位系统的环境中实现自我定位和导航。</p><h3 id=视觉里程计的基本原理>视觉里程计的基本原理</h3><p>视觉里程计利用相机捕获的连续图像帧中的视觉信息，通过特征提取和匹配来计算相机在空间中的位姿变化。主要的流程通常包括以下几个步骤：</p><ol><li><strong>特征提取</strong>：在图像中检测关键特征点，常用的特征检测算法有SIFT、SURF、ORB等。这些特征点可以用来跟踪相机运动。</li><li><strong>特征匹配</strong>：在相邻图像帧中，匹配提取的特征点，得到每个特征点在不同帧中的对应关系。</li><li><strong>估计运动（姿态估计）</strong>：通过匹配的特征点，使用PnP、ICP（Iterative Closest Point）或双目三角测量等方法<strong>计算相机的姿态变化，即位移和旋转（外参）</strong>。</li><li><strong>优化和滤波</strong>：使用优化算法（如Bundle Adjustment）来最小化重投影误差，提高运动估计的精度。对于长时间运行的系统，还可以加入滤波算法（如卡尔曼滤波、粒子滤波）来提高系统的鲁棒性。</li></ol><h3 id=视觉里程计的分类>视觉里程计的分类</h3><p>视觉里程计根据使用的图像传感器类型和特征提取方式可分为以下几类：</p><ol><li><strong>单目视觉里程计</strong><ul><li>使用单个相机，通常依赖特征点的运动来估计相机的位姿变化。</li><li>优点：硬件要求低，计算相对简单。</li><li>缺点：由于缺少深度信息，精度有限，依赖于图像特征点的质量。</li></ul></li><li><strong>双目视觉里程计</strong><ul><li>使用双目相机，利用双目视差来获取三维深度信息。</li><li>优点：可以直接获取深度信息，更精确地估计位姿变化。</li><li>缺点：硬件要求高于单目视觉。</li></ul></li><li><strong>RGB-D视觉里程计</strong><ul><li>使用RGB-D相机（如Kinect）获取带有深度信息的图像。</li><li>优点：直接获取深度信息，适用于室内环境。</li><li>缺点：受光线影响大，通常只适用于近距离场景。</li></ul></li><li><strong>特征点法与直接法</strong><ul><li>特征点法：通过检测和跟踪图像中的特征点，计算相机的运动。常用算法有ORB-SLAM、LDSO等。</li><li>直接法：直接利用图像灰度信息进行配准，如DSO（Direct Sparse Odometry）。<strong>在光线稳定、纹理丰富的情况下，直接法效率更高</strong>。</li></ul></li></ol><h3 id=常用的视觉里程计算法>常用的视觉里程计算法</h3><ol><li><strong>ORB-SLAM</strong>：<ul><li>基于特征点的SLAM（Simultaneous Localization and Mapping）方法，采用ORB特征，适合于多种类型相机。ORB-SLAM包含回环检测和重定位等模块，非常适合长期运行。</li></ul></li><li><strong>LSD-SLAM</strong>：<ul><li>基于稀疏直接法的SLAM方法，使用图像灰度值进行位姿估计，适合在低纹理场景下运行。主要用于单目相机。</li></ul></li><li><strong>DSO（Direct Sparse Odometry）</strong>：<ul><li>直接法的视觉里程计，通过稀疏图像配准直接估计相机位姿，在非特征丰富的场景中表现良好，且计算速度较快。</li></ul></li><li><strong>VINS-Mono / VINS-Fusion</strong>：<ul><li>基于因子图优化的视觉里程计算法，融合IMU（惯性测量单元）数据，提升了单目相机在三维空间中的定位精度，特别适用于动态环境中。</li></ul></li></ol><h2 id=2d-2d-对极几何>2d-2d 对极几何</h2><p><a href=https://www.cnblogs.com/narjaja/p/10768179.html target=_blank rel="noopener noreffer">https://www.cnblogs.com/narjaja/p/10768179.html</a></p><p><strong>通过二维图像点的对应关系，恢复出在两帧之间摄像机的运动</strong></p><p><img class=lazyload src=/svg/loading.min.svg data-src=Summary%20127e4020f060808da8e0f23caf646e39/image%2027.png data-srcset="Summary%20127e4020f060808da8e0f23caf646e39/image%2027.png, Summary%20127e4020f060808da8e0f23caf646e39/image%2027.png 1.5x, Summary%20127e4020f060808da8e0f23caf646e39/image%2027.png 2x" data-sizes=auto alt=Summary%20127e4020f060808da8e0f23caf646e39/image%2027.png title=image.png></p><p>对极约束简洁地给出了两个匹配点的空间位置关系。相机位姿估计问题变为以下两步：</p><ol><li>根据配对点的像素位置求出 E 或者 F （都可以8点法求解）。</li><li>根据 E 或者 F 求出 R, t。
由于 E 和 F 只相差了相机内参，而内参在 SLAM 中通常是已知的 ，所以实践当中往往使用形式更简单的 E。</li></ol><p><img class=lazyload src=/svg/loading.min.svg data-src=Summary%20127e4020f060808da8e0f23caf646e39/image%2028.png data-srcset="Summary%20127e4020f060808da8e0f23caf646e39/image%2028.png, Summary%20127e4020f060808da8e0f23caf646e39/image%2028.png 1.5x, Summary%20127e4020f060808da8e0f23caf646e39/image%2028.png 2x" data-sizes=auto alt=Summary%20127e4020f060808da8e0f23caf646e39/image%2028.png title=image.png></p><pre><code>                                      x是归一化平面上的坐标，p是像素坐标。
</code></pre><h3 id=基础矩阵f>基础矩阵F</h3><p><img class=lazyload src=/svg/loading.min.svg data-src=Summary%20127e4020f060808da8e0f23caf646e39/image%2029.png data-srcset="Summary%20127e4020f060808da8e0f23caf646e39/image%2029.png, Summary%20127e4020f060808da8e0f23caf646e39/image%2029.png 1.5x, Summary%20127e4020f060808da8e0f23caf646e39/image%2029.png 2x" data-sizes=auto alt=Summary%20127e4020f060808da8e0f23caf646e39/image%2029.png title=image.png></p><p>基础矩阵是一个 3×3 的矩阵，定义了在两个视图中对应点之间的约束关系。给定两个视图中的一个对应点对 (x,x′)，它们满足以下关系：</p><pre><code>                                                   x′TFx=0
</code></pre><p>其中：</p><ul><li><p>x和 x′ 分别是图像1和图像2中的对应点的齐次坐标。</p></li><li><p>F是基础矩阵，描述两个图像之间的对应关系，具有 7 个自由度</p><pre><code>                        （*detF*=0的约束、尺度等价性各减去一个自由度）。
</code></pre></li></ul><p>基础矩阵的作用是将图像1中的点通过约束关系映射到图像2中相应的极线，建立点和极线的关系。由于基础矩阵不依赖于相机内参，因此它可以应用于任意两个视图，<strong>适用于未标定的摄像机</strong>。<strong>8点法求解</strong></p><h3 id=本质矩阵e>本质矩阵E</h3><p>不包含内参</p><p><strong>本质矩阵 E = t∧ R</strong>。它是一个 3 × 3 的矩阵，内有 9 个未知数</p><p>平移向量 t 对应的反对称矩阵 <strong>t∧</strong> 与旋转矩阵 R 的乘积</p><p>5个自由度（平移和旋转各有 3 个自由度，尺度等价性减去1个自由度）</p><p><strong>8点法求解</strong>（最少5点）</p><p><img class=lazyload src=/svg/loading.min.svg data-src=Summary%20127e4020f060808da8e0f23caf646e39/image%2030.png data-srcset="Summary%20127e4020f060808da8e0f23caf646e39/image%2030.png, Summary%20127e4020f060808da8e0f23caf646e39/image%2030.png 1.5x, Summary%20127e4020f060808da8e0f23caf646e39/image%2030.png 2x" data-sizes=auto alt=Summary%20127e4020f060808da8e0f23caf646e39/image%2030.png title=image.png></p><p>系数矩阵是根据对 e 的多项式进行拆分得到的</p><p>再对E进行SVD分解恢复R,t</p><h3 id=单应矩阵h>单应矩阵H</h3><p><img class=lazyload src=/svg/loading.min.svg data-src=Summary%20127e4020f060808da8e0f23caf646e39/image%2031.png data-srcset="Summary%20127e4020f060808da8e0f23caf646e39/image%2031.png, Summary%20127e4020f060808da8e0f23caf646e39/image%2031.png 1.5x, Summary%20127e4020f060808da8e0f23caf646e39/image%2031.png 2x" data-sizes=auto alt=Summary%20127e4020f060808da8e0f23caf646e39/image%2031.png title=image.png></p><p>p2 ≃ Hp1 . nT是平面P的法向量，d是平面常数项参数</p><p>纯旋转或特征点在同一平面</p><p><img class=lazyload src=/svg/loading.min.svg data-src=Summary%20127e4020f060808da8e0f23caf646e39/image%2032.png data-srcset="Summary%20127e4020f060808da8e0f23caf646e39/image%2032.png, Summary%20127e4020f060808da8e0f23caf646e39/image%2032.png 1.5x, Summary%20127e4020f060808da8e0f23caf646e39/image%2032.png 2x" data-sizes=auto alt=Summary%20127e4020f060808da8e0f23caf646e39/image%2032.png title=image.png></p><p>自由度为 8 的单应矩阵可以通过 <strong>4 对匹配特征点</strong>算出 DLT求解</p><p>可分解出R、t 数值法或解析法</p><h3 id=自由度>自由度</h3><ul><li>单应矩阵有 8 个自由度。尽管它包含 9 个元素 hij，但由于其定义是 up-to-scale 的（即在同一个变换下，矩阵的所有元素乘以一个非零常数仍然描述相同的单应性变换），所以只需要 8 个独立参数来确定这个矩阵。</li><li>为了固定尺度，<strong>一般将 h33 归一化为 1 或将矩阵整体除以某一个元素，以便矩阵变换唯一确定</strong>。</li></ul><h2 id=3d-2d-pnp>3d-2d PnP</h2><p>Perspective-n-Point</p><p>通过一组已知的三维点及其在二维图像中的投影点来确定相机姿态（位姿，即旋转和平移）的算法。</p><p><a href=https://www.zywvvd.com/notes/study/camera-imaging/pnp/pnp/ target=_blank rel="noopener noreffer">https://www.zywvvd.com/notes/study/camera-imaging/pnp/pnp/</a></p><h3 id=pnp问题的定义>PnP问题的定义</h3><p>给定：</p><ol><li><strong>n个三维点的坐标</strong>：在物体坐标系中，这些点的三维坐标已知，表示为(Xi​,Yi​,Zi​)。</li><li><strong>n个二维图像点</strong>：这些三维点在图像平面上的投影位置已知，表示为(xi​,yi​)。</li><li><strong>相机内参矩阵</strong>：<strong>已知相机的内参矩阵K</strong>，包括焦距和主点位置等参数。内参未知时可尝试UPnP或同时估计内外参的方法，但这些方法通常比已知内参的PnP问题更复杂，并且可能对噪声和初始化更敏感。</li></ol><p><strong>目标是求解相机的外参</strong>，即相机相对于物体的旋转矩阵 R 和平移向量 t，将三维点投影到图像上。</p><h3 id=pnp问题的求解步骤>PnP问题的求解步骤</h3><ol><li><p><strong>模型建立</strong>：在相机坐标系中，根据相机内参矩阵和外参矩阵，得到3D点到2D点的投影关系公式。</p><p><img class=lazyload src=/svg/loading.min.svg data-src=Summary%20127e4020f060808da8e0f23caf646e39/image%2033.png data-srcset="Summary%20127e4020f060808da8e0f23caf646e39/image%2033.png, Summary%20127e4020f060808da8e0f23caf646e39/image%2033.png 1.5x, Summary%20127e4020f060808da8e0f23caf646e39/image%2033.png 2x" data-sizes=auto alt=Summary%20127e4020f060808da8e0f23caf646e39/image%2033.png title=image.png></p></li></ol><ul><li>其中 K为相机内参矩阵，[R∣t][R | t][R∣t] 为外参矩阵。</li><li><strong>方程求解</strong>：根据不同方法（如EPnP、RANSAC+PnP等）求解出R 和 t。</li><li><strong>优化与精确化</strong>：通常会进行非线性优化（如Levenberg-Marquardt），以最小化重投影误差，进一步精确外参矩阵。</li><li><strong>验证与应用</strong>：将解出的姿态应用到实际场景中，可以得到相机的位置和方向。</li></ul><h3 id=dltdirect-linear-transformation><strong>DLT（Direct Linear Transformation）</strong></h3><p>空间点P，齐次坐标为 P = (X, Y, Z, 1)T，投影到特征点 x1 =(u1 , v1 , 1)T （归一化平面，与图像点只差一个内参，因此可等效替代）</p><p>定义增广矩阵 [R|t] 为一个 3 × 4 的矩阵，包含了旋转与平移信息</p><p><img class=lazyload src=/svg/loading.min.svg data-src=Summary%20127e4020f060808da8e0f23caf646e39/image%2034.png data-srcset="Summary%20127e4020f060808da8e0f23caf646e39/image%2034.png, Summary%20127e4020f060808da8e0f23caf646e39/image%2034.png 1.5x, Summary%20127e4020f060808da8e0f23caf646e39/image%2034.png 2x" data-sizes=auto alt=Summary%20127e4020f060808da8e0f23caf646e39/image%2034.png title=image.png></p><p>12个未知数，一对匹配点提供2个关于t的线性约束</p><ul><li>在<strong>点数多于6个的情况下</strong>，通过线性方法直接求解PnP问题。DLT方法虽然简单，但易受噪声影响，因此通常用于粗略估计。</li><li>优势：实现简单，但对噪声敏感。</li></ul><h3 id=p3p>P3P</h3><p><a href=https://www.cnblogs.com/mafuqiang/p/8302663.html target=_blank rel="noopener noreffer">https://www.cnblogs.com/mafuqiang/p/8302663.html</a></p><p><img class=lazyload src=/svg/loading.min.svg data-src=Summary%20127e4020f060808da8e0f23caf646e39/image%2035.png data-srcset="Summary%20127e4020f060808da8e0f23caf646e39/image%2035.png, Summary%20127e4020f060808da8e0f23caf646e39/image%2035.png 1.5x, Summary%20127e4020f060808da8e0f23caf646e39/image%2035.png 2x" data-sizes=auto alt=Summary%20127e4020f060808da8e0f23caf646e39/image%2035.png title=image.png></p><p>将P3P问题<strong>转换为ICP问题求解</strong>，核心是求解3个2d点在当前相机坐标系下的3d坐标</p><p>需要求解二元二次方程，最少需要一个点，还需要<strong>额外的第4个点验证</strong></p><p>方程的建立是依据余弦定理</p><p><img class=lazyload src=/svg/loading.min.svg data-src=Summary%20127e4020f060808da8e0f23caf646e39/image%2036.png data-srcset="Summary%20127e4020f060808da8e0f23caf646e39/image%2036.png, Summary%20127e4020f060808da8e0f23caf646e39/image%2036.png 1.5x, Summary%20127e4020f060808da8e0f23caf646e39/image%2036.png 2x" data-sizes=auto alt=Summary%20127e4020f060808da8e0f23caf646e39/image%2036.png title=image.png></p><h3 id=bundle-adjustment>Bundle Adjustment</h3><p>定义于李代数上的非线性最小二乘问题</p><p><strong>最小化重投影误差</strong></p><p>没有回环也可以做局部BA</p><h3 id=光束法平差的基本原理>光束法平差的基本原理</h3><ol><li><p><strong>已知条件</strong>：</p><ul><li>多个视角下的图像以及对应的特征点。</li><li>每个视角的相机内参（如焦距、主点位置、畸变系数）和外参（位置和朝向）。</li></ul></li><li><p><strong>目标</strong>：</p><ul><li><strong>优化相机的内外参数和三维点的位置</strong>，使得通过相机投影回到图像的三维点与实际检测到的二维特征点之间的重投影误差最小。</li></ul></li><li><p><strong>重投影误差</strong>：</p><ul><li>重投影误差是指将三维点通过相机模型投影到图像平面后，<strong>计算得到的二维点与实际检测到的二维特征点之间的差距</strong>。这个误差通常用平方和来表示：</li></ul><p><img class=lazyload src=/svg/loading.min.svg data-src=Summary%20127e4020f060808da8e0f23caf646e39/image%2037.png data-srcset="Summary%20127e4020f060808da8e0f23caf646e39/image%2037.png, Summary%20127e4020f060808da8e0f23caf646e39/image%2037.png 1.5x, Summary%20127e4020f060808da8e0f23caf646e39/image%2037.png 2x" data-sizes=auto alt=Summary%20127e4020f060808da8e0f23caf646e39/image%2037.png title=image.png></p><p>其中，pij是第j个相机观察到的第i个三维点的实际图像位置，π是投影函数，Rj和tj是第j个相机的旋转和位移。</p></li></ol><h2 id=3d-3d-icp>3d-3d ICP</h2><p><a href=https://zhuanlan.zhihu.com/p/96908474 target=_blank rel="noopener noreffer">zhuanlan.zhihu.com</a></p><p><strong>两种常见的点云配准方法ICP&amp;NDT</strong></p><p>点云配准算法 求解R,t</p><p>icp、ndt</p><p>没有一个明确的最少点数要求，但为了确保ICP算法的鲁棒性和准确性，通常推荐使用尽可能多的点，同时保证点云的均匀分布和质量。</p><p>不涉及二维到三维，因此不需要内参</p><h2 id=三角测量空间前方交会-已知观察未知>三角测量（空间前方交会 已知观察未知）</h2><p><a href=https://baike.baidu.com/item/%E7%A9%BA%E9%97%B4%E5%89%8D%E6%96%B9%E4%BA%A4%E4%BC%9A target=_blank rel="noopener noreffer">https://baike.baidu.com/item/%E7%A9%BA%E9%97%B4%E5%89%8D%E6%96%B9%E4%BA%A4%E4%BC%9A</a></p><p>由<a href="https://baike.baidu.com/item/%E7%AB%8B%E4%BD%93%E5%83%8F%E5%AF%B9/0?fromModule=lemma_inlink" target=_blank rel="noopener noreffer">立体像对</a>左右两影像的内、<a href="https://baike.baidu.com/item/%E5%A4%96%E6%96%B9%E4%BD%8D%E5%85%83%E7%B4%A0/0?fromModule=lemma_inlink" target=_blank rel="noopener noreffer">外方位元素</a>和<a href="https://baike.baidu.com/item/%E5%90%8C%E5%90%8D%E5%83%8F%E7%82%B9/0?fromModule=lemma_inlink" target=_blank rel="noopener noreffer">同名像点</a>的影像坐标测量值来确定该点的物方空间坐标。</p><p><a href=https://gutsgwh1997.github.io/2020/03/31/%E5%A4%9A%E8%A7%86%E5%9B%BE%E4%B8%89%E8%A7%92%E5%8C%96/ target=_blank rel="noopener noreffer">https://gutsgwh1997.github.io/2020/03/31/%E5%A4%9A%E8%A7%86%E5%9B%BE%E4%B8%89%E8%A7%92%E5%8C%96/</a></p><p><img class=lazyload src=/svg/loading.min.svg data-src=Summary%20127e4020f060808da8e0f23caf646e39/image%2038.png data-srcset="Summary%20127e4020f060808da8e0f23caf646e39/image%2038.png, Summary%20127e4020f060808da8e0f23caf646e39/image%2038.png 1.5x, Summary%20127e4020f060808da8e0f23caf646e39/image%2038.png 2x" data-sizes=auto alt=Summary%20127e4020f060808da8e0f23caf646e39/image%2038.png title=image.png></p><p>已知夹角和基线（R和t）、内参矩阵以及特征点像素坐标p1、p2，求解深度s1、s2</p><p>s1 x1 = s2 Rx2 + t.</p><h2 id=orb-slam2>orb-slam2</h2><p>单目、双目、RGBD</p><p><img class=lazyload src=/svg/loading.min.svg data-src=Summary%20127e4020f060808da8e0f23caf646e39/image%2039.png data-srcset="Summary%20127e4020f060808da8e0f23caf646e39/image%2039.png, Summary%20127e4020f060808da8e0f23caf646e39/image%2039.png 1.5x, Summary%20127e4020f060808da8e0f23caf646e39/image%2039.png 2x" data-sizes=auto alt=Summary%20127e4020f060808da8e0f23caf646e39/image%2039.png title=image.png></p><p>主线程 创建了<a href=https://github.com/gaoyichao/ORB_SLAM2/blob/master/include/System.h target=_blank rel="noopener noreffer">System</a>类型的对象，核心控制器，负责调度</p><p>四个并行子线程</p><p>轨迹追踪</p><p>局部建图</p><p>回环检测 全部完成后会开启全局BA线程</p><p>可视化稀疏地图</p><h1 id=三维重建>三维重建</h1><h2 id=sfm算法原理>sfm算法原理</h2><p><a href=https://wjiajie.github.io/contents/slam-sfm/sfm-intro/ target=_blank rel="noopener noreffer">https://wjiajie.github.io/contents/slam-sfm/sfm-intro/</a></p><p>从无序图片中进行三维重建的离线算法</p><p><strong>从运动中恢复物体的三维结构，估计出图片的R,t，结合相机内参重建稀疏点云</strong></p><p><img class=lazyload src=/svg/loading.min.svg data-src=Summary%20127e4020f060808da8e0f23caf646e39/image%2040.png data-srcset="Summary%20127e4020f060808da8e0f23caf646e39/image%2040.png, Summary%20127e4020f060808da8e0f23caf646e39/image%2040.png 1.5x, Summary%20127e4020f060808da8e0f23caf646e39/image%2040.png 2x" data-sizes=auto alt=Summary%20127e4020f060808da8e0f23caf646e39/image%2040.png title=image.png></p><p>重建稀疏点云–重建稠密点云–<strong>点云融合</strong>-生成网格–添加纹理</p><p>SFM负责重建稀疏点云这一部分，从多张视图中估计出照片的旋转平移矩阵R,t，结合相机内参恢复物体稀疏点云结构。</p><p>获得两张图片中的对应点，然后估计基础矩阵F，再估计本征矩阵E，再通过SVD分解求得较好的R,t，得到物体的三维点，最后将多个稀疏点云融合</p><p><img class=lazyload src=/svg/loading.min.svg data-src=Summary%20127e4020f060808da8e0f23caf646e39/image%2041.png data-srcset="Summary%20127e4020f060808da8e0f23caf646e39/image%2041.png, Summary%20127e4020f060808da8e0f23caf646e39/image%2041.png 1.5x, Summary%20127e4020f060808da8e0f23caf646e39/image%2041.png 2x" data-sizes=auto alt=Summary%20127e4020f060808da8e0f23caf646e39/image%2041.png title=image.png></p><p>内参未知的情况下，F可通过SVD分解<strong>重构本质矩阵</strong></p><h3 id=sfm-流程总结><strong>SfM 流程总结：</strong></h3><p>1、关键点检测（Keypoint Detection）：</p><p>从多张图像中提取关键点（如图中的雕塑）。这些关键点通常是图像中具有显著特征的点，例如角点或纹理丰富的区域。关键点的提取有助于在不同视图中进行匹配。</p><p>2、关键点匹配（Matching Keypoints）：</p><p>在不同的图像之间对关键点进行匹配，找到在不同视图中对应的点对。这些匹配的点对为后续的几何推导奠定基础。通过匹配，可以确定哪些点在多幅图像中是相同的三维点的投影。</p><p>3、基础矩阵计算（Fundamental Matrix Calculation）：</p><p>使用匹配到的关键点对，计算基础矩阵 F 。基础矩阵 F 描述了两个图像之间的几何约束关系，而不依赖于相机的内参。这一矩阵可以将一个图像中的点映射到另一个图像的极线上，定义了视图间的点对几何约束。</p><p>4、本质矩阵计算（Essential Matrix Calculation）：</p><p>在相机的内参已知的情况下，将基础矩阵 F 转换为本质矩阵 E 。本质矩阵包含了两台相机之间的相对旋转和平移信息。它不仅描述了点在两幅图像之间的几何关系，还体现了相机的相对姿态。</p><p>5、相机位姿恢复（[R|t] Decomposition）：</p><p>从本质矩阵 E 中分解出相机的相对位姿，即旋转矩阵 R 和平移向量 t 。这些参数描述了相机在三维空间中的相对位置和方向，从而使我们能够将两个图像的坐标系关联起来。</p><p>6、三角测量（Triangulation）：</p><p>利用多视角的点对和已估计的相机位姿，通过三角测量恢复三维空间中关键点的实际位置。这一步<strong>通过几何关系计算出三维点的空间坐标，从而生成场景的稀疏三维结构</strong>。</p><p>7、构建三维结构（3D Structure Construction）：</p><p>最终，经过上述步骤，得到的三维点可以组合成稀疏的三维点云模型，描述了场景的基本结构。这一稀疏模型可进一步用于密集重建、光束平差优化等后续步骤。</p><h2 id=colmap>colmap</h2><p>特征提取</p><p>特征匹配</p><p>SFM稀疏点云生成</p><p>去畸变</p><p>稠密点云生成</p><p>稠密点云融合 全局BA（非线性优化）</p><h1 id=openmvs>openMVS</h1><p>ply格式转为mvs</p><p>生成网格</p><p>添加纹理</p><p>结果可视化</p><h2 id=nerf>NeRF</h2><p>辐射神经场</p><p>直接输出是视频</p><p>本质上是一个生成逼真渲染的技术，隐式表达</p><h2 id=sfm与slam关系>sfm与slam关系</h2><ul><li><strong>共同点</strong>：<ul><li>两者都涉及到相机的运动和三维结构的重建，通常需要提取特征点并进行匹配。</li><li>都依赖于几何关系和优化技术。</li></ul></li><li><strong>不同点</strong>：<ul><li><strong>实时性</strong>：SLAM强调实时性，适用于动态环境；SFM通常是离线处理。</li><li><strong>目标</strong>：SFM主要关注重建场景的三维模型，而SLAM则同时关注定位和地图构建。</li><li><strong>数据来源</strong>：SLAM可以利用多种传感器数据（如激光、IMU等），而SFM主要基于视觉图像。</li></ul></li></ul><h2 id=ply数据格式>ply数据格式</h2><p><strong>Stanford Triangle Format</strong></p><p><a href=https://huangwang.github.io/2019/06/04/PLY%E6%96%87%E4%BB%B6%E6%A0%BC%E5%BC%8F%E5%88%86%E6%9E%90/ target=_blank rel="noopener noreffer">https://huangwang.github.io/2019/06/04/PLY%E6%96%87%E4%BB%B6%E6%A0%BC%E5%BC%8F%E5%88%86%E6%9E%90/</a></p><p>该格式主要用以储存立体扫描结果的三维数值，透过多边形片面的集合描述三维物体，与其他格式相较之下这是较为简单的方法。它可以储存的资讯包含颜色、透明度、表面法向量、材质座标与资料可信度，并能对多边形的正反两面设定不同的属性。</p><p>element vertex 100
property float x
property float y
property float z
property float nx
property float ny
property float nz
&mldr;</p><h1 id=传统图像算法>传统图像算法</h1><h2 id=特征提取><strong>特征提取</strong></h2><ul><li><p><strong>Harris角点检测</strong>：检测图像中的角点，用于特征匹配和识别。</p><p><strong>角点是向各个方向移动窗口，灰度值都发生较大变化的点</strong></p><p><a href=https://dezeming.top/wp-content/uploads/2021/05/Harris%e8%a7%92%e7%82%b9%e6%a3%80%e6%b5%8b%e5%8e%9f%e7%90%86%e4%b8%8eOpenCV%e6%ba%90%e7%a0%81%e8%a7%a3%e8%af%bb-1.pdf target=_blank rel="noopener noreffer"></a></p><p><img class=lazyload src=/svg/loading.min.svg data-src=Summary%20127e4020f060808da8e0f23caf646e39/image%2042.png data-srcset="Summary%20127e4020f060808da8e0f23caf646e39/image%2042.png, Summary%20127e4020f060808da8e0f23caf646e39/image%2042.png 1.5x, Summary%20127e4020f060808da8e0f23caf646e39/image%2042.png 2x" data-sizes=auto alt=Summary%20127e4020f060808da8e0f23caf646e39/image%2042.png title=image.png></p><p><img class=lazyload src=/svg/loading.min.svg data-src=Summary%20127e4020f060808da8e0f23caf646e39/image%2043.png data-srcset="Summary%20127e4020f060808da8e0f23caf646e39/image%2043.png, Summary%20127e4020f060808da8e0f23caf646e39/image%2043.png 1.5x, Summary%20127e4020f060808da8e0f23caf646e39/image%2043.png 2x" data-sizes=auto alt=Summary%20127e4020f060808da8e0f23caf646e39/image%2043.png title=image.png></p><p><strong>主要是根据矩阵分析的方法，构造了角点响应函数，以此作为判定依据</strong></p></li><li><p><strong>SIFT（尺度不变特征变换）</strong>：提取局部特征点，在尺度和旋转上具有不变性，常用于图像匹配。</p></li></ul><p><a href=https://dezeming.top/wp-content/uploads/2021/07/Sift%e7%ae%97%e6%b3%95%e5%8e%9f%e7%90%86%e4%b8%8eOpenCV%e6%ba%90%e7%a0%81%e8%a7%a3%e8%af%bb.pdf target=_blank rel="noopener noreffer"></a></p><p><a href=https://blog.csdn.net/lhanchao/article/details/52345845 target=_blank rel="noopener noreffer">https://blog.csdn.net/lhanchao/article/details/52345845</a></p><p><strong>第一步是特征点检测，第二步是生成特征描述</strong></p><ol><li>尺度空间上极值点检测</li><li>关键点精确定位</li><li>关键点方向分配（根据36个bins的峰值确定主导方向）</li><li>生成特征描述子（周围分为4 × 4个区域，各计算8个方向的梯度，128维特征描述子）</li></ol><ul><li>特征点的 36 个方向是用来确定每个特征点的主方向，以便于在旋转和缩放变化中保持一致性。</li><li>描述子的 8 个方向则是在特征点的主方向基础上，通过对局部区域的梯度信息进行统计生成的，确保描述子能够有效捕捉周围的局部特征。</li><li>这两者的结合使得 SIFT 能够在各种图像变换下保持良好的特征匹配性能。</li></ul><h3 id=主方向与描述子的关系>主方向与描述子的关系</h3><ul><li><strong>旋转不变性</strong>：特征点的主方向用于对描述子进行旋转对齐。这样可以确保即使图像发生旋转，描述子仍然可以保持一致性。<ul><li>具体来说，<strong>生成描述子时，首先将局部区域的梯度信息相对于特征点的主方向进行旋转</strong>。这种方法使得描述子在不同视角下对同一物体的表示是一致的。</li></ul></li><li><strong>特征匹配</strong>：由于描述子是相对于主方向计算的，因此在特征匹配时，可以更容易地找到相似的特征点。例如，在两幅图像中找到具有相同描述子的特征点时，它们的主方向一致性提供了额外的匹配准确性。</li></ul><p>首先构造高斯金字塔尺度空间，对图像进行高斯模糊不断降采样</p><p>具体分为4步：</p><p>•第一阶段的计算搜索所有的尺度上的所有图像区域。该算法利用高斯函数的微分来识别对
尺度和方向不变性的潜在兴趣点。
• 第二阶段是关键点定位，在每个候选位置，一个精细的模型用于确定位置和尺度。关键点是根据它们的稳定性来选择的。</p><p><img class=lazyload src=/svg/loading.min.svg data-src=Summary%20127e4020f060808da8e0f23caf646e39/image%2044.png data-srcset="Summary%20127e4020f060808da8e0f23caf646e39/image%2044.png, Summary%20127e4020f060808da8e0f23caf646e39/image%2044.png 1.5x, Summary%20127e4020f060808da8e0f23caf646e39/image%2044.png 2x" data-sizes=auto alt=Summary%20127e4020f060808da8e0f23caf646e39/image%2044.png title=image.png></p><p>离散空间的极值点并不一定是真的极值点，通过函数拟合精确定位
• 第三阶段是方向分配，基于局部图像梯度方向为每个关键点位置分配一个或多个方向。所有的后续操作都是对已相对于每个特征的指定方向、比例和位置进行变换的图像执行的，从而为这些变换提供不变性。
• 最后生成关键点描述子（descriptor，又叫描述符），在每个关键点周围区域的选定比例上测量局部图像梯度。这些被转换成一种表示方法，允许较大的局部形状失真和照明变化。</p><p>对描述子使用关键点所在尺度空间内4 × 4 个子区域中计算8 个方向的
梯度信息，也就是总共4 × 4 × 8 = 128 维向量</p><ul><li><strong>SURF（加速稳健特征）</strong>：比SIFT更快，性能也较好，用于特征点提取和描述。</li></ul><p>ORB</p><h2 id=边缘检测>边缘检测</h2><p>canny</p><p><a href=https://zj-image-processing.readthedocs.io/zh-cn/latest/opencv/code/%5BCanny%5D%E8%BE%B9%E7%BC%98%E6%A3%80%E6%B5%8B/#_4 target=_blank rel="noopener noreffer">https://zj-image-processing.readthedocs.io/zh-cn/latest/opencv/code/%5BCanny%5D%E8%BE%B9%E7%BC%98%E6%A3%80%E6%B5%8B/#_4</a></p><p><code>Canny</code>算子是常用的边缘检测算法，其执行步骤如下：</p><ol><li>应用高斯滤波器平滑图像以去除噪声</li><li>计算图像的强度梯度</li><li>应用非最大抑制（<code>Non-maximum suppression</code>）消除边缘检测的虚假响应</li></ol><p>（不同于目标检测NMS）</p><ol><li>应用<strong>双阈值</strong>确定潜在边缘</li></ol><p>根据梯度强度将像素分为三类：</p><ul><li><strong>强边缘</strong>：梯度强度大于高阈值的像素，被直接标记为边缘。</li><li><strong>弱边缘</strong>：梯度强度在低阈值和高阈值之间的像素，可能是边缘，但需要进一步判断。</li><li><strong>非边缘</strong>：梯度强度小于低阈值的像素，被抑制为非边缘。</li></ul><ol><li>通过滞后（<code>hysteresis</code>）方法跟踪边缘：通过抑制所有其他弱边缘和未连接到强边缘的边缘，完成边缘检测</li></ol><h2 id=图像拼接>图像拼接</h2><h2 id=特征点法最常用>特征点法（最常用）</h2><ul><li><p><strong>特征提取</strong>：</p><ul><li>使用特征检测算法（如 SIFT、SURF、ORB 等）从每张图像中提取关键点，并计算对应的特征描述子。</li></ul></li><li><p><strong>特征匹配</strong>：</p><ul><li>使用最近邻匹配或基于距离比率的匹配（如 KNN 匹配）来找到不同图像中的相似特征点对。FLANN快速最近邻</li></ul></li><li><p><strong>求解单应矩阵（Homography Matrix）</strong>：</p><ul><li>使用 <strong>RANSAC 等鲁棒算法</strong>估计两张图像间的单应矩阵，以剔除错误匹配点，并获取精确的几何变换关系。</li></ul></li><li><p><strong>图像变换与对齐</strong>：</p><ul><li>使用单应矩阵将图像进行几何变换（如透视变换或仿射变换），实现图像的对齐。</li></ul><h3 id=总结对比>总结对比</h3><table><thead><tr><th>特性</th><th>仿射变换</th><th>单应变换</th></tr></thead><tbody><tr><td>矩阵大小</td><td>3×2</td><td>3×3</td></tr><tr><td>自由度</td><td>6 个参数</td><td>8 个参数（一个比例因子）</td></tr><tr><td>支持变换类型</td><td>旋转、平移、缩放、剪切</td><td>透视变换（包含仿射变换）</td></tr><tr><td>适用场景</td><td>小视角变换、图像小幅旋转、平移、缩放</td><td>大视角变换、透视失真、图像拼接</td></tr><tr><td>准确性</td><td>适合简单几何变形</td><td>适合复杂几何变形</td></tr><tr><td>适用平面</td><td>适合在同一平面内的平移或旋转</td><td>适用于不同平面视角拍摄的透视场景</td></tr></tbody></table></li><li><p><strong>图像融合</strong>：</p><ul><li>采用多种融合算法（如渐变融合、拉普拉斯融合）对重叠区域进行平滑过渡，减少拼接痕迹。</li></ul></li></ul><h2 id=光流法>光流法</h2><p>lk光流 稀疏光流</p><p><strong>基于灰度不变假设，适用于特征缺失的情况</strong></p><p>光流法是一种基于图像运动估计的拼接方法，主要应用于相邻帧的运动估计。</p><p>适合于处理连续帧图像间的拼接，尤其在视频帧拼接、微小相对运动的场景中效果较好。</p><p>光流法基于假设：图像中相邻帧之间的亮度在运动过程中是保持恒定的，即光流约束方程。它通过对相邻帧图像像素点进行运动估计，从而计算出每个像素的位移向量，进而实现图像拼接。</p><p>光流约束方程</p><h2 id=图像配准>图像配准</h2><p>基于特征点</p><p>光流法</p><p>模板匹配 通过计算两幅图像之间的互相关系数，找到最佳的平移量进行对齐</p><h2 id=去噪增强>去噪、增强</h2><p><strong>滤波器(平滑、积分运算)</strong>：</p><ul><li><strong>均值滤波</strong>：通过计算邻域像素的均值来平滑图像，减少噪声。</li><li><strong>高斯滤波</strong>：使用高斯函数作为卷积核进行平滑，能更好地保留边缘信息。</li><li><strong>中值滤波</strong>：用邻域像素的中值替代中心像素，有效去除椒盐噪声。</li></ul><p>锐化、微分运算</p><p><strong>直方图均衡化</strong>：</p><p>通过调整图像的直方图，使得图像的对比度得到增强，尤其适用于低对比度图像。</p><p>变为灰度分布均匀的新图像</p><h2 id=形态学处理>形态学处理</h2><ul><li><strong>膨胀和腐蚀</strong>：用于图像的形态学处理，膨胀增加物体的边界，腐蚀则减小物体的边界。</li><li><strong>开运算和闭运算</strong>：开运算用于去除小的物体，闭运算用于填补小的空洞。</li></ul><h2 id=hdr><strong>HDR</strong></h2><p><a href=https://linux.cn/article-9754-1.html target=_blank rel="noopener noreffer">https://linux.cn/article-9754-1.html</a></p><p><a href=https://www.orzzz.cn/index.php/archives/44/ target=_blank rel="noopener noreffer">https://www.orzzz.cn/index.php/archives/44/</a></p><p>CRF估计：与计算机视觉中的许多问题一样，找到 CRF 的问题本质是一个最优解问题，其目标是使由数据项和平滑项组成的目标函数最小化。 这些问题通常会降维到线性最小二乘问题，这些问题可以使用奇异值分解Singular Value Decomposition（SVD）来解决，奇异值分解是所有线性代数包的一部分。<strong>Debevec算法</strong></p><p>hdr合成：针对某一个位置的像素点，将不同曝光图片的这个位置的辐照度<strong>进行加权平均</strong>，求出最终 <code>hdr</code> 图片的这个位置的像素值。<strong>多种策略，中间亮度权重更高</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>cv2</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>np</span><span class=o>.</span><span class=n>set_printoptions</span><span class=p>(</span><span class=n>suppress</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 1、读图</span>
</span></span><span class=line><span class=cl><span class=n>img_fn</span> <span class=o>=</span> <span class=p>[</span><span class=s2>&#34;img_0.033.jpg&#34;</span><span class=p>,</span> <span class=s2>&#34;img_0.25.jpg&#34;</span><span class=p>,</span> <span class=s2>&#34;img_2.5.jpg&#34;</span><span class=p>,</span> <span class=s2>&#34;img_15.jpg&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>img_list</span> <span class=o>=</span> <span class=p>[</span><span class=n>cv2</span><span class=o>.</span><span class=n>imread</span><span class=p>(</span><span class=n>fn</span><span class=p>)</span> <span class=k>for</span> <span class=n>fn</span> <span class=ow>in</span> <span class=n>img_fn</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>exposure_times</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([</span><span class=mf>0.033</span><span class=p>,</span> <span class=mf>0.25</span><span class=p>,</span> <span class=mf>2.5</span><span class=p>,</span> <span class=mf>15.0</span><span class=p>],</span> <span class=n>dtype</span><span class=o>=</span><span class=n>np</span><span class=o>.</span><span class=n>float32</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 2、对齐</span>
</span></span><span class=line><span class=cl><span class=n>alignMTB</span> <span class=o>=</span> <span class=n>cv2</span><span class=o>.</span><span class=n>createAlignMTB</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>alignMTB</span><span class=o>.</span><span class=n>process</span><span class=p>(</span><span class=n>img_list</span><span class=p>,</span> <span class=n>img_list</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 3、计算响应曲线crf</span>
</span></span><span class=line><span class=cl><span class=n>calibrateDebevec</span> <span class=o>=</span> <span class=n>cv2</span><span class=o>.</span><span class=n>createCalibrateDebevec</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>responseDebevec</span> <span class=o>=</span> <span class=n>calibrateDebevec</span><span class=o>.</span><span class=n>process</span><span class=p>(</span><span class=n>img_list</span><span class=p>,</span> <span class=n>exposure_times</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=nb>type</span><span class=p>(</span><span class=n>responseDebevec</span><span class=p>),</span><span class=n>responseDebevec</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>responseDebevec</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 4、hdr融合</span>
</span></span><span class=line><span class=cl><span class=n>mergeDebevec</span> <span class=o>=</span> <span class=n>cv2</span><span class=o>.</span><span class=n>createMergeDebevec</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>hdrDebevec</span> <span class=o>=</span> <span class=n>mergeDebevec</span><span class=o>.</span><span class=n>process</span><span class=p>(</span><span class=n>img_list</span><span class=p>,</span> <span class=n>exposure_times</span><span class=p>,</span> <span class=n>responseDebevec</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>cv2</span><span class=o>.</span><span class=n>imwrite</span><span class=p>(</span><span class=s2>&#34;hdrDebevec.hdr&#34;</span><span class=p>,</span> <span class=n>hdrDebevec</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 5、色调映射</span>
</span></span><span class=line><span class=cl><span class=n>tonemapMantiuk</span> <span class=o>=</span> <span class=n>cv2</span><span class=o>.</span><span class=n>createTonemapMantiuk</span><span class=p>(</span><span class=mf>2.2</span><span class=p>,</span> <span class=mf>0.85</span><span class=p>,</span> <span class=mf>1.2</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>ldrMantiuk</span> <span class=o>=</span> <span class=n>tonemapMantiuk</span><span class=o>.</span><span class=n>process</span><span class=p>(</span><span class=n>hdrDebevec</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>ldrMantiuk</span> <span class=o>=</span> <span class=mi>3</span> <span class=o>*</span> <span class=n>ldrMantiuk</span>
</span></span><span class=line><span class=cl><span class=n>cv2</span><span class=o>.</span><span class=n>imwrite</span><span class=p>(</span><span class=s2>&#34;ldr-Mantiuk.jpg&#34;</span><span class=p>,</span> <span class=n>ldrMantiuk</span> <span class=o>*</span> <span class=mi>255</span><span class=p>)</span>
</span></span></code></pre></div><h2 id=变换技术>变换技术</h2><h3 id=傅里叶变换><strong>傅里叶变换</strong></h3><p>将图像从空间域转换到频率域，常用于图像分析和滤波。</p><p>空间域是指图像本身的像素值分布，它直接描述了图像的视觉内容。</p><p>频率域是指<strong>图像的频率成分分布，它描述了图像中像素值变化的速率，描述结构特征。空间域和频率域提供了两种不同的视角来分析和处理图像。空间域直接反映了图像的视觉内容，而频率域则揭示了图像中的频率成分及其分布，描述的是整体特征，不直接与空间位置对应</strong>。</p><h3 id=空间域与频率域>空间域与频率域</h3><ul><li><strong>空间域</strong>：在空间域中，图像是由像素组成的矩阵，每个像素值表示该位置的灰度或颜色值。空间域描述了图像的直接视觉内容。</li><li><strong>频率域</strong>：在频率域中，<strong>图像被分解为不同的频率成分，表示图像中像素值变化的速率</strong>。<strong>频率域描述了图像的结构特征，如边缘和纹理</strong>。</li></ul><h3 id=低频与高频>低频与高频</h3><ul><li><strong>低频成分</strong>：代表图像中的大面积平滑区域，变化缓慢。低频成分在频谱的中心区域，包含了图像的整体轮廓和主要特征。</li><li><strong>高频成分</strong>：代表图像中的细节和边缘，变化迅速。高频成分在频谱的外围区域，包含了图像的细节和噪声。</li><li><strong>频谱图</strong>：通过傅里叶变换得到的频谱图直观地显示了图像的频率成分，<strong>中心亮表示低频成分强，外围亮表示高频成分强。</strong></li></ul><p>幅度谱和相位谱是图像傅里叶变换的两个重要组成部分。幅度谱描述频率成分的强度，反映图像的对比度和亮度；相位谱描述频率成分的相位，反映图像的结构和细节。</p><p><a href=https://blog.csdn.net/weixin_39504171/article/details/94553357 target=_blank rel="noopener noreffer">图像处理2：二维图像的频谱图理解_图像频谱图的含义-CSDN博客</a></p><p>1、频谱图以图像的中心为圆心,,半径对应频率高低。低频半径小,高频半径大,中心为直流分量,某点的灰度值对应该频率的能量高低。</p><p>2、频谱图用来表示图像的频率分布,频率分布是图像的整体特征,因此频谱图与原图像之间并不存在坐标点上的对应关系。</p><ul><li><strong>小波变换</strong>：用于多分辨率分析，能在不同尺度上表示图像信息。</li><li>PCA 降维(主成分分析)：对原始信息进行<strong>重新线性组合</strong>，指向信息量更大的方向。</li></ul><h2 id=raw图像>RAW图像</h2><p><strong>RAW图像保留了传感器的原始数据，没有经过ISP的进一步处理，因此能够为后续的图像处理提供更灵活的调整空间。</strong></p><p><a href=https://www.cnblogs.com/powerforme/p/16722603.html target=_blank rel="noopener noreffer">https://www.cnblogs.com/powerforme/p/16722603.html</a></p><p>RAW格式文件本质上是一个没有经任何图像处理的源文件，它能原原本本的记录下相机拍摄到的的信息，没有因为图像处理（如锐化、增加色彩对比）和压缩而造成的信息丢失，但需要经过相关的ISP处理才能够呈现出来。</p><h2 id=isp-图像信号处理>ISP 图像信号处理</h2><p>ISP（Image Signal Processing）算法是图像信号处理的关键技术，主要应用在<strong>相机传感器捕获图像后的处理环节中，将原始传感器数据转换为可视图像</strong>。ISP算法通常用于数码相机、智能手机摄像头和监控摄像头等设备中。ISP算法可以包含多个步骤，以实现对色彩、清晰度、动态范围等的优化。</p><h3 id=主要isp算法模块>主要ISP算法模块</h3><ol><li><p><strong>去噪（Denoising）</strong>：</p><ul><li>图像传感器会在低光环境和高增益设置下产生噪声。去噪算法（如中值滤波、均值滤波、双边滤波、非局部均值滤波等）可以减少这些噪声，同时保持边缘细节。</li></ul></li><li><p><strong>黑电平校正（Black Level Correction）</strong>：</p><ul><li>由于传感器的基准噪声，所有像素的起始电平并非零。黑电平校正可以将电平重新归一化，调整黑色像素的基准。</li></ul></li><li><p><strong>白平衡（White Balance）</strong>：</p><ul><li>不同光源会导致色温不同，白平衡校正能让白色区域在不同光源下看起来都接近白色。自动白平衡算法可以调整RGB通道的增益，以实现色彩还原。</li></ul></li><li><p><strong>去伪色（Demosaicing）</strong>：</p><ul><li>图像传感器通常使用拜耳滤波器捕获彩色信息，每个像素只能记录一种颜色。去伪色算法通过插值填充其他颜色的信息，从而生成完整的RGB图像。</li></ul></li><li><p><strong>色彩校正（Color Correction）</strong>：</p><ul><li>传感器的RGB响应与人眼感知的颜色不同。色彩校正矩阵（CCM）可以校准RGB信号，使得图像的色彩更符合真实场景。</li></ul></li><li><p><strong>伽马校正（Gamma Correction）</strong>：</p><ul><li>伽马校正用于调整图像的亮度分布，使图像在视觉上更加自然。此操作根据非线性伽马曲线，将图像的亮暗区域调整到更适合人眼观察的效果。</li></ul></li><li><p><strong>锐化（Sharpening）</strong>：</p><ul><li>锐化处理可以增强图像的边缘细节，增加图像的清晰度。常用的锐化算法有拉普拉斯锐化、Unsharp Masking等。</li></ul></li><li><p><strong>色调映射（Tone Mapping）</strong>：</p><ul><li>主要用于HDR图像处理，将高动态范围图像映射到低动态范围显示设备上，以适应屏幕显示。常用的色调映射算法包括全局色调映射和局部色调映射。</li></ul></li><li><p><strong>伽马校正与亮度增强</strong>：</p><ul><li>伽马校正可以使图像的亮度更符合人眼的感知。同时，通过亮度增强，图像在低光条件下会显得更明亮。</li></ul></li><li><p><strong>畸变校正（Distortion Correction）</strong>：</p><ul><li>广角镜头或其他特殊镜头会导致图像变形。畸变校正算法可以纠正镜头引起的桶形或枕形畸变。</li></ul></li><li><p><strong>图像裁剪和缩放（Cropping and Scaling）</strong>：</p><ul><li>为了适应不同显示分辨率和需求，对图像进行裁剪和缩放是必不可少的操作。双线性插值和双三次插值是常用的缩放算法。</li></ul></li><li><p><strong>降采样（Downsampling）</strong>：</p><ul><li>在某些应用场景中，如视频流传输中需要更低的分辨率。降采样通过减少图像分辨率，来降低带宽需求。</li></ul></li><li><p><strong>去马赛克（Demosaicing）</strong></p><p>由于大多数图像传感器采用拜耳滤波器阵列（Bayer Filter Array），每个像素只能记录红、绿或蓝中的一种颜色，去马赛克算法会通过插值计算还原每个像素的完整RGB信息，以生成彩色图像。</p></li></ol><h2 id=3a算法>3A算法</h2><p>3A算法（Auto Focus、Auto Exposure、Auto White Balance）是相机图像处理中的三项核心自动化技术，用于自动优化图像质量。</p><h3 id=1-自动对焦auto-focusaf>1. 自动对焦（Auto Focus，AF）</h3><p>自动对焦算法用于<strong>将目标对象调至清晰成像的位置</strong>，确保图像具有最佳锐度。常见的自动对焦技术包括：</p><ul><li><strong>相位检测自动对焦（PDAF）</strong>：通过检测图像传感器上成对的光线相位差，快速计算出焦点位置，适用于快速运动物体。</li><li><strong>对比度检测自动对焦</strong>：通过分析图像对比度的变化来确定焦点，但速度较慢。</li><li><strong>激光自动对焦（Laser AF）</strong>：利用激光测距，可以在暗光条件下快速对焦。</li></ul><p>在3A算法中，自动对焦模块通过对场景的动态分析，选择合适的对焦模式，并不断调整以适应场景的变化。</p><h3 id=2-自动曝光auto-exposureae>2. 自动曝光（Auto Exposure，AE）</h3><p>自动曝光算法通过<strong>调节图像传感器的曝光参数</strong>（如快门速度、光圈大小和ISO感光度），确保图像亮度适中，不会过曝或欠曝。AE通常包括以下几种测光模式：</p><ul><li><strong>点测光</strong>：测量特定区域的亮度，适合高对比度的场景。</li><li><strong>中心加权测光</strong>：主要测量画面中心的亮度，但也参考边缘区域，适用于大多数场景。</li><li><strong>矩阵测光</strong>：将画面分为多个区域，综合分析亮度来决定曝光值，是较为智能的一种方式。</li></ul><p>AE算法会分析场景的亮度分布和动态范围，并根据不同场景的需要，动态调整曝光参数。</p><h3 id=3-自动白平衡auto-white-balanceawb>3. 自动白平衡（Auto White Balance，AWB）</h3><p>自动白平衡算法通过调节红、绿、蓝三通道的增益，以还<strong>原场景中的真实色彩，防止不同光源色温带来的色偏</strong>。常见的白平衡算法有：</p><ul><li><strong>灰度世界假设</strong>：假设图像中所有颜色的平均值是中性灰，从而推导出适合的色温。</li><li><strong>动态白平衡检测</strong>：通过实时分析图像中高光、中间调和阴影区域的颜色信息，动态调整白平衡。</li><li><strong>基于场景的白平衡</strong>：在特定场景（如日光、荧光灯）下，直接使用预设的白平衡参数。</li></ul><p>AWB算法会分析场景的光源特性，自动调节三通道的增益值，<strong>使得白色区域尽可能接近真实白色</strong>，从而使图像的色彩更加自然。</p><h1 id=计算机视觉>计算机视觉</h1><h2 id=图像分类>图像分类</h2><p>lenet 第一个成功应用 mnist</p><p>alexnet 精心设计</p><p>vgg 模块化，设计范式</p><p>googlenet 并行化</p><h3 id=resnet-只学习输入与残差的映射>resnet 只学习输入与残差的映射</h3><p><img class=lazyload src=/svg/loading.min.svg data-src=Summary%20127e4020f060808da8e0f23caf646e39/image%2045.png data-srcset="Summary%20127e4020f060808da8e0f23caf646e39/image%2045.png, Summary%20127e4020f060808da8e0f23caf646e39/image%2045.png 1.5x, Summary%20127e4020f060808da8e0f23caf646e39/image%2045.png 2x" data-sizes=auto alt=Summary%20127e4020f060808da8e0f23caf646e39/image%2045.png title=image.png></p><p><img class=lazyload src=/svg/loading.min.svg data-src=Summary%20127e4020f060808da8e0f23caf646e39/image%2046.png data-srcset="Summary%20127e4020f060808da8e0f23caf646e39/image%2046.png, Summary%20127e4020f060808da8e0f23caf646e39/image%2046.png 1.5x, Summary%20127e4020f060808da8e0f23caf646e39/image%2046.png 2x" data-sizes=auto alt=Summary%20127e4020f060808da8e0f23caf646e39/image%2046.png title=image.png></p><h2 id=目标检测><strong>目标检测</strong></h2><p>锚框（anchor box）：以每个像素为中心，生成多个缩放比和宽高比不同的边界框</p><h3 id=yolo>yolo</h3><h3 id=yolo的核心思想>YOLO的核心思想</h3><p>YOLO的基本思想是<strong>将输入图像划分为网格，每个网格负责检测该区域内的目标</strong>。每个网格预测多个边界框及其对应的类别分数，经过一系列优化筛选后，输出最终的检测结果。YOLO避免了传统的区域建议步骤，使得检测速度大幅提升。</p><h3 id=yolo的具体工作流程>YOLO的具体工作流程</h3><ol><li><strong>图像划分和特征提取</strong><ul><li>YOLO将输入图像划分为固定大小的网格（例如 7x7）。</li><li>使用CNN提取图像特征并将其映射到这些网格。</li></ul></li><li><strong>边界框预测</strong><ul><li>每个网格预测多个边界框及每个边界框对应的置信度分数，这些分数表示框中是否存在物体以及边界框定位的精确度。</li><li>每个边界框同时预测目标类别的分数，这个分数是类别概率与置信度的乘积。</li></ul></li><li><strong>非极大值抑制（NMS）</strong><ul><li>YOLO使用NMS去除重叠较多的边界框，仅保留最具有代表性的检测结果。</li><li>这一步可以有效减少冗余框，保持检测精度。</li></ul></li><li><strong>输出目标类别和边界框</strong><ul><li>YOLO直接输出最终的边界框和类别，完成目标检测任务。</li></ul></li></ol><h3 id=yolo的主要版本演变>YOLO的主要版本演变</h3><ol><li><strong>YOLOv1</strong><ul><li>初版YOLO将图像划分为7x7网格，每个网格预测2个边界框和20个类别分数，速度较快，但在小目标检测上表现一般。</li></ul></li><li><strong>YOLOv2（YOLO9000）</strong><ul><li>引入Batch Normalization提高稳定性，使用锚框（Anchor Boxes）预测，能够检测9,000个类别，提高了准确率。</li></ul></li><li><strong>YOLOv3</strong></li></ol><ul><li>YOLOv3使用<strong>多尺度预测</strong>，对不同大小的目标检测更加友好，增加了网络深度和复杂度，<strong>提升了小目标的检测性能</strong>。<strong>NMS</strong></li><li><strong>多尺度特征检测</strong><ul><li>YOLOv3引入了FPN（特征金字塔网络）思想，能够在不同特征尺度上进行检测。这种设计特别适合检测不同大小的目标，尤其是小目标的检测。</li><li>具体来说，YOLOv3在三种尺度上检测目标，分别在较低、较中和较高的特征层中进行预测。</li></ul></li><li><strong>Darknet-53主干网络</strong><ul><li>YOLOv3使用了一种新设计的卷积神经网络Darknet-53作为主干网络。Darknet-53由53层卷积层组成，相比于YOLOv2的Darknet-19更加深层，提高了特征提取能力。</li><li>使用了残差块（Residual Block）结构，避免梯度消失问题，并且提升了特征表示能力。</li></ul></li><li><strong>改进的分类和定位方法</strong><ul><li>YOLOv3采用逻辑二元交叉熵损失（Binary Cross-Entropy Loss）来预测每个类别的概率，使分类损失计算更加高效。</li><li>仍然使用非极大值抑制（NMS）来移除重叠边框。</li></ul></li><li><strong>锚框机制</strong><ul><li>YOLOv3引入了预定义的锚框，与R-CNN系列的Region Proposal方法类似，能更好地适应不同尺寸的目标。</li></ul></li></ul><ol><li><strong>YOLOv4和YOLOv5</strong><ul><li>采用了<strong>更多优化技术</strong>（如CSP、SPP模块），性能进一步提升。YOLOv5在结构设计和推理速度上有显著改进，得到了广泛应用。</li></ul></li><li><strong>YOLOv7</strong><ul><li>使用了先进的模型架构和特征增强模块，提升了精度和速度，在主流的COCO数据集上达到较高的检测精度。</li></ul></li></ol><h3 id=yolov3-vs-yolov5-性能对比>YOLOv3 vs YOLOv5 性能对比</h3><table><thead><tr><th>特性</th><th>YOLOv3</th><th>YOLOv5</th></tr></thead><tbody><tr><td>主干网络</td><td>Darknet-53</td><td>自定义CSPNet（轻量化多版本可选）</td></tr><tr><td>主要框架</td><td>Darknet（C）</td><td>PyTorch（更容易上手和开发）</td></tr><tr><td>多尺度检测</td><td>支持</td><td>支持</td></tr><tr><td>数据增强</td><td>较少</td><td>Mosaic增强、混合卷积、自动学习率调整</td></tr><tr><td>模型复杂度</td><td>较高</td><td>更加轻量化（尤其是YOLOv5s）</td></tr><tr><td>推理速度</td><td>较慢（尤其在资源受限设备上）</td><td>更快</td></tr><tr><td>精度和召回</td><td>在大多数任务中表现略逊于YOLOv5</td><td>精度和召回优于YOLOv3，尤其在小目标检测中</td></tr></tbody></table><h3 id=yolo的优缺点>YOLO的优缺点</h3><ul><li><strong>优点</strong><ul><li><strong>速度快</strong>：YOLO可以在一次前向传播内完成目标检测，速度极快，适合实时检测场景。</li><li><strong>端到端设计</strong>：YOLO将目标检测当作一个单一的回归问题，不需要额外的区域建议步骤。</li></ul></li><li><strong>缺点</strong><ul><li><strong>定位精度有限</strong>：YOLO在边框回归方面的精度较高，但仍可能不如基于区域建议的算法（如Faster R-CNN）。</li><li><strong>小目标检测效果较差</strong>：早期版本的YOLO在小目标上表现不佳，但后续版本（如YOLOv3及以上）对此有所改进。</li></ul></li></ul><h3 id=ssd>ssd</h3><p>单发多框</p><p>所尺度检测</p><h3 id=rcnn系列>rcnn系列</h3><p>rcnn</p><p><img class=lazyload src=/svg/loading.min.svg data-src=Summary%20127e4020f060808da8e0f23caf646e39/image%2047.png data-srcset="Summary%20127e4020f060808da8e0f23caf646e39/image%2047.png, Summary%20127e4020f060808da8e0f23caf646e39/image%2047.png 1.5x, Summary%20127e4020f060808da8e0f23caf646e39/image%2047.png 2x" data-sizes=auto alt=Summary%20127e4020f060808da8e0f23caf646e39/image%2047.png title=image.png></p><p><img class=lazyload src=/svg/loading.min.svg data-src=Summary%20127e4020f060808da8e0f23caf646e39/image%2048.png data-srcset="Summary%20127e4020f060808da8e0f23caf646e39/image%2048.png, Summary%20127e4020f060808da8e0f23caf646e39/image%2048.png 1.5x, Summary%20127e4020f060808da8e0f23caf646e39/image%2048.png 2x" data-sizes=auto alt=Summary%20127e4020f060808da8e0f23caf646e39/image%2048.png title=image.png></p><p>fast-rcnn</p><p>faster-rcnn</p><p><strong>rpn区域提议网络</strong></p><p>roi pooling 区域兴趣池化</p><p>mask-rcnn</p><h3 id=nms>NMS</h3><p>去除重叠区域的低置信度检测框，提高检测精度</p><p>NMS基本算法的具体步骤如下：</p><p>1）依据框的分数（即目标的概率）将所有预测框排序；
2）选择最大分数的检测框M，将其他与M框重叠度大（IoU超过阈值Nt）的框抑制；
3）迭代这一过程直到所有框被检测完成。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># NMS 算法的简化伪代码：</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>nms</span><span class=p>(</span><span class=n>boxes</span><span class=p>,</span> <span class=n>scores</span><span class=p>,</span> <span class=n>iou_threshold</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=c1># boxes 是检测框列表，scores 是对应的置信度分数</span>
</span></span><span class=line><span class=cl>    <span class=n>sorted_indices</span> <span class=o>=</span> <span class=nb>sorted</span><span class=p>(</span><span class=nb>range</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>scores</span><span class=p>)),</span> <span class=n>key</span><span class=o>=</span><span class=k>lambda</span> <span class=n>k</span><span class=p>:</span> <span class=n>scores</span><span class=p>[</span><span class=n>k</span><span class=p>],</span> <span class=n>reverse</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>keep_boxes</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=k>while</span> <span class=n>sorted_indices</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=c1># 选择置信度最高的框</span>
</span></span><span class=line><span class=cl>        <span class=n>current_idx</span> <span class=o>=</span> <span class=n>sorted_indices</span><span class=o>.</span><span class=n>pop</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>keep_boxes</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>boxes</span><span class=p>[</span><span class=n>current_idx</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 计算当前框与其他框的 IoU 并删除重叠度过高的框，更新sorted_indices</span>
</span></span><span class=line><span class=cl>        <span class=n>sorted_indices</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>            <span class=n>idx</span> <span class=k>for</span> <span class=n>idx</span> <span class=ow>in</span> <span class=n>sorted_indices</span>
</span></span><span class=line><span class=cl>            <span class=k>if</span> <span class=n>compute_iou</span><span class=p>(</span><span class=n>boxes</span><span class=p>[</span><span class=n>current_idx</span><span class=p>],</span> <span class=n>boxes</span><span class=p>[</span><span class=n>idx</span><span class=p>])</span> <span class=o>&lt;</span> <span class=n>iou_threshold</span>
</span></span><span class=line><span class=cl>        <span class=p>]</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>keep_boxes</span>
</span></span></code></pre></div><h2 id=图像分割>图像分割</h2><p>FCN 第一个端到端图像分割模型，反卷积</p><p>U-Net 对称式编解码结构，跳层连接+转置卷积</p><p>Segnet 对称式编解码结构，反池化、无跳层连接，计算效率更高</p><p>Deeplab系列 主要是ASPP模块，多个尺度上使用<strong>空洞卷积</strong></p><p>空洞卷积增大了感受野，但是计算量不变</p><h3 id=空洞空间金字塔池化aspp>空洞空间金字塔池化（ASPP）</h3><ul><li><strong>设计</strong>：ASPP 模块使用多个<strong>不同空洞率的空洞卷积</strong>并行处理输入特征图。这些空洞卷积可以捕捉不同尺度的上下文信息。</li><li><strong>聚合</strong>：ASPP 将不同空洞卷积的输出特征图进行拼接（concatenation），生成具有多尺度信息的特征表示。</li><li><strong>全局特征</strong>：ASPP 通常还包括一个全局平均池化层，以获取全局上下文信息，并将其与其他特征图结合。</li></ul><p><a href=https://blog.csdn.net/duanyajun987/article/details/82108006 target=_blank rel="noopener noreffer">对全局平均池化（GAP）过程的理解_全局平均池化层是由谁在哪提出的-CSDN博客</a></p><p>MobileNetUnet</p><p>Unet作为基本结构，MobileNet来提取特征</p><h2 id=点云分割>点云分割</h2><p><a href=https://blog.csdn.net/u014636245/article/details/82763269 target=_blank rel="noopener noreffer">https://blog.csdn.net/u014636245/article/details/82763269</a></p><p><a href=https://blog.csdn.net/weixin_43199584/article/details/104950696 target=_blank rel="noopener noreffer">https://blog.csdn.net/weixin_43199584/article/details/104950696</a></p><p>1、无序性 T-Net对称函数处理</p><p>2、稀疏性 <strong>局部采样+分组</strong>提取局部特征 pointnet++</p><p>3、非结构性 使用mlp而非CNN提取特征</p><h2 id=pointnet>pointnet</h2><p>CVPR2017 提出</p><p><strong>T-Net处理无序点集，基于mlp提取特征，高层特征与浅层特征融合</strong></p><p><strong>缺少邻域信息、对稀疏性敏感</strong></p><p><img class=lazyload src=/svg/loading.min.svg data-src=Summary%20127e4020f060808da8e0f23caf646e39/pointnet.jpg data-srcset="Summary%20127e4020f060808da8e0f23caf646e39/pointnet.jpg, Summary%20127e4020f060808da8e0f23caf646e39/pointnet.jpg 1.5x, Summary%20127e4020f060808da8e0f23caf646e39/pointnet.jpg 2x" data-sizes=auto alt=Summary%20127e4020f060808da8e0f23caf646e39/pointnet.jpg title=pointnet.jpg></p><p>各个点的操作过于独立，点的邻域信息没有得到有效利用（学位论文）</p><ul><li><strong>基本结构</strong>：PointNet 是<strong>首个直接处理点云的深度学习模型</strong>，使用对称函数（如最大池化）来处理无序点集。</li><li><strong>特点</strong>：通过全连接层提取特征，并通过池化层生成全局特征。PointNet 可以用于分类和分割任务。</li><li><strong>分割方法</strong>：在分割任务中，PointNet 会对每个点进行分类，输出每个点的标签。</li></ul><p>共享的mlp理解：</p><ul><li>在 PointNet 中，MLP 的权重是共享的，这意味着对于点云中的每个点，使用相同的 MLP 参数进行特征提取。换句话说，输入点云中的每个点都通过同一组权重来生成其特征表示。</li><li>这种权重共享的设计确保了网络对输入点的顺序不敏感，并使得网络能够处理任意数量的输入点。</li></ul><p>共享的 MLP 是 PointNet 网络设计中的一个核心部分，它通过共享权重来处理无序点云数据，提取每个点的特征，最终实现高效的全局特征聚合。这个设计使得 PointNet 能够在处理复杂的三维数据时，既保持了计算效率，又能够有效地捕捉几何信息。</p><h2 id=pointnet-1>pointnet++</h2><p>通过对数据<strong>进行采样、分组的方式提取局部特征</strong>,并使用MSG(multi-scale grouping)、
MRG(multi-resolution grouping)等策略自适应的处理密度不均匀的点云数据**（学位论文）**</p><ul><li><strong>改进</strong>：PointNet++ 是对 PointNet 的扩展，通过分层抽样和分组处理<strong>捕捉局部特征</strong>。</li><li><strong>特点</strong>：在每一层中，PointNet++ 会对点云进行分层处理，使模型能够捕捉更细粒度的特征。</li><li><strong>分割方法</strong>：适用于大规模点云数据，能够有效地进行语义分割和实例分割。</li></ul><h1 id=硕士论文改进>硕士论文改进</h1><p>1、紧凑网络 FC层——深度可分离卷积</p><p>2、结构化剪枝 减少宽度、深度的方式</p><p>3、参数量化 float32 to uint8</p><p><img class=lazyload src=/svg/loading.min.svg data-src=Summary%20127e4020f060808da8e0f23caf646e39/image%2049.png data-srcset="Summary%20127e4020f060808da8e0f23caf646e39/image%2049.png, Summary%20127e4020f060808da8e0f23caf646e39/image%2049.png 1.5x, Summary%20127e4020f060808da8e0f23caf646e39/image%2049.png 2x" data-sizes=auto alt=Summary%20127e4020f060808da8e0f23caf646e39/image%2049.png title=image.png></p><p>改进后的PointNet网络共有12层,整体可分为三个部分。
第一部分是前5层Conv1~ Conv5,主要作用是<strong>提取浅层特征</strong>,层名称后面的参数如1×1、1×9等表示的是卷积核尺寸,最后一个参数表示输出通道数,对于输入的n个点数据,每个点具有(x、y、z、r、g、b、x&rsquo;、y&rsquo;、z&rsquo;)9个特征,经过第一个卷积层Conv1之后,每个点的特征将会被融合为1个,之后再经过卷积层Conv2~ Conv5逐级提取特征,卷积层中的其他细节还包括高和宽两个方向上的步长分别为[1,1],填充方式使用的是“VALID”类型,激活函数为修正
线性单元ReLU,这些参数设定对于其他部分的卷积操作也是一致的;</p><p>第二部分是中间三层,主要作用是<strong>全局特征筛选和增加网络表达能力</strong>,该部分对于Conv5的输出先作一个最大池化,池化时的卷积核尺寸为n×1,相当于个输出通道保留一个特征,共得到256个全局特征,接着经过两个<strong>替代了FC层的可分离卷积操作</strong>对全局特征进行非线性变换,输出128个特征值;</p><p>第三部分是最后四层,作用是对前两部分提取的<strong>浅层和全局特征进行跳层连接并输出最终分割结果</strong>,得到每一个点属于13个类别中各个类别的概率。</p><h1 id=模型压缩>模型压缩</h1><h2 id=经典轻量化模型>经典轻量化模型</h2><p><a href="https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650735882&amp;idx=1&amp;sn=84eb9fd048df96b67061c46fe211ddbb&amp;scene=0" target=_blank rel="noopener noreffer">https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650735882&idx=1&sn=84eb9fd048df96b67061c46fe211ddbb&scene=0</a></p><p><img class=lazyload src=/svg/loading.min.svg data-src=Summary%20127e4020f060808da8e0f23caf646e39/image%2050.png data-srcset="Summary%20127e4020f060808da8e0f23caf646e39/image%2050.png, Summary%20127e4020f060808da8e0f23caf646e39/image%2050.png 1.5x, Summary%20127e4020f060808da8e0f23caf646e39/image%2050.png 2x" data-sizes=auto alt=Summary%20127e4020f060808da8e0f23caf646e39/image%2050.png title=image.png></p><h3 id=squeezenet-改进alexnet设计fire-module>squeezenet 改进AlexNet，设计Fire Module，</h3><h3 id=mobilenet><strong>mobilenet</strong></h3><p><strong>深度可分离卷积 完全分组卷积+点卷积</strong></p><p>逐深度卷积DW + 逐点卷积PW</p><p>通道可以看作为特征，普通的多输入输出卷积是密集连接的</p><p><img class=lazyload src=/svg/loading.min.svg data-src=Summary%20127e4020f060808da8e0f23caf646e39/image%2051.png data-srcset="Summary%20127e4020f060808da8e0f23caf646e39/image%2051.png, Summary%20127e4020f060808da8e0f23caf646e39/image%2051.png 1.5x, Summary%20127e4020f060808da8e0f23caf646e39/image%2051.png 2x" data-sizes=auto alt=Summary%20127e4020f060808da8e0f23caf646e39/image%2051.png title=image.png></p><p>广泛应用于移动端和嵌入式设备中的深度学习模型。它的主要思想是将标准卷积操作分解为两个更简单的步骤：深度卷积和逐点卷积。</p><h3 id=深度可分离卷积的组成>深度可分离卷积的组成</h3><ol><li><strong>深度卷积（Depthwise Convolution）</strong>：<ul><li>对输入的每个通道分别进行卷积操作。假设输入有 C 个通道，深度卷积将对每个通道应用一个 k×k 的卷积核。</li><li>输出的每个通道对应输入通道的卷积结果，因此输出通道的数量仍然为 C。</li></ul></li><li><strong>逐点卷积（Pointwise Convolution）</strong>：<ul><li>使用 1×1 的卷积核进行卷积，主要作用是对深度卷积的输出<strong>进行通道之间的线性组合。一组权重得到一个输出通道，总数是Cin×Cout</strong></li><li>逐点卷积的输出通道数可以根据需要进行调整。</li></ul></li></ol><h3 id=shufflenet>shufflenet</h3><p>改进Depthwise Separable Convolution，通道洗牌技术，每个组会包含其他组的信息</p><h3 id=xception>Xception</h3><p>googlenet V3改进</p><h2 id=轻量化模型设计>轻量化模型设计</h2><p><img class=lazyload src=/svg/loading.min.svg data-src=Summary%20127e4020f060808da8e0f23caf646e39/image%2052.png data-srcset="Summary%20127e4020f060808da8e0f23caf646e39/image%2052.png, Summary%20127e4020f060808da8e0f23caf646e39/image%2052.png 1.5x, Summary%20127e4020f060808da8e0f23caf646e39/image%2052.png 2x" data-sizes=auto alt=Summary%20127e4020f060808da8e0f23caf646e39/image%2052.png title=image.png></p><ul><li><strong>剪枝</strong>：去除不重要的神经元或连接，减小模型规模。</li><li><strong>量化</strong>：将浮点数参数转换为低位数表示（如 int8），减少存储和计算需求。</li><li><strong>知识蒸馏</strong>：通过训练小模型（学生模型）来模仿大模型（教师模型）的行为，提高小模型的性能。<strong>需要有一个大模型作为先验知识，用来帮助训练小模型</strong></li><li>紧凑网络</li><li>低秩分解</li></ul><h1 id=机器学习>机器学习</h1><p>模型 损失 策略</p><h2 id=分类>分类</h2><p>逻辑回归 通常二分类</p><p>朴素贝叶斯</p><p>KNN</p><p>决策树</p><h3 id=svm>SVM</h3><p>其核心思想是寻找一个最佳的分离超平面，将不同类别的数据点充分分开</p><p>合页损失</p><p>核函数在支持向量机和其他机器学习模型中发挥着重要作用，通过隐式地将数据映射到高维空间，从而能够有效地<strong>处理非线性可分的问题</strong></p><p>通过<code>sklearn.svm.SVC</code> 类来实现。以下是 SVM 主要的参数及其作用：</p><h3 id=1-c惩罚系数>1. <code>C</code>（惩罚系数）</h3><ul><li>控制分类的松弛程度。</li><li>较大的 <code>C</code> 值会让模型对训练样本进行严格分类，从而减少误分类（即小间隔的硬边界），但可能导致过拟合。</li><li>较小的 <code>C</code> 值会增大间隔，使得算法更具泛化性，但可能允许一定的误分类（即软边界）。</li></ul><h3 id=2-kernel核函数类型>2. <code>kernel</code>（核函数类型）</h3><ul><li>SVM 使用核函数将<strong>数据映射到高维空间，使得非线性数据可以进行线性分割</strong>。</li><li>常用的核函数包括：<ul><li><code>linear</code>：线性核，适用于线性可分数据。</li><li><code>poly</code>：多项式核，用于更复杂的多项式边界。</li><li><code>rbf</code>：径向基核（高斯核），用于大多数非线性问题。</li><li><code>sigmoid</code>：S 型核，有时用于神经网络中。</li></ul></li><li>默认值是 <code>rbf</code>。</li></ul><h3 id=3-degree多项式核的阶数>3. <code>degree</code>（多项式核的阶数）</h3><ul><li>仅在 <code>kernel='poly'</code> 时有效。</li><li>指定多项式核的阶数，常见的值为 2 或 3，阶数越高，模型越复杂。</li></ul><p><strong>集成模型类：</strong></p><p>随机森林</p><p>Bagging</p><p>AdaBoost</p><p>GBDT</p><p>XGBoost</p><h2 id=回归>回归</h2><p><a href=https://ster.im/py_sklearn_1/#%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E5%9B%9E%E5%BD%92 target=_blank rel="noopener noreffer">https://ster.im/py_sklearn_1/#%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E5%9B%9E%E5%BD%92</a></p><p>基础模型：</p><ul><li><strong>线性回归</strong>（包含岭回归、Lasso回归、弹性网络回归）</li><li>树回归</li><li><strong>支持向量机回归 SVR</strong></li><li>K近邻回归</li></ul><p>集成模型：</p><ul><li>随机森林回归</li><li>极端随机树回归</li><li>AdaBoost回归</li><li>Gradient Boosting回归</li></ul><h2 id=聚类无监督>聚类（无监督）</h2><p>K-means</p><ul><li><strong>原理</strong>：通过将数据点分配到最近的簇心来形成簇，然后更新簇心，重复这一过程直到收敛。</li><li><strong>优点</strong>：简单易实现，计算效率高，适用于大规模数据。</li><li><strong>缺点</strong>：需要预先指定簇的数量 k，对初始簇心敏感，可能会陷入局部最优。</li></ul><h3 id=算法伪代码>算法伪代码</h3><pre tabindex=0><code>输入：数据点集 X，簇数量 k
初始化：随机选择 k 个数据点作为初始簇心
重复直到收敛：
1. 对每个数据点 x_i ∈ X：
a. 计算与每个簇心的距离
b. 将 x_i 分配给最近的簇心
2. 对每个簇 j：
a. 更新簇心为属于该簇的所有点的均值
输出：每个数据点的簇分配和最终簇心
</code></pre><p>Mean Shift</p><p>GMM（Gaussian Mixture Model）</p><h2 id=降维>降维</h2><p>PCA与SVD</p><ul><li>PCA 需要计算数据协方差矩阵 C=m−11​XTX，并找出其特征向量和特征值。而 SVD 将 X 分解为 X=UΣVT 后，矩阵 V 的列就是协方差矩阵 C 的特征向量。</li><li>奇异值 Σ 中的每个值的平方，按比例与协方差矩阵的特征值相关联。这表示数据在主成分方向上的方差大小。</li><li><strong>SVD 是实现 PCA 的一种方法</strong>：在实际应用中，PCA 通常使用 SVD 来计算数据的主成分，因为 SVD 更稳定且适用于任意矩阵。</li><li>使用 SVD 进行 PCA 时，主成分是由 SVD 中的右奇异向量（矩阵 V）决定的，而奇异值的平方与数据的方差成比例。</li></ul><h2 id=离群检测>离群检测</h2><h3 id=1-基于统计的方法>1. 基于统计的方法</h3><ul><li><p><strong>Z-Score 方法</strong>：</p><ul><li><p>通过计算每个数据点的 Z-Score（标准分数）来判断离群点。如果 Z-Score 超过设定的阈值，则认为该点是离群点。</p></li><li><p>公式：
Z=σ(X−μ)​
其中，μ 是均值，σ 是标准差。</p><p>Z=(X−μ)σZ = \frac{(X - \mu)}{\sigma}</p><p>μ\mu</p><p>σ\sigma</p></li></ul></li><li><p><strong>箱形图（Box Plot）</strong>：</p><ul><li>通过分析数据的四分位数（Q1, Q3）和四分位距（IQR = Q3 - Q1）来判断离群点。通常，超过 Q1 - 1.5 * IQR 或 Q3 + 1.5 * IQR 的数据点被视为离群点。</li></ul></li></ul><p>from sklearn.covariance import EllipticEnvelope</p><p><strong>Elliptic Envelope</strong> 是一种基于统计的方法，用于离群检测，特别适用于高维数据的异常点检测。该算法通过拟合一个椭圆形的边界来估计数据的分布，从而识别离群点。</p><h1 id=深度学习>深度学习</h1><h2 id=cnn>CNN</h2><p>卷积 使用卷积核（过滤器）提取局部特征，通过<strong>权重共享</strong>降低参数数量</p><p>激活 非线性化</p><p>池化 缓解位置敏感性，增加平移不变性</p><p>全局平均池化 GAP 每个通道的特征图计算一个平均值，一般用来替代全连接层</p><p>全连接 每个神经元与前一层的所有神经元相连，常用于分类任务的输出。</p><pre><code>   对于图像特征，**先Flatten后使用**
</code></pre><p>BN层 提高模型稳定性和收敛性，批次内按特征进行标准化</p><p>初始化 Xavier</p><h2 id=过拟合欠拟合>过拟合、欠拟合</h2><h3 id=过拟合-模型太复杂测试集上效果差>过拟合 模型太复杂，测试集上效果差</h3><p>1、降低模型复杂度</p><p>2、增加数据、生成数据、数据增强</p><p>3、dropout</p><p>4、正则化（L1、L2）</p><ul><li><p><strong>L1 正则化（Lasso）</strong>：通过引入权重绝对值的惩罚项，促使某些权重变为零，从而进行特征选择。<strong>倾向于产生稀疏的权重向量，因为它将一些权重推向零</strong></p><pre><code>在权重接近零时的惩罚更为显著
</code></pre></li><li><p><strong>L2 正则化（Ridge）</strong>：通过引入权重平方和的惩罚项，限制权重的大小，从而防止模型对训练数据的过度拟合。</p></li></ul><p>5、早停 Early Stop</p><p>6、验证交叉</p><h3 id=欠拟合-模型太简单训练集-测试集上效果都差>欠拟合 模型太简单，训练集、 测试集上效果都差</h3><p>增加模型复杂度</p><p>减少正则化</p><p><strong>增加训练时间</strong></p><h2 id=交叉验证>交叉验证</h2><p>用于模型选择</p><h2 id=激活函数>激活函数</h2><p>relu</p><p>Leaky ReLU</p><p>sigmoid 二分类</p><p>tanh (-1, 1)</p><p>当输入在0附近时，tanh函数接近线性变换。函数的形状类似于sigmoid函数，
不同的是tanh函数关于坐标系原点中心对称。</p><h2 id=sigmoid和softmax区别>sigmoid和softmax区别</h2><ul><li><p><strong>Sigmoid</strong>：</p><ul><li><p><strong>公式</strong>：σ(x)=1+e−x1​</p><p>σ(x)=11+e−x\sigma(x) = \frac{1}{1 + e^{-x}}</p></li><li><p><strong>输出范围</strong>：将输入值映射到 (0, 1) 区间，<strong>适用于二分类问题</strong>。</p></li></ul></li><li><p><strong>Softmax</strong>：Softmax(zi​)=∑j=1n​ezj​ezi​​</p><ul><li><p><strong>公式</strong>：对于一个向量 z=[z1​,z2​,…,zn​]，Softmax 定义为：</p><p>z=[z1,z2,…,zn]z = [z_1, z_2, \ldots, z_n]</p></li></ul><p>Softmax(zi)=ezi∑j=1nezj\text{Softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{n} e^{z_j}}</p><ul><li><strong>输出范围</strong>：将输入值转换为 (0, 1) 区间，并且所有输出值的和为 1，适用于<strong>多分类</strong>问题。</li></ul></li></ul><h2 id=损失函数>损失函数</h2><p><img class=lazyload src=/svg/loading.min.svg data-src=Summary%20127e4020f060808da8e0f23caf646e39/image%2053.png data-srcset="Summary%20127e4020f060808da8e0f23caf646e39/image%2053.png, Summary%20127e4020f060808da8e0f23caf646e39/image%2053.png 1.5x, Summary%20127e4020f060808da8e0f23caf646e39/image%2053.png 2x" data-sizes=auto alt=Summary%20127e4020f060808da8e0f23caf646e39/image%2053.png title=image.png></p><h3 id=分类损失>分类损失</h3><p>0-1损失 二分类</p><p><strong>交叉熵损失</strong></p><p><a href=https://www.vectorexplore.com/tech/loss-functions/cross-entropy/ target=_blank rel="noopener noreffer">https://www.vectorexplore.com/tech/loss-functions/cross-entropy/</a></p><p><strong>衡量两个分布之间的相似程度(距离)</strong></p><p>模型训练时，通过<strong>最小化交叉熵损失函数</strong>，我们可以使模型预测值的概率分布逐步接近真实的概率分布。</p><ul><li><p><strong>二元交叉熵损失（Binary Cross-Entropy Loss）</strong>：</p><p><img class=lazyload src=/svg/loading.min.svg data-src=Summary%20127e4020f060808da8e0f23caf646e39/image%2054.png data-srcset="Summary%20127e4020f060808da8e0f23caf646e39/image%2054.png, Summary%20127e4020f060808da8e0f23caf646e39/image%2054.png 1.5x, Summary%20127e4020f060808da8e0f23caf646e39/image%2054.png 2x" data-sizes=auto alt=Summary%20127e4020f060808da8e0f23caf646e39/image%2054.png title=image.png></p><p>n表示样本数量</p></li><li><p><strong>多元交叉熵损失（Categorical Cross-Entropy Loss）</strong>：</p><p><img class=lazyload src=/svg/loading.min.svg data-src=Summary%20127e4020f060808da8e0f23caf646e39/image%2055.png data-srcset="Summary%20127e4020f060808da8e0f23caf646e39/image%2055.png, Summary%20127e4020f060808da8e0f23caf646e39/image%2055.png 1.5x, Summary%20127e4020f060808da8e0f23caf646e39/image%2055.png 2x" data-sizes=auto alt=Summary%20127e4020f060808da8e0f23caf646e39/image%2055.png title=image.png></p></li><li><p>适用于多分类问题，C 表示类别数。</p></li><li><p>C=2时，等价于BCE</p></li></ul><p>KL散度/相对熵</p><p>对数损失</p><p>指数损失</p><p>合页损失 svm</p><h3 id=回归损失>回归损失</h3><p><img class=lazyload src=/svg/loading.min.svg data-src=Summary%20127e4020f060808da8e0f23caf646e39/image%2056.png data-srcset="Summary%20127e4020f060808da8e0f23caf646e39/image%2056.png, Summary%20127e4020f060808da8e0f23caf646e39/image%2056.png 1.5x, Summary%20127e4020f060808da8e0f23caf646e39/image%2056.png 2x" data-sizes=auto alt=Summary%20127e4020f060808da8e0f23caf646e39/image%2056.png title=image.png></p><p>L1损失-MAE</p><p>L2损失-MSE</p><p>huber损失</p><h3 id=检测分割>检测、分割</h3><p>IoU损失</p><h2 id=优化算法>优化算法</h2><p>梯度下降法（每次使用全部数据，计算一个平均下降方向，速度慢）</p><p>随机梯度下降（每次使用单个数据，不稳定）</p><p>小批量随机梯度下降 （兼顾速度和稳定性）</p><p>动量法 加了一个“动量”项，用于平滑参数更新</p><h3 id=adam>Adam</h3><p><strong>结合了动量法和 RMSProp，计算梯度的一阶矩（均值）和二阶矩（方差）来调整学习率，使得优化过程更加平滑。</strong></p><p>Adam 优化器的主要参数包括学习率（learning rate）、β1（一阶矩估计的指数衰减率）、β2（二阶矩估计的指数衰减率）和ε（数值稳定性参数）。以下是这些参数的详细说明：</p><ol><li><strong>学习率（learning rate）（默认值：0.001）：</strong>
学习率决定了每次参数更新的步长大小。较小的学习率可以使模型收敛更加稳定，但可能会导致训练速度较慢；较大的学习率可以加快训练速度，但可能会导致模型在参数空间中跳跃过大，难以收敛。</li><li><strong>β1（一阶矩估计的指数衰减率）（默认值：0.9）：</strong>
Adam 优化器使用指数移动平均来估计梯度的一阶矩，β1 参数控制了一阶矩估计的衰减率。较大的 β1 值会使得历史梯度信息对当前梯度的影响更大，使优化过程更加稳定。</li><li><strong>β2（二阶矩估计的指数衰减率）（默认值：0.999）：</strong>
Adam 优化器还使用指数移动平均来估计梯度的二阶矩，β2 参数控制了二阶矩估计的衰减率。较大的 β2 值会使得历史梯度平方的影响更大，对梯度更新的方向进行调整，有助于降低噪声对优化过程的影响。</li><li><strong>ε（数值稳定性参数）（默认值：1e-7）：</strong>
ε 参数是为了提高数值稳定性而添加的一个小的常数，防止分母为零。它在计算梯度的二阶矩估计时使用，避免了除零错误。</li></ol><p>这些参数通常会根据具体的任务和数据集进行调整，以获得最佳的性能和收敛速度。通常情况下，Adam 优化器的默认参数值已经在许多任务中表现良好，因此在大多数情况下，不需要进行额外的调整。</p><h2 id=rnn>RNN</h2><p>循环神经网络是具有<strong>隐状态</strong>的神经网络</p><pre><code>        后一步的输入依赖前一步的输出，层间有连接
</code></pre><p>即隐藏层之间的节点不再无连接而是有连接的，并且隐藏层的输入不仅包括输入层的输出还包括上一时刻隐藏层的输出。</p><p><a href=https://blog.csdn.net/bestrivern/article/details/90723524 target=_blank rel="noopener noreffer">https://blog.csdn.net/bestrivern/article/details/90723524</a></p><p><a href=http://www.jokerak.com/deep-learning/5-RNN/#_5-1-%E6%A6%82%E5%BF%B5 target=_blank rel="noopener noreffer">http://www.jokerak.com/deep-learning/5-RNN/#_5-1-%E6%A6%82%E5%BF%B5</a></p><p>LSTM</p><p>seq2seq</p><h2 id=cnn与rnn区别>CNN与RNN区别</h2><h3 id=输入数据类型>输入数据类型</h3><ul><li><strong>CNN</strong>：<ul><li>主要处理二维或三维数据，如图像（高度、宽度、颜色通道）和视频帧。</li><li>通过卷积层提取空间特征，适合于图像分类、目标检测等任务。</li></ul></li><li><strong>RNN</strong>：<ul><li>主要处理一维序列数据，如时间序列、文本等。</li><li><strong>能够处理变长输入</strong>，适合于自然语言处理、语音识别等任务。</li></ul></li></ul><h3 id=训练和计算>训练和计算</h3><ul><li><strong>CNN</strong>：<ul><li><strong>并行计算较为容易，因为卷积操作可以在不同的区域同时进行</strong>。</li><li>通常使用批量训练，加速计算过程。</li></ul></li><li><strong>RNN</strong>：<ul><li>训练过程通常是顺序的，因为<strong>每个时间步的输出依赖于前一个时间步的状态，难以并行化。</strong></li><li>更<strong>容易出现梯度消失或梯度爆炸</strong>的问题，通常使用长短期记忆网络（LSTM）或门控递归单元（GRU）来缓解这些问题。</li></ul></li></ul><h2 id=cnn与mlp>CNN与MLP</h2><ol><li><strong>结构：</strong><ul><li>多层感知机（MLP）是一种基本的前馈神经网络结构，由<strong>多个全连接层组成，每个神经元与上一层的所有神经元相连</strong>。</li><li>卷积神经网络（CNN）是一种专门用于处理网格结构数据（如图像）的神经网络结构，其中包含卷积层、池化层和全连接层等。卷积层通过卷积操作来提取局部特征，池化层通过池化操作来减少特征图的大小和参数数量。</li></ul></li><li><strong>输入数据的结构：</strong><ul><li>多层感知机（MLP）通常用于处理一维的数据，例如序列数据、文本数据等。</li><li>卷积神经网络（CNN）通常用于处理二维的数据，例如图像数据，但也可以扩展到处理三维的数据，例如视频数据。</li></ul></li><li><strong>参数共享：</strong><ul><li>在卷积神经网络（CNN）中，<strong>卷积操作具有参数共享的特性</strong>，即卷积核在整个特征图上移动时所使用的参数是相同的，这大大减少了需要学习的参数数量，提高了模型的效率和泛化能力。</li><li>在多层感知机（MLP）中，每个神经元与上一层的所有神经元相连，参数独立，没有参数共享的概念。</li></ul></li><li><strong>特征提取：</strong><ul><li>卷积神经网络（CNN）通过卷积操作和池化操作来逐层提取输入数据的特征，从而学习到数据的局部和全局特征，具有良好的特征提取能力。</li><li>多层感知机（MLP）则是通过全连接层来<strong>对输入数据进行线性变换和非线性变换</strong>，对于复杂的结构化数据，需要更多的层和参数来进行学习，且往往需要更多的数据来进行训练。不激活时相当于是多个线性变换</li></ul></li></ol><h3 id=上采样反池化反卷积>上采样、反池化、反卷积</h3><p><a href=https://blog.csdn.net/A_a_ron/article/details/79181108 target=_blank rel="noopener noreffer">https://blog.csdn.net/A_a_ron/article/details/79181108</a></p><p><img class=lazyload src=/svg/loading.min.svg data-src=Summary%20127e4020f060808da8e0f23caf646e39/image%2057.png data-srcset="Summary%20127e4020f060808da8e0f23caf646e39/image%2057.png, Summary%20127e4020f060808da8e0f23caf646e39/image%2057.png 1.5x, Summary%20127e4020f060808da8e0f23caf646e39/image%2057.png 2x" data-sizes=auto alt=Summary%20127e4020f060808da8e0f23caf646e39/image%2057.png title=image.png></p><p><a href=https://blog.csdn.net/yq_forever/article/details/101169771 target=_blank rel="noopener noreffer">https://blog.csdn.net/yq_forever/article/details/101169771</a></p><p>上采样 最近邻插值、双线性插值等</p><p>反池化 需要池化索引</p><p>反卷积 参数需要学习</p><h3 id=bnln>BN、LN</h3><ul><li><strong>Batch Normalization (BN)</strong>：<strong>在批次维度上进行归一化</strong>，即对每个 mini-batch 内的<strong>所有样本的同一特征进行归一化</strong>。公式中使用的是每个 mini-batch 内特征的均值和方差。适合卷积神经网络（CNN）等批量大小比较大的场景。<strong>特征归一化</strong></li><li><strong>Layer Normalization (LN)</strong>：在层维度上进行归一化，即对<strong>每个样本的所有特征进行归一化</strong>。公式中使用的是单个样本的所有特征的均值和方差。适合循环神经网络（RNN）和 Transformer 等批量大小较小或动态性强的场景。<strong>样本归一化</strong></li></ul><h1 id=数据分析>数据分析</h1><ul><li><strong>Pandas</strong>：数据处理和分析的核心库。</li><li><strong>NumPy</strong>：支持高性能的数值计算。</li><li><strong>Matplotlib</strong>：基础绘图库。</li><li><strong>Seaborn</strong>：统计</li><li>Scikit-Learn</li><li>SciPy</li></ul><p>数据清洗是数据分析中至关重要的步骤，能够有效提高数据的质量。清洗步骤包括删除重复、处理缺失值、转换数据类型、去除异常值、标准化和归一化、文本清理、编码分类数据等。Python 中，<code>pandas</code>、<code>numpy</code> 和 <code>sklearn</code> 等库为数据清洗提供了强大的功能。</p><p>缺失值处理 插值、删除、替换</p><p>异常值处理 删除、替换</p><p>数据预处理 StandardScaler、MinMaxScaler</p><p>将数据按列转换为均值为 0、标准差为 1 的标准正态分布</p><p><code>StandardScaler</code> 会对<strong>每一列特征分别计算</strong>其均值和标准差，并用以下公式将数据标准化：</p><p><img class=lazyload src=/svg/loading.min.svg data-src=Summary%20127e4020f060808da8e0f23caf646e39/image%2058.png data-srcset="Summary%20127e4020f060808da8e0f23caf646e39/image%2058.png, Summary%20127e4020f060808da8e0f23caf646e39/image%2058.png 1.5x, Summary%20127e4020f060808da8e0f23caf646e39/image%2058.png 2x" data-sizes=auto alt=Summary%20127e4020f060808da8e0f23caf646e39/image%2058.png title=image.png></p><p><strong>与BN类似都是对特征进行标准化</strong></p><h1 id=大模型>大模型</h1><h2 id=gan>GAN</h2><p>生成器、判别器</p><h2 id=chatgpt>chatgpt</h2><h2 id=transformer架构>transformer架构</h2><p><a href="https://www.notion.so/Transformer-138d578c02cb47a48eb99517a7e8af3c?pvs=21" target=_blank rel="noopener noreffer"><strong>Transformer</strong></a></p><p>基于自注意力机制和多头注意力机制</p><h3 id=attention-is-all-you-need><strong>Attention Is All You Need</strong></h3><ul><li><strong>作者</strong>：Vaswani et al.</li><li><strong>年份</strong>：2017</li><li><strong>链接</strong>：<a href=https://arxiv.org/abs/1706.03762 target=_blank rel="noopener noreffer">论文链接</a></li><li><strong>贡献</strong>：提出了 Transformer 架构，完全基于自注意力机制，取代了传统的循环神经网络（RNN）。这是注意力机制在自然语言处理中的重要里程碑。</li></ul><h3 id=注意力机制>注意力机制</h3><p>注意力就是权重，即关注程度</p><p>Q，K，V</p><p>查询（自主性提示）、键、值</p><p><img class=lazyload src=/svg/loading.min.svg data-src=Summary%20127e4020f060808da8e0f23caf646e39/%25E6%25B3%25A8%25E6%2584%258F%25E5%258A%259B%25E6%259C%25BA%25E5%2588%25B6Transformer_%25E8%25B5%25B5%25E4%25BA%2591_20240408_%25E9%25A1%25B5%25E9%259D%25A2_08.png data-srcset="Summary%20127e4020f060808da8e0f23caf646e39/%25E6%25B3%25A8%25E6%2584%258F%25E5%258A%259B%25E6%259C%25BA%25E5%2588%25B6Transformer_%25E8%25B5%25B5%25E4%25BA%2591_20240408_%25E9%25A1%25B5%25E9%259D%25A2_08.png, Summary%20127e4020f060808da8e0f23caf646e39/%25E6%25B3%25A8%25E6%2584%258F%25E5%258A%259B%25E6%259C%25BA%25E5%2588%25B6Transformer_%25E8%25B5%25B5%25E4%25BA%2591_20240408_%25E9%25A1%25B5%25E9%259D%25A2_08.png 1.5x, Summary%20127e4020f060808da8e0f23caf646e39/%25E6%25B3%25A8%25E6%2584%258F%25E5%258A%259B%25E6%259C%25BA%25E5%2588%25B6Transformer_%25E8%25B5%25B5%25E4%25BA%2591_20240408_%25E9%25A1%25B5%25E9%259D%25A2_08.png 2x" data-sizes=auto alt=Summary%20127e4020f060808da8e0f23caf646e39/%25E6%25B3%25A8%25E6%2584%258F%25E5%258A%259B%25E6%259C%25BA%25E5%2588%25B6Transformer_%25E8%25B5%25B5%25E4%25BA%2591_20240408_%25E9%25A1%25B5%25E9%259D%25A2_08.png title=注意力机制&amp;Transformer_赵云_20240408_页面_08.png></p><p>权重由<strong>注意力评分函数</strong>计算</p><h2 id=自注意力机制-self-attention>自注意力机制 <strong>（Self-Attention）</strong></h2><p>source=target这种特殊的注意力机制</p><p>在<strong>同一输入序列内部计算注意力</strong>，使得每个元素都可以与其他元素进行交互，增强上下文信息的理解。</p><p><img class=lazyload src=/svg/loading.min.svg data-src=Summary%20127e4020f060808da8e0f23caf646e39/%25E6%25B3%25A8%25E6%2584%258F%25E5%258A%259B%25E6%259C%25BA%25E5%2588%25B6Transformer_%25E8%25B5%25B5%25E4%25BA%2591_20240408_%25E9%25A1%25B5%25E9%259D%25A2_14.png data-srcset="Summary%20127e4020f060808da8e0f23caf646e39/%25E6%25B3%25A8%25E6%2584%258F%25E5%258A%259B%25E6%259C%25BA%25E5%2588%25B6Transformer_%25E8%25B5%25B5%25E4%25BA%2591_20240408_%25E9%25A1%25B5%25E9%259D%25A2_14.png, Summary%20127e4020f060808da8e0f23caf646e39/%25E6%25B3%25A8%25E6%2584%258F%25E5%258A%259B%25E6%259C%25BA%25E5%2588%25B6Transformer_%25E8%25B5%25B5%25E4%25BA%2591_20240408_%25E9%25A1%25B5%25E9%259D%25A2_14.png 1.5x, Summary%20127e4020f060808da8e0f23caf646e39/%25E6%25B3%25A8%25E6%2584%258F%25E5%258A%259B%25E6%259C%25BA%25E5%2588%25B6Transformer_%25E8%25B5%25B5%25E4%25BA%2591_20240408_%25E9%25A1%25B5%25E9%259D%25A2_14.png 2x" data-sizes=auto alt=Summary%20127e4020f060808da8e0f23caf646e39/%25E6%25B3%25A8%25E6%2584%258F%25E5%258A%259B%25E6%259C%25BA%25E5%2588%25B6Transformer_%25E8%25B5%25B5%25E4%25BA%2591_20240408_%25E9%25A1%25B5%25E9%259D%25A2_14.png title=注意力机制&amp;Transformer_赵云_20240408_页面_14.png></p><p>每个词表示为其他所有词的加权和</p><p><strong>整体架构</strong></p><p><img class=lazyload src=/svg/loading.min.svg data-src=Summary%20127e4020f060808da8e0f23caf646e39/%25E6%25B3%25A8%25E6%2584%258F%25E5%258A%259B%25E6%259C%25BA%25E5%2588%25B6Transformer_%25E8%25B5%25B5%25E4%25BA%2591_20240408_%25E9%25A1%25B5%25E9%259D%25A2_13.png data-srcset="Summary%20127e4020f060808da8e0f23caf646e39/%25E6%25B3%25A8%25E6%2584%258F%25E5%258A%259B%25E6%259C%25BA%25E5%2588%25B6Transformer_%25E8%25B5%25B5%25E4%25BA%2591_20240408_%25E9%25A1%25B5%25E9%259D%25A2_13.png, Summary%20127e4020f060808da8e0f23caf646e39/%25E6%25B3%25A8%25E6%2584%258F%25E5%258A%259B%25E6%259C%25BA%25E5%2588%25B6Transformer_%25E8%25B5%25B5%25E4%25BA%2591_20240408_%25E9%25A1%25B5%25E9%259D%25A2_13.png 1.5x, Summary%20127e4020f060808da8e0f23caf646e39/%25E6%25B3%25A8%25E6%2584%258F%25E5%258A%259B%25E6%259C%25BA%25E5%2588%25B6Transformer_%25E8%25B5%25B5%25E4%25BA%2591_20240408_%25E9%25A1%25B5%25E9%259D%25A2_13.png 2x" data-sizes=auto alt=Summary%20127e4020f060808da8e0f23caf646e39/%25E6%25B3%25A8%25E6%2584%258F%25E5%258A%259B%25E6%259C%25BA%25E5%2588%25B6Transformer_%25E8%25B5%25B5%25E4%25BA%2591_20240408_%25E9%25A1%25B5%25E9%259D%25A2_13.png title=注意力机制&amp;Transformer_赵云_20240408_页面_13.png></p><p><strong>训练时使用的是教师强制，预测第n+1个token时输入前n个token真实值</strong></p><p>训练时：第i个decoder的输入 = encoder输出 + ground truth embeding</p><p>预测时：第i个decoder的输入 = encoder输出 + 第(i-1)个decoder输出</p><p><strong>训练时因为知道ground truth embeding，相当于知道正确答案，网络可以一次训练完成</strong>。</p><p>预测时，首先输入start，输出预测的第一个单词 然后start和新单词组成新的query，再输入decoder来预测下一个单词，循环往复 直至end</p><p><strong>相当于训练时是有标签的</strong></p><p><img class=lazyload src=/svg/loading.min.svg data-src=Summary%20127e4020f060808da8e0f23caf646e39/%25E6%25B3%25A8%25E6%2584%258F%25E5%258A%259B%25E6%259C%25BA%25E5%2588%25B6Transformer_%25E8%25B5%25B5%25E4%25BA%2591_20240408_%25E9%25A1%25B5%25E9%259D%25A2_15.png data-srcset="Summary%20127e4020f060808da8e0f23caf646e39/%25E6%25B3%25A8%25E6%2584%258F%25E5%258A%259B%25E6%259C%25BA%25E5%2588%25B6Transformer_%25E8%25B5%25B5%25E4%25BA%2591_20240408_%25E9%25A1%25B5%25E9%259D%25A2_15.png, Summary%20127e4020f060808da8e0f23caf646e39/%25E6%25B3%25A8%25E6%2584%258F%25E5%258A%259B%25E6%259C%25BA%25E5%2588%25B6Transformer_%25E8%25B5%25B5%25E4%25BA%2591_20240408_%25E9%25A1%25B5%25E9%259D%25A2_15.png 1.5x, Summary%20127e4020f060808da8e0f23caf646e39/%25E6%25B3%25A8%25E6%2584%258F%25E5%258A%259B%25E6%259C%25BA%25E5%2588%25B6Transformer_%25E8%25B5%25B5%25E4%25BA%2591_20240408_%25E9%25A1%25B5%25E9%259D%25A2_15.png 2x" data-sizes=auto alt=Summary%20127e4020f060808da8e0f23caf646e39/%25E6%25B3%25A8%25E6%2584%258F%25E5%258A%259B%25E6%259C%25BA%25E5%2588%25B6Transformer_%25E8%25B5%25B5%25E4%25BA%2591_20240408_%25E9%25A1%25B5%25E9%259D%25A2_15.png title=注意力机制&amp;Transformer_赵云_20240408_页面_15.png></p><h3 id=concat-操作>concat 操作</h3><pre tabindex=0><code>拼接输出：将所有注意力头的输出拼接在一起，形成一个大的特征向量。
Concat(Attention1,Attention2,…,Attentionh)=Attention1∥Attention2∥…∥Attentionh
Concat(Attention1,Attention2,…,Attentionh)=Attention1∥Attention2∥…∥Attentionh

其中 hh 是注意力头的数量，∥∥ 表示拼接操作。
</code></pre><p><img class=lazyload src=/svg/loading.min.svg data-src=Summary%20127e4020f060808da8e0f23caf646e39/%25E6%25B3%25A8%25E6%2584%258F%25E5%258A%259B%25E6%259C%25BA%25E5%2588%25B6Transformer_%25E8%25B5%25B5%25E4%25BA%2591_20240408_%25E9%25A1%25B5%25E9%259D%25A2_16.png data-srcset="Summary%20127e4020f060808da8e0f23caf646e39/%25E6%25B3%25A8%25E6%2584%258F%25E5%258A%259B%25E6%259C%25BA%25E5%2588%25B6Transformer_%25E8%25B5%25B5%25E4%25BA%2591_20240408_%25E9%25A1%25B5%25E9%259D%25A2_16.png, Summary%20127e4020f060808da8e0f23caf646e39/%25E6%25B3%25A8%25E6%2584%258F%25E5%258A%259B%25E6%259C%25BA%25E5%2588%25B6Transformer_%25E8%25B5%25B5%25E4%25BA%2591_20240408_%25E9%25A1%25B5%25E9%259D%25A2_16.png 1.5x, Summary%20127e4020f060808da8e0f23caf646e39/%25E6%25B3%25A8%25E6%2584%258F%25E5%258A%259B%25E6%259C%25BA%25E5%2588%25B6Transformer_%25E8%25B5%25B5%25E4%25BA%2591_20240408_%25E9%25A1%25B5%25E9%259D%25A2_16.png 2x" data-sizes=auto alt=Summary%20127e4020f060808da8e0f23caf646e39/%25E6%25B3%25A8%25E6%2584%258F%25E5%258A%259B%25E6%259C%25BA%25E5%2588%25B6Transformer_%25E8%25B5%25B5%25E4%25BA%2591_20240408_%25E9%25A1%25B5%25E9%259D%25A2_16.png title=注意力机制&amp;Transformer_赵云_20240408_页面_16.png></p><p>Transformer 模型中的前馈网络主要负责对输入进行特征转换和表示学习，以便于模型理解输入序列的语义信息和结构关系。这些<strong>前馈网络通常是浅层的，只有一个或两个全连接隐藏层，以减少模型的复杂度，并避免过拟合</strong>。</p><p><img class=lazyload src=/svg/loading.min.svg data-src=Summary%20127e4020f060808da8e0f23caf646e39/%25E6%25B3%25A8%25E6%2584%258F%25E5%258A%259B%25E6%259C%25BA%25E5%2588%25B6Transformer_%25E8%25B5%25B5%25E4%25BA%2591_20240408_%25E9%25A1%25B5%25E9%259D%25A2_17.png data-srcset="Summary%20127e4020f060808da8e0f23caf646e39/%25E6%25B3%25A8%25E6%2584%258F%25E5%258A%259B%25E6%259C%25BA%25E5%2588%25B6Transformer_%25E8%25B5%25B5%25E4%25BA%2591_20240408_%25E9%25A1%25B5%25E9%259D%25A2_17.png, Summary%20127e4020f060808da8e0f23caf646e39/%25E6%25B3%25A8%25E6%2584%258F%25E5%258A%259B%25E6%259C%25BA%25E5%2588%25B6Transformer_%25E8%25B5%25B5%25E4%25BA%2591_20240408_%25E9%25A1%25B5%25E9%259D%25A2_17.png 1.5x, Summary%20127e4020f060808da8e0f23caf646e39/%25E6%25B3%25A8%25E6%2584%258F%25E5%258A%259B%25E6%259C%25BA%25E5%2588%25B6Transformer_%25E8%25B5%25B5%25E4%25BA%2591_20240408_%25E9%25A1%25B5%25E9%259D%25A2_17.png 2x" data-sizes=auto alt=Summary%20127e4020f060808da8e0f23caf646e39/%25E6%25B3%25A8%25E6%2584%258F%25E5%258A%259B%25E6%259C%25BA%25E5%2588%25B6Transformer_%25E8%25B5%25B5%25E4%25BA%2591_20240408_%25E9%25A1%25B5%25E9%259D%25A2_17.png title=注意力机制&amp;Transformer_赵云_20240408_页面_17.png></p><p><img class=lazyload src=/svg/loading.min.svg data-src=Summary%20127e4020f060808da8e0f23caf646e39/%25E6%25B3%25A8%25E6%2584%258F%25E5%258A%259B%25E6%259C%25BA%25E5%2588%25B6Transformer_%25E8%25B5%25B5%25E4%25BA%2591_20240408_%25E9%25A1%25B5%25E9%259D%25A2_18.png data-srcset="Summary%20127e4020f060808da8e0f23caf646e39/%25E6%25B3%25A8%25E6%2584%258F%25E5%258A%259B%25E6%259C%25BA%25E5%2588%25B6Transformer_%25E8%25B5%25B5%25E4%25BA%2591_20240408_%25E9%25A1%25B5%25E9%259D%25A2_18.png, Summary%20127e4020f060808da8e0f23caf646e39/%25E6%25B3%25A8%25E6%2584%258F%25E5%258A%259B%25E6%259C%25BA%25E5%2588%25B6Transformer_%25E8%25B5%25B5%25E4%25BA%2591_20240408_%25E9%25A1%25B5%25E9%259D%25A2_18.png 1.5x, Summary%20127e4020f060808da8e0f23caf646e39/%25E6%25B3%25A8%25E6%2584%258F%25E5%258A%259B%25E6%259C%25BA%25E5%2588%25B6Transformer_%25E8%25B5%25B5%25E4%25BA%2591_20240408_%25E9%25A1%25B5%25E9%259D%25A2_18.png 2x" data-sizes=auto alt=Summary%20127e4020f060808da8e0f23caf646e39/%25E6%25B3%25A8%25E6%2584%258F%25E5%258A%259B%25E6%259C%25BA%25E5%2588%25B6Transformer_%25E8%25B5%25B5%25E4%25BA%2591_20240408_%25E9%25A1%25B5%25E9%259D%25A2_18.png title=注意力机制&amp;Transformer_赵云_20240408_页面_18.png></p><h2 id=线性变换>线性变换</h2><p>在 Transformer 模型中，线性变换后的输出是一个 logits 向量，这个向量用于表示每个可能输出（通常是词汇表中的词）的得分。具体来说，以下是线性变换后输出的详细过程：</p><h3 id=1-解码器输出>1. 解码器输出</h3><p>在 Transformer 的解码器部分，经过多层自注意力和编码器-解码器注意力之后，最终生成的输出是一个表示当前时间步的上下文特征的向量。这一向量通常被称为解码器的输出。</p><h3 id=2-线性变换>2. 线性变换</h3><ul><li><p><strong>线性层</strong>：解码器的输出经过一个线性层（全连接层），将其映射到词汇表的维度。假设词汇表大小为 VVV，解码器的输出向量维度为 dmodeld_{model}dmodel​，则线性变换的公式为：Logits=W⋅Output+b</p><p>Logits=W⋅Output+b\text{Logits} = W \cdot \text{Output} + b</p><p>其中：</p><ul><li><p>WWW 是权重矩阵，维度为 V×dmodel​。</p><p>V×dmodelV \times d_{model}</p></li><li><p>Output\text{Output}Output 是解码器的输出向量，维度为 dmodel​。</p><p>dmodeld_{model}</p></li><li><p>bbb 是偏置项，维度为 V。</p><p>VV</p></li></ul></li></ul><h3 id=3-输出解释>3. 输出解释</h3><ul><li><p><strong>Logits 向量</strong>：线性变换后的输出，即 logits 向量，维度为 V，表示模型对于词汇表中每个词的得分。这些得分并不直接表示概率，而是用来计算每个词的相对可能性。</p><p>VV</p></li></ul><h3 id=4-进一步处理>4. 进一步处理</h3><ul><li><p><strong>Softmax 函数</strong>：将 logits 向量传递给 Softmax 函数，转换为概率分布，使得每个词的输出概率可以用来选择下一个生成的词。P(yi​=k)=∑j​elogitsj​elogitsk​​</p><p>P(yi=k)=elogitsk∑jelogitsjP(y_i = k) = \frac{e^{\text{logits}<em>k}}{\sum</em>{j} e^{\text{logits}_j}}</p></li></ul><h3 id=总结>总结</h3><p>在 Transformer 中，线性变换后的输出是一个 logits 向量，它用于表示模型对每个可能输出的得分。这个 logits 向量随后通过 Softmax 函数转换为概率分布，从而实现下一个词的预测。</p><h2 id=softmax激活>softmax激活</h2><p>在 Transformer 模型中，Softmax 函数被用于预测输出概率，特别是在生成序列时，例如文本生成、翻译等任务。以下是 Transformer 如何利用 Softmax 进行输出概率预测的详细步骤：</p><h3 id=1-模型结构>1. 模型结构</h3><p>Transformer 主要由编码器和解码器组成。解码器负责生成输出序列，其最后一层通常是线性层后接 Softmax 函数。</p><h3 id=2-生成输出序列的步骤>2. 生成输出序列的步骤</h3><h3 id=21-输入处理>2.1 输入处理</h3><ul><li>在解码器中，输入通常是前一个时间步的输出（或者是特定的起始符号）。</li><li>解码器中的自注意力机制和编码器-解码器注意力机制用于生成当前时间步的上下文表示。</li></ul><h3 id=22-线性变换>2.2 线性变换</h3><ul><li><p>解码器的输出经过线性层（也称为全连接层），将其映射到词汇表大小的维度。Logits=W⋅Output</p><p>Logits=W⋅Output\text{Logits} = W \cdot \text{Output}</p><p>其中 WWW 是权重矩阵，Output\text{Output}Output 是解码器的输出。</p></li></ul><h3 id=23-softmax-函数>2.3 Softmax 函数</h3><ul><li><p>对于生成的 logits，使用 Softmax 函数将其转换为概率分布：P(yi​=k)=∑j​elogitsj​elogitsk​​</p><p>P(yi=k)=elogitsk∑jelogitsjP(y_i = k) = \frac{e^{\text{logits}<em>k}}{\sum</em>{j} e^{\text{logits}_j}}</p><p>其中 kkk 表示词汇表中的每个词，logitsk\text{logits}_klogitsk​ 是对应词的 logits 值。</p></li></ul><h3 id=3-输出解释-1>3. 输出解释</h3><ul><li><p>Softmax 的输出 P(yi​) 表示当前时间步生成每个词的概率。这些概率可以用于选择最可能的下一个词：</p><p>P(yi)P(y_i)</p><ul><li><strong>采样</strong>：可以通过随机选择一个词（根据概率分布）来生成文本。</li><li><strong>贪婪搜索</strong>：选择概率最大的词作为输出。</li><li><strong>束搜索（Beam Search）</strong>：在生成过程中维护多个候选序列，以提高生成的质量。</li></ul></li></ul><h3 id=4-损失计算>4. 损失计算</h3><ul><li><p>在训练过程中，使用交叉熵损失来衡量模型预测的概率分布与实际目标分布之间的差距：Loss=−i∑​yi​log(P(yi​))</p><p>Loss=−∑iyilog⁡(P(yi))\text{Loss} = -\sum_{i} y_i \log(P(y_i))</p><p>其中 yiy_iyi​ 是目标词的独热编码，P(yi)P(y_i)P(yi​) 是模型预测的概率。</p></li></ul><h3 id=总结-1>总结</h3><p>在 Transformer 中，Softmax 函数通过将线性变换后的输出映射为概率分布，使得模型能够预测下一个词的可能性。这一机制使得 Transformer 在序列生成任务中表现出色，能够灵活处理多种类型的文本生成和翻译任务。</p><h2 id=训练中的shifted-right操作>训练中的shifted right操作</h2><p>在 Transformer 模型的训练中，“<strong>shifted right</strong>” 是指将目标输出序列向右移动一个位置，从而在解码器中进行自回归的训练过程。这是序列生成任务（例如机器翻译、文本生成）中常用的技巧，用于<strong>确保解码器在预测当前词时只能访问先前的词，而不能看到未来词，从而避免“数据泄露”</strong>。以下是这一操作的详细过程：</p><h3 id=1-shifted-right-的操作步骤>1. Shifted Right 的操作步骤</h3><p>假设我们有目标输出序列 y=[y1,y2,y3,…,yn]\text{y} = [y_1, y_2, y_3, \ldots, y_n]y=[y1​,y2​,y3​,…,yn​]：</p><ul><li><strong>原始目标序列</strong>：包含模型应该预测的词，例如 <code>[y_1, y_2, y_3, \ldots, y_n]</code>。</li><li><strong>Shifted Right 序列</strong>：在解码器输入时，将目标序列右移一位，并在最前面插入一个特定的起始标记 <code>&lt;start></code>（例如 <code>[&lt;start>, y_1, y_2, \ldots, y_{n-1}]</code>），用于指导模型生成完整的输出序列。</li></ul><h3 id=2-为什么需要-shifted-right>2. 为什么需要 Shifted Right</h3><ul><li><p><strong>自回归建模</strong>：在生成第 t 个词 yt​ 时，模型只能访问它之前的词（即 <code>[y_1, y_2, \ldots, y_{t-1}]</code>），从而模拟生成过程中的自回归特性。</p><p>tt</p><p>yty_t</p></li><li><p><strong>避免信息泄露</strong>：如果解码器在第 t 步直接看到 yt​ 之后的词，模型会利用未来信息，导致训练与实际生成过程不一致。因此，Shifted Right 强制模型只能基于先前的上下文来预测当前词。</p><p>tt</p><p>yty_t</p></li></ul><h3 id=3-实际应用>3. 实际应用</h3><p>在具体实现中，Shifted Right 通常在解码器输入中完成。例如：</p><ul><li><strong>训练输入</strong>：输入解码器的序列为 <code>[&lt;start>, y_1, y_2, ..., y_{n-1}]</code>。</li><li><strong>训练标签</strong>：模型的预测目标为 <code>[y_1, y_2, y_3, ..., y_n]</code>，用于计算损失。</li></ul><p>这样，解码器在第 ttt 步预测 yty_tyt​ 时，能够访问的是 [<start>,y1,y2,&mldr;,yt−1][<start>, y_1, y_2, &mldr;, y_{t-1}][<start>,y1​,y2​,&mldr;,yt−1​] 而不是包含未来词的完整序列。</p><h3 id=4-masking-和-shifted-right-配合>4. Masking 和 Shifted Right 配合</h3><p>在实际操作中，Transformer 的解码器还使用了 <strong>Masking（掩码）</strong> 技术，进一步确保模型仅关注之前的词。例如，利用三角矩阵掩码让解码器在第 ttt 步只能访问到 [<start>,y1,y2,…,yt−1][<start>, y_1, y_2, \ldots, y_{t-1}][<start>,y1​,y2​,…,yt−1​]。</p><h3 id=总结-2>总结</h3><p>Shifted Right 是 Transformer 模型在解码器中训练序列生成任务的关键操作，通过向右移动目标序列，模型能够模拟生成序列的自回归过程，确保预测时不会使用未来信息。</p><h3 id=ln层>LN层</h3><ul><li><strong>Batch Normalization (BN)</strong>：<strong>在批次维度上进行归一化</strong>，即对每个 mini-batch 内的<strong>所有样本的同一特征进行归一化</strong>。公式中使用的是每个 mini-batch 内特征的均值和方差。适合卷积神经网络（CNN）等批量大小比较大的场景。</li><li><strong>Layer Normalization (LN)</strong>：在层维度上进行归一化，即对<strong>每个样本的所有特征进行归一化</strong>。公式中使用的是单个样本的所有特征的均值和方差。适合循环神经网络（RNN）和 Transformer 等批量大小较小或动态性强的场景。</li></ul><h1 id=量子点光谱传感技术>量子点光谱传感技术</h1><p><img class=lazyload src=/svg/loading.min.svg data-src=Summary%20127e4020f060808da8e0f23caf646e39/20220907__%25E8%25BD%25AC%25E6%25AD%25A3%25E8%25BF%25B0%25E8%2581%258C%25E6%258A%25A5%25E5%2591%258A__%25E8%25B5%25B5%25E4%25BA%2591_%25E9%25A1%25B5%25E9%259D%25A2_19.png data-srcset="Summary%20127e4020f060808da8e0f23caf646e39/20220907__%25E8%25BD%25AC%25E6%25AD%25A3%25E8%25BF%25B0%25E8%2581%258C%25E6%258A%25A5%25E5%2591%258A__%25E8%25B5%25B5%25E4%25BA%2591_%25E9%25A1%25B5%25E9%259D%25A2_19.png, Summary%20127e4020f060808da8e0f23caf646e39/20220907__%25E8%25BD%25AC%25E6%25AD%25A3%25E8%25BF%25B0%25E8%2581%258C%25E6%258A%25A5%25E5%2591%258A__%25E8%25B5%25B5%25E4%25BA%2591_%25E9%25A1%25B5%25E9%259D%25A2_19.png 1.5x, Summary%20127e4020f060808da8e0f23caf646e39/20220907__%25E8%25BD%25AC%25E6%25AD%25A3%25E8%25BF%25B0%25E8%2581%258C%25E6%258A%25A5%25E5%2591%258A__%25E8%25B5%25B5%25E4%25BA%2591_%25E9%25A1%25B5%25E9%259D%25A2_19.png 2x" data-sizes=auto alt=Summary%20127e4020f060808da8e0f23caf646e39/20220907__%25E8%25BD%25AC%25E6%25AD%25A3%25E8%25BF%25B0%25E8%2581%258C%25E6%258A%25A5%25E5%2591%258A__%25E8%25B5%25B5%25E4%25BA%2591_%25E9%25A1%25B5%25E9%259D%25A2_19.png title="20220907  转正述职报告  赵云_页面_19.png"></p><p><img class=lazyload src=/svg/loading.min.svg data-src=Summary%20127e4020f060808da8e0f23caf646e39/%25E5%25B9%25BB%25E7%2581%25AF%25E7%2589%25871.png data-srcset="Summary%20127e4020f060808da8e0f23caf646e39/%25E5%25B9%25BB%25E7%2581%25AF%25E7%2589%25871.png, Summary%20127e4020f060808da8e0f23caf646e39/%25E5%25B9%25BB%25E7%2581%25AF%25E7%2589%25871.png 1.5x, Summary%20127e4020f060808da8e0f23caf646e39/%25E5%25B9%25BB%25E7%2581%25AF%25E7%2589%25871.png 2x" data-sizes=auto alt=Summary%20127e4020f060808da8e0f23caf646e39/%25E5%25B9%25BB%25E7%2581%25AF%25E7%2589%25871.png title=幻灯片1.PNG></p><p><img class=lazyload src=/svg/loading.min.svg data-src=Summary%20127e4020f060808da8e0f23caf646e39/%25E5%25B9%25BB%25E7%2581%25AF%25E7%2589%25872.png data-srcset="Summary%20127e4020f060808da8e0f23caf646e39/%25E5%25B9%25BB%25E7%2581%25AF%25E7%2589%25872.png, Summary%20127e4020f060808da8e0f23caf646e39/%25E5%25B9%25BB%25E7%2581%25AF%25E7%2589%25872.png 1.5x, Summary%20127e4020f060808da8e0f23caf646e39/%25E5%25B9%25BB%25E7%2581%25AF%25E7%2589%25872.png 2x" data-sizes=auto alt=Summary%20127e4020f060808da8e0f23caf646e39/%25E5%25B9%25BB%25E7%2581%25AF%25E7%2589%25872.png title=幻灯片2.PNG></p><h1 id=火点监测>火点监测</h1><p><strong>监测的主要原理是利用火点像元在3.9um处的中红外波段对高温热源的敏感特性，根据阈值参数以及各种判识条件进行逐像元火点判识；</strong></p><h2 id=遥感火点监测原理>遥感火点监测原理</h2><p><img class=lazyload src=/svg/loading.min.svg data-src=Summary%20127e4020f060808da8e0f23caf646e39/image%2059.png data-srcset="Summary%20127e4020f060808da8e0f23caf646e39/image%2059.png, Summary%20127e4020f060808da8e0f23caf646e39/image%2059.png 1.5x, Summary%20127e4020f060808da8e0f23caf646e39/image%2059.png 2x" data-sizes=auto alt=Summary%20127e4020f060808da8e0f23caf646e39/image%2059.png title=image.png></p><p><img class=lazyload src=/svg/loading.min.svg data-src=Summary%20127e4020f060808da8e0f23caf646e39/image%2060.png data-srcset="Summary%20127e4020f060808da8e0f23caf646e39/image%2060.png, Summary%20127e4020f060808da8e0f23caf646e39/image%2060.png 1.5x, Summary%20127e4020f060808da8e0f23caf646e39/image%2060.png 2x" data-sizes=auto alt=Summary%20127e4020f060808da8e0f23caf646e39/image%2060.png title=image.png></p><p><img class=lazyload src=/svg/loading.min.svg data-src=Summary%20127e4020f060808da8e0f23caf646e39/image%2061.png data-srcset="Summary%20127e4020f060808da8e0f23caf646e39/image%2061.png, Summary%20127e4020f060808da8e0f23caf646e39/image%2061.png 1.5x, Summary%20127e4020f060808da8e0f23caf646e39/image%2061.png 2x" data-sizes=auto alt=Summary%20127e4020f060808da8e0f23caf646e39/image%2061.png title=image.png></p><p><img class=lazyload src=/svg/loading.min.svg data-src=Summary%20127e4020f060808da8e0f23caf646e39/image%2062.png data-srcset="Summary%20127e4020f060808da8e0f23caf646e39/image%2062.png, Summary%20127e4020f060808da8e0f23caf646e39/image%2062.png 1.5x, Summary%20127e4020f060808da8e0f23caf646e39/image%2062.png 2x" data-sizes=auto alt=Summary%20127e4020f060808da8e0f23caf646e39/image%2062.png title=image.png></p><p><img class=lazyload src=/svg/loading.min.svg data-src=Summary%20127e4020f060808da8e0f23caf646e39/image%2063.png data-srcset="Summary%20127e4020f060808da8e0f23caf646e39/image%2063.png, Summary%20127e4020f060808da8e0f23caf646e39/image%2063.png 1.5x, Summary%20127e4020f060808da8e0f23caf646e39/image%2063.png 2x" data-sizes=auto alt=Summary%20127e4020f060808da8e0f23caf646e39/image%2063.png title=image.png></p><p><img class=lazyload src=/svg/loading.min.svg data-src=Summary%20127e4020f060808da8e0f23caf646e39/image%2064.png data-srcset="Summary%20127e4020f060808da8e0f23caf646e39/image%2064.png, Summary%20127e4020f060808da8e0f23caf646e39/image%2064.png 1.5x, Summary%20127e4020f060808da8e0f23caf646e39/image%2064.png 2x" data-sizes=auto alt=Summary%20127e4020f060808da8e0f23caf646e39/image%2064.png title=image.png></p><p><img class=lazyload src=/svg/loading.min.svg data-src=Summary%20127e4020f060808da8e0f23caf646e39/image%2065.png data-srcset="Summary%20127e4020f060808da8e0f23caf646e39/image%2065.png, Summary%20127e4020f060808da8e0f23caf646e39/image%2065.png 1.5x, Summary%20127e4020f060808da8e0f23caf646e39/image%2065.png 2x" data-sizes=auto alt=Summary%20127e4020f060808da8e0f23caf646e39/image%2065.png title=image.png></p><p><img class=lazyload src=/svg/loading.min.svg data-src=Summary%20127e4020f060808da8e0f23caf646e39/image%2066.png data-srcset="Summary%20127e4020f060808da8e0f23caf646e39/image%2066.png, Summary%20127e4020f060808da8e0f23caf646e39/image%2066.png 1.5x, Summary%20127e4020f060808da8e0f23caf646e39/image%2066.png 2x" data-sizes=auto alt=Summary%20127e4020f060808da8e0f23caf646e39/image%2066.png title=image.png></p><h2 id=静止卫星火点判识计算机实现过程><strong>静止卫星火点判识计算机实现过程</strong></h2><p>1、读输入数据</p><p>（1）读取拼接、投影后L2
2000m通道数据、太阳方位角、天顶角</p><p>（2）读取静态投影文件经纬度、卫星方位角、天顶角</p><p>（3）读取全球土地覆盖类型二进制文件</p><p>（4）根据太阳和卫星的方位角、天顶角计算每个像元位置的耀斑角</p><p>2、进行火点判识</p><p>（1）读取xml阈值参数配置文件，并计算每个像元位置的经纬度分块id</p><p>（2）进行像元标记，包括耀斑、水体、荒漠、云区、非检测区、可处理像元等</p><p>（3）计算每个像元位置的背景亮温平均值</p><p>（4）计算每个像元位置的背景亮温标准差</p><p>（5**）根据阈值参数、各种判识条件进行逐像元火点判识**</p><p>3、火点信息处理</p><p>（1）确定火点分区，并记录每个火点的输出信息</p><p>（2）读取行政边界数据文件</p><p>（3）计算火点的亚像元面积</p><p>（4）火区信息处理，计算像元个数、火区面积、中心经纬度、土地类型比例、行政边界等</p><p>（5）计算火点可信度等级</p><p>4、火点信息输出</p><p>（1）输出.dat文件、判火标记HDF文件、KML、文本文件等</p><p>（2）插入MySQL数据库</p><h2 id=极轨>极轨</h2><p>类似静止</p></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span>Updated on 2025-04-08</span></div></div><div class=post-info-line><div class=post-info-md></div><div class=post-info-share><span><a href=javascript:void(0); title="Share on Twitter" data-sharer=twitter data-url=https://fjyu95.github.io/posts/summary-127e4020f060808da8e0f23caf646e39/ data-title=知识笔记 data-hashtags="deep learning,CV,遥感,光学"><i class="fab fa-twitter fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Facebook" data-sharer=facebook data-url=https://fjyu95.github.io/posts/summary-127e4020f060808da8e0f23caf646e39/ data-hashtag="deep learning"><i class="fab fa-facebook-square fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Hacker News" data-sharer=hackernews data-url=https://fjyu95.github.io/posts/summary-127e4020f060808da8e0f23caf646e39/ data-title=知识笔记><i class="fab fa-hacker-news fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Line" data-sharer=line data-url=https://fjyu95.github.io/posts/summary-127e4020f060808da8e0f23caf646e39/ data-title=知识笔记><i data-svg-src=https://cdn.jsdelivr.net/npm/simple-icons@7.3.0/icons/line.svg aria-hidden=true></i></a><a href=javascript:void(0); title="Share on 微博" data-sharer=weibo data-url=https://fjyu95.github.io/posts/summary-127e4020f060808da8e0f23caf646e39/ data-title=知识笔记><i class="fab fa-weibo fa-fw" aria-hidden=true></i></a></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fas fa-tags fa-fw" aria-hidden=true></i>&nbsp;<a href=/tags/deep-learning/>Deep Learning</a>,&nbsp;<a href=/tags/cv/>CV</a>,&nbsp;<a href=/tags/%E9%81%A5%E6%84%9F/>遥感</a>,&nbsp;<a href=/tags/%E5%85%89%E5%AD%A6/>光学</a></section><section><span><a href=javascript:void(0); onclick=window.history.back()>Back</a></span>&nbsp;|&nbsp;<span><a href=/>Home</a></span></section></div><div class=post-nav><a href=/posts/%D1%86%D1%81%D0%B4%D1%85%D1%86%D0%BA%D0%B0%D1%85%D0%B7-4930a5ba56eb4f559ace49268f2a64b0/ class=prev rel=prev title=摄影技巧><i class="fas fa-angle-left fa-fw" aria-hidden=true></i>摄影技巧</a>
<a href=/posts/3dgs-138e4020f06080e7b9ced1f2074982d5/ class=next rel=next title=3DGS>3DGS<i class="fas fa-angle-right fa-fw" aria-hidden=true></i></a></div></div></article></div></main><footer class=footer><div class=footer-container><div class=footer-line>Powered by <a href=https://gohugo.io/ target=_blank rel="noopener noreffer" title="Hugo 0.145.0">Hugo</a> | Theme - <a href=https://github.com/dillonzq/LoveIt target=_blank rel="noopener noreffer" title="LoveIt 0.3.0"><i class="far fa-kiss-wink-heart fa-fw" aria-hidden=true></i> LoveIt</a></div><div class=footer-line itemscope itemtype=http://schema.org/CreativeWork><i class="far fa-copyright fa-fw" aria-hidden=true></i><span itemprop=copyrightYear>2020 - 2025</span><span class=author itemprop=copyrightHolder>&nbsp;<a href=https://fjyu95.github.io/ target=_blank>fjyu95</a></span></div></div></footer></div><div id=fixed-buttons><a href=# id=back-to-top class=fixed-button title="Back to Top"><i class="fas fa-arrow-up fa-fw" aria-hidden=true></i>
</a><a href=# id=view-comments class=fixed-button title="View Comments"><i class="fas fa-comment fa-fw" aria-hidden=true></i></a></div><script type=text/javascript src=https://cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/sharer.js@0.5.1/sharer.min.js></script><script type=text/javascript>window.config={code:{copyTitle:"Copy to clipboard",maxShownLines:50},comment:{}}</script><script type=text/javascript src=/js/theme.min.js></script></body></html>